{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b3139d-1ac0-4486-80f0-e54d08259ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import IntSlider\n",
    "from __future__ import print_function\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee726465-0b88-46f7-877f-5aee84616ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy\n",
    "import shap\n",
    "import numpy as np\n",
    "import ZCA as zca\n",
    "import statsmodels.api as sm\n",
    "import matplotlib as plt\n",
    "\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import IPython\n",
    "\n",
    "import os\n",
    "os.environ['R_HOME'] = '/mnt/distvol/R/4.0.5/lib64/R/'\n",
    "import rpy2.robjects as R\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import arange\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import seaborn as sns\n",
    "from ModelDiagnostics import Plot\n",
    "from sklearn.cluster import DBSCAN\n",
    "from clustergram import Clustergram\n",
    "import urbangrammar_graphics as ugg\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy import stats\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from scipy.stats import f\n",
    "\n",
    "import dtale\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bf8827-4f34-413b-9a4d-bd6565e33372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#power = PowerTransformer(method='box-cox')\n",
    "\n",
    "def testNormal (x):    \n",
    "    \n",
    "    k2, p = stats.normaltest(x)\n",
    "    alpha = .001\n",
    "    #print(\"p = {:g}\".format(p))    \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        #print(p)\n",
    "        #print(alpha)\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "        xt, _ = stats.yeojohnson(x)\n",
    "        #xt, _ = stats.boxcox(x)        \n",
    "        print(_)\n",
    "        xt = pd.DataFrame(xt)\n",
    "        \n",
    "        return _, pd.DataFrame(xt).set_index(x.index)\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")    \n",
    "        return 1, pd.DataFrame(x)\n",
    "\n",
    "def inverse_boxcox (data, lambdas):\n",
    "    return inv_boxcox(data, lambdas.values)\n",
    "    \n",
    "def transform_boxcox_l(data, l_):\n",
    "    transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        #print(i)\n",
    "        if l_.iloc[i].values == 1:\n",
    "            inner_scale = data.iloc[:,i]            \n",
    "        else:\n",
    "            inner_scale = pd.DataFrame(stats.boxcox((data.iloc[:,i]), lmbda=l_.iloc[i].values))\n",
    "            \n",
    "        inner_scale.index = data.index\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed\n",
    "\n",
    "def transform_boxcox (data):\n",
    "    transformed = pd.DataFrame()\n",
    "    transformed_lambdas = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        l, inner_scale = testNormal(data.iloc[:,i])\n",
    "        inner_scale.set_index(data.index)\n",
    "\n",
    "        transformed_lambdas = pd.concat([transformed_lambdas,pd.DataFrame(pd.Series(l))],axis=0)\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed, transformed_lambdas\n",
    "\n",
    "def revert_boxcox (data, lambdas):\n",
    "    reverted = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):        \n",
    "        if lambdas.iloc[i].values == 1 :\n",
    "            revert = data.iloc[:,i]\n",
    "        else:\n",
    "            revert = pd.DataFrame(inv_boxcox(data.iloc[:,i].values, lambdas.iloc[i].values))            \n",
    "        revert.index = data.index\n",
    "        reverted = pd.concat([reverted,revert],axis=1)\n",
    "        \n",
    "    reverted.columns = data.columns\n",
    "    return reverted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "id": "2ec9d431-186e-4171-8a8a-378ab2ab2e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_deltas(data):\n",
    "\n",
    "    R.r('''\n",
    "               f <- function(values) {\n",
    "                        #system(\"which openssl\")\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              \n",
    "                              #return(d)\n",
    "                              r = residuals(varvefd, reg = TRUE)\n",
    "                              print(r)\n",
    "                              return(r)\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f = R.globalenv['f']\n",
    "    d=R.conversion.rpy2py((r_f(R.conversion.py2rpy(data.dropna()))))\n",
    "    return(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a133da3-b0fd-4741-a0aa-db8cc75f878d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weights(d, num_k):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    \n",
    "    for k in range(1, num_k):\n",
    "        w_k = np.append(w_k, -w_k[-1] * ((d - k + 1)) / k)\n",
    "        \n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def get_weights_floored(d, num_k, floor=1e-3):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$ provided weight above a minimum value\n",
    "    (floor) for the weights to prevent computation of weights for the entire\n",
    "    time series.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "        floor (float): minimum value for the weights for computational efficiency.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    k = 1\n",
    "    \n",
    "    while k < num_k:\n",
    "        w_k_latest = -w_k[-1] * ((d - k + 1)) / k\n",
    "        if abs(w_k_latest) <= floor:\n",
    "            break\n",
    "\n",
    "        w_k = np.append(w_k, w_k_latest)\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def frac_diff(df, d, floor=1e-3):\n",
    "    r\"\"\"Fractionally difference time series via CPU.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe of raw time series values.\n",
    "        d (float): differencing value from 0 to 1 where > 1 has no FD.\n",
    "        floor (float): minimum value of weights, ignoring anything smaller.\n",
    "    \"\"\"\n",
    "    # Get weights window\n",
    "    weights = get_weights_floored(d=d, num_k=len(df), floor=floor)\n",
    "    weights_window_size = len(weights)\n",
    "    \n",
    "    # Reverse weights\n",
    "    weights = weights[::-1]\n",
    "    \n",
    "    # Blank fractionally differenced series to be filled\n",
    "    df_fd = []\n",
    "\n",
    "    # Slide window of time series, to calculated fractionally differenced values\n",
    "    # per window\n",
    "    for idx in range(weights_window_size, df.shape[0]):\n",
    "        # Dot product of weights and original values\n",
    "        # to get fractionally differenced values\n",
    "        date_idx = df.index[idx]\n",
    "        df_fd.append(np.dot(weights.T, df.iloc[idx - weights_window_size:idx]).item())\n",
    "    \n",
    "    # Return FD values and weights\n",
    "    df_fd = pd.DataFrame(df_fd)\n",
    "    \n",
    "    return df_fd, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e53d3-2864-4d69-a45a-ce48217e4fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d949ef06-ae96-4828-b02b-21a92a11b29f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6233b-8abf-4f78-83f5-a3e7a2e26fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_data = pd.read_csv('/mnt/distvol/combined_set.csv')\n",
    "all_data.index = all_data.iloc[:,0]\n",
    "all_data = all_data.iloc[:,1:]\n",
    "\n",
    "filter_ = all_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b9636-4857-4aaa-b13d-e2077e957c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(Y):\n",
    "    \n",
    "    #Y = x\n",
    "    #output_slider_variable.value\n",
    "    internalFilter = filter_.copy()\n",
    "    internalFilter.remove(Y)\n",
    "    all_data_ = pd.concat([all_data[Y],all_data[internalFilter]], axis=1)    \n",
    "    #print(all_data_.describe())\n",
    "    display(all_data_.describe())\n",
    "    x_ticks = all_data_.index[np.arange(0, len(all_data.index), 5)]\n",
    "    plt.plot(all_data_[Y])\n",
    "    plt.xticks(x_ticks, rotation = 45)\n",
    "    plt.show()        \n",
    "    plt.hist(all_data_[Y], bins='auto')\n",
    "    plt.show()\n",
    "    diff = pd.DataFrame((all_data_[Y]-all_data_[Y].shift(-1))).dropna()\n",
    "    plt.hist(diff, bins='auto')\n",
    "    plt.show()\n",
    "    return(all_data_)\n",
    "    \n",
    "out = interactive(f3, Y=filter_)\n",
    "\n",
    "#output_slider_variable.observe(f4, 'value')\n",
    "\n",
    "print(\"choose Y\")\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1c5c4-9718-4cbb-a820-9fd94e3fc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = get_deltas(out.result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "2070d863-3329-4f28-8b4e-a74b5480cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "differenced = pd.DataFrame(np.transpose(d.reshape(len(out.result.columns),len(out.result))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2d32f-b9a1-4196-9d43-1359b65f1c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492f94d-cba1-455a-b1d4-157a346f6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = dtale.show(out.result)\n",
    "d_.open_browser()\n",
    "d_._url  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "4273fdd8-d805-4928-b7dd-bda427cf51fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport concurrent.futures\\nfrom concurrent.futures import wait, ALL_COMPLETED\\nfrom fracdiff import fdiff\\n\\ncores = int(len(os.sched_getaffinity(0)))\\n\\ndef getDifferenced(i):\\n    #v = d[[i]]\\n    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i], d=d[[i]], floor=5e-5)\\n    \\n    a = np.array(out.result.iloc[:, i])\\n    \\n    return fdiff(a, n=d[i], axis=0)\\n    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i]), d=v, floor=5e-5)\\n\\npool01 = concurrent.futures.ProcessPoolExecutor(cores)\\n\\nfutures01 = [pool01.submit(getDifferenced, args) for args in range(0,len(d))]\\n\\nwait(futures01, timeout=None, return_when=ALL_COMPLETED)\\n'"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import concurrent.futures\n",
    "from concurrent.futures import wait, ALL_COMPLETED\n",
    "from fracdiff import fdiff\n",
    "\n",
    "cores = int(len(os.sched_getaffinity(0)))\n",
    "\n",
    "def getDifferenced(i):\n",
    "    #v = d[[i]]\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i], d=d[[i]], floor=5e-5)\n",
    "    \n",
    "    a = np.array(out.result.iloc[:, i])\n",
    "    \n",
    "    return fdiff(a, n=d[i], axis=0)\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i]), d=v, floor=5e-5)\n",
    "\n",
    "pool01 = concurrent.futures.ProcessPoolExecutor(cores)\n",
    "\n",
    "futures01 = [pool01.submit(getDifferenced, args) for args in range(0,len(d))]\n",
    "\n",
    "wait(futures01, timeout=None, return_when=ALL_COMPLETED)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfa758-3cf3-4410-bfc6-85105b37a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Differenced_Set = pd.DataFrame()\n",
    "for f in range(0,len(futures01)):\n",
    "    value = pd.DataFrame(futures01[f].result())\n",
    "    Differenced_Set = pd.concat([Differenced_Set,value],axis=1)\n",
    "    \n",
    "Differenced_Set.columns = out.result.columns\n",
    "Differenced_Set.index = out.result.index\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e2820-110a-4786-9035-98543c301b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for f in range(0,len(futures01)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "        plt.hist(Differenced_Set.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        Differenced_Set.iloc[:,1].plot()\n",
    "        plt.show()\n",
    "        plt.hist(all_data.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        all_data.iloc[:,1].plot()\n",
    "        plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce862ad-f7fc-4f0b-b376-c29d32d01281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b6b7d-d0c5-4077-b822-d79e626067b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49f670-bbdb-42a8-b9e3-e75e1b60e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = out.result.corr()\n",
    "#.abs()\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0016982-6857-424e-868a-da44b16ff66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece4be0-8244-434e-b66e-9c1f39a7c2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "4d4a92b7-e5b3-49ed-b31f-5959f90e8fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "def critical_r(n, alpha = .05 ):\n",
    "    df = n - 2\n",
    "    critical_t = scipy.stats.t.isf(alpha / 2, df)\n",
    "    critical_r = np.sqrt( (critical.t^2) / ( (critical.t^2) + df ) )\n",
    "    return(critical_r)\n",
    "\n",
    "def xcorr(x, y, maxlags=10):\n",
    "    Nx = len(x)\n",
    "    if Nx != len(y):\n",
    "        raise ValueError('x and y must be equal length')\n",
    "\n",
    "    c = np.correlate(x, y, mode=2)\n",
    "\n",
    "    if maxlags is None:\n",
    "        maxlags = Nx - 1\n",
    "\n",
    "    if maxlags >= Nx or maxlags < 1:\n",
    "        raise ValueError('maxlags must be None or strictly positive < %d' % Nx)\n",
    "\n",
    "    c = c[Nx - 1 - maxlags:Nx + maxlags]\n",
    "\n",
    "    return c\n",
    "\n",
    "def getLagged_Set(set_, og):\n",
    "\n",
    "    #set_.index = all_data.index\n",
    "\n",
    "    #maxl = int(len(set_)/2)\n",
    "    maxl = 5\n",
    "    \n",
    "    Lagged_Differenced_Set = pd.DataFrame()\n",
    "    Lagged_Set = pd.DataFrame()\n",
    "    #TrainO_Lagged_Set = pd.DataFrame()\n",
    "    lags = []\n",
    "    lagcorrs = []\n",
    "    ogcorrs = []\n",
    "\n",
    "    for f in range(1,len(set_.columns)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "\n",
    "        data_1 = set_.iloc[:,0]\n",
    "        data_2 = set_.iloc[:,f]\n",
    "\n",
    "        ogc = np.array(pd.concat([data_1 - np.mean(data_1),data_2 - np.mean(data_2)],axis=1).corr())[1,0]    \n",
    "        ogcorrs.append(ogc)\n",
    "\n",
    "        #corr = xcorr(data_1 - np.mean(data_1), data_2 - np.mean(data_2),maxlags=5)\n",
    "\n",
    "        set1 = data_1 - np.mean(data_1)\n",
    "        set2 = data_2 - np.mean(data_2)\n",
    "\n",
    "        corrs_ = []\n",
    "        for i in range(0,maxl):\n",
    "            c = np.array((pd.concat([set1,set2.shift(i)],axis=1).dropna()).corr())[0,1]\n",
    "            corrs_.append(c)\n",
    "\n",
    "        #corr = np.correlate(data_1 - np.mean(data_1), data_2 - np.mean(data_2),mode='full')\n",
    "        #plt.plot(corr)\n",
    "        #plt.show()\n",
    "\n",
    "        #lag = corr.argmax() - (len(data_1) - 1)\n",
    "        lag = abs(pd.Series(corrs_)).idxmax()\n",
    "\n",
    "        #print(corr)\n",
    "        lagc = np.array(pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1).corr())[1,0]\n",
    "\n",
    "        #print(lag)\n",
    "\n",
    "        #print(ogc)\n",
    "        #print(lagc)\n",
    "\n",
    "        #print(lag)\n",
    "        #plt.plot(data_1, 'r*')\n",
    "        #plt.plot(data_2, 'b*')\n",
    "\n",
    "        lag_merged = pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1)\n",
    "\n",
    "        x_ticks = all_data.index[np.arange(0, len(all_data.index), 5)]\n",
    "        #plt.xticks(x_ticks, rotation = 45)    \n",
    "        #plt.show()\n",
    "\n",
    "        #plt.scatter(data_2.shift(lag),data_1)\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_acf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_pacf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        y = data_1\n",
    "        X = data_2\n",
    "        #reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        #print(reg.score(X, y),reg.coef_,reg.intercept_)\n",
    "\n",
    "        model = sm.OLS(y,X)\n",
    "        results = model.fit()\n",
    "        #print(results.summary())\n",
    "\n",
    "        #Lagged_Differenced_Set = pd.concat([Lagged_Differenced_Set,data_2.shift(lag)],axis=1)\n",
    "        #TrainO_Lagged_Set = pd.concat([TrainO_Lagged_Set,all_data.iloc[:,f].shift(lag)],axis=1)\n",
    "        #if lag>0:\n",
    "            #lag = 0\n",
    "\n",
    "        Lagged_Set = pd.concat([Lagged_Set,og.iloc[:,f].shift(lag)],axis=1)\n",
    "\n",
    "        lagcorrs.append(lagc)\n",
    "\n",
    "        lags.append(lag)\n",
    "\n",
    "    Lagged_Set.index = set_.index\n",
    "    #stats = pd.concat([pd.DataFrame(set_.columns[1:]),pd.DataFrame(lags),pd.DataFrame(lagcorrs),pd.DataFrame(ogcorrs)],axis=1)\n",
    "    stats = pd.concat([pd.DataFrame(set_.columns[1:]),pd.DataFrame(lags),pd.DataFrame(ogcorrs)],axis=1)\n",
    "    \n",
    "    #print(Lagged_Set)\n",
    "    #return stats, pd.concat([data_1,Lagged_Set],axis=1),  pd.concat([data_1,Lagged_Differenced_Set],axis=1)\n",
    "    return stats, pd.concat([data_1,Lagged_Set],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb26a0-bb46-4fd9-b9b8-2de6a3a7f875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "3e80173c-962f-4b0e-b170-8186d5f9b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lagged_Set = pd.DataFrame()\n",
    "Lagged_Differenced_Set = pd.DataFrame()\n",
    "\n",
    "ls_stats, Lagged_Set= getLagged_Set(differenced, out.result)\n",
    "lso_stats, Lagged_Set_offset= getLagged_Set(pd.concat([differenced.iloc[:,0].shift(-1),differenced.iloc[:,1:]],axis=1), out.result)\n",
    "\n",
    "Lagged_Set_offset.index = out.result.index\n",
    "Lagged_Set_offset.dropna(inplace= True)\n",
    "Lagged_Set_offset.columns = out.result.columns\n",
    "\n",
    "#Lagged_Differenced_Set.dropna(inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b19b33-dfd6-4233-8e3c-4f266a9ce8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "id": "541e943c-2c5b-4cc8-86ca-87a1dab05a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^SP500TR</th>\n",
       "      <th>DGS10</th>\n",
       "      <th>DTB3</th>\n",
       "      <th>DGS3MO</th>\n",
       "      <th>MORTGAGE30US</th>\n",
       "      <th>DFII10</th>\n",
       "      <th>T5YIFR</th>\n",
       "      <th>BAMLHYH0A0HYM2TRIV</th>\n",
       "      <th>BAMLCC0A1AAATRIV</th>\n",
       "      <th>DGS1</th>\n",
       "      <th>...</th>\n",
       "      <th>MRK</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NKE</th>\n",
       "      <th>PG</th>\n",
       "      <th>TRV</th>\n",
       "      <th>UNH</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WBA</th>\n",
       "      <th>DIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-09-30</th>\n",
       "      <td>0.060820</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-12-31</th>\n",
       "      <td>0.076878</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-03-31</th>\n",
       "      <td>-0.004395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-06-30</th>\n",
       "      <td>-0.012870</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-09-30</th>\n",
       "      <td>0.057999</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>-0.035486</td>\n",
       "      <td>-0.231272</td>\n",
       "      <td>-0.139409</td>\n",
       "      <td>-0.139576</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>-0.056758</td>\n",
       "      <td>-0.014645</td>\n",
       "      <td>0.026680</td>\n",
       "      <td>-0.183227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067092</td>\n",
       "      <td>0.071684</td>\n",
       "      <td>-0.010617</td>\n",
       "      <td>0.042059</td>\n",
       "      <td>-0.079265</td>\n",
       "      <td>0.055494</td>\n",
       "      <td>-0.041490</td>\n",
       "      <td>-0.027629</td>\n",
       "      <td>-0.114633</td>\n",
       "      <td>-0.089830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>0.137470</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.205011</td>\n",
       "      <td>-0.206562</td>\n",
       "      <td>-0.048233</td>\n",
       "      <td>-1.412198</td>\n",
       "      <td>-0.042913</td>\n",
       "      <td>-0.044535</td>\n",
       "      <td>0.048502</td>\n",
       "      <td>-0.145904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030525</td>\n",
       "      <td>0.122910</td>\n",
       "      <td>-0.005052</td>\n",
       "      <td>-0.013421</td>\n",
       "      <td>-0.080922</td>\n",
       "      <td>0.045550</td>\n",
       "      <td>-0.006380</td>\n",
       "      <td>0.076205</td>\n",
       "      <td>-0.216030</td>\n",
       "      <td>-0.126801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>0.075139</td>\n",
       "      <td>-0.238254</td>\n",
       "      <td>-0.314105</td>\n",
       "      <td>-0.313234</td>\n",
       "      <td>-0.080166</td>\n",
       "      <td>6.574263</td>\n",
       "      <td>-0.104170</td>\n",
       "      <td>0.074650</td>\n",
       "      <td>0.035055</td>\n",
       "      <td>-0.322416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053685</td>\n",
       "      <td>0.106936</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>-0.022128</td>\n",
       "      <td>-0.146659</td>\n",
       "      <td>0.076968</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.085244</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.132230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>0.091881</td>\n",
       "      <td>-0.496250</td>\n",
       "      <td>-0.868509</td>\n",
       "      <td>-0.871179</td>\n",
       "      <td>-0.088578</td>\n",
       "      <td>0.959941</td>\n",
       "      <td>-0.057046</td>\n",
       "      <td>0.037628</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>-0.836772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031781</td>\n",
       "      <td>0.160039</td>\n",
       "      <td>0.236566</td>\n",
       "      <td>0.145938</td>\n",
       "      <td>0.095168</td>\n",
       "      <td>0.095972</td>\n",
       "      <td>0.033116</td>\n",
       "      <td>0.095196</td>\n",
       "      <td>0.084698</td>\n",
       "      <td>0.148307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-31</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ^SP500TR     DGS10      DTB3    DGS3MO  MORTGAGE30US    DFII10  \\\n",
       "2003-09-30  0.060820       NaN       NaN       NaN           NaN       NaN   \n",
       "2003-12-31  0.076878       NaN       NaN       NaN           NaN       NaN   \n",
       "2004-03-31 -0.004395       NaN       NaN       NaN           NaN       NaN   \n",
       "2004-06-30 -0.012870       NaN       NaN       NaN           NaN       NaN   \n",
       "2004-09-30  0.057999       NaN       NaN       NaN           NaN       NaN   \n",
       "...              ...       ...       ...       ...           ...       ...   \n",
       "2020-03-31 -0.035486 -0.231272 -0.139409 -0.139576      0.011354 -0.005393   \n",
       "2020-06-30  0.137470 -0.003356 -0.205011 -0.206562     -0.048233 -1.412198   \n",
       "2020-09-30  0.075139 -0.238254 -0.314105 -0.313234     -0.080166  6.574263   \n",
       "2020-12-31  0.091881 -0.496250 -0.868509 -0.871179     -0.088578  0.959941   \n",
       "2021-03-31       NaN       NaN       NaN       NaN           NaN       NaN   \n",
       "\n",
       "              T5YIFR  BAMLHYH0A0HYM2TRIV  BAMLCC0A1AAATRIV      DGS1  ...  \\\n",
       "2003-09-30       NaN                 NaN               NaN       NaN  ...   \n",
       "2003-12-31       NaN                 NaN               NaN       NaN  ...   \n",
       "2004-03-31       NaN                 NaN               NaN       NaN  ...   \n",
       "2004-06-30       NaN                 NaN               NaN       NaN  ...   \n",
       "2004-09-30       NaN                 NaN               NaN       NaN  ...   \n",
       "...              ...                 ...               ...       ...  ...   \n",
       "2020-03-31 -0.056758           -0.014645          0.026680 -0.183227  ...   \n",
       "2020-06-30 -0.042913           -0.044535          0.048502 -0.145904  ...   \n",
       "2020-09-30 -0.104170            0.074650          0.035055 -0.322416  ...   \n",
       "2020-12-31 -0.057046            0.037628         -0.005473 -0.836772  ...   \n",
       "2021-03-31       NaN                 NaN               NaN       NaN  ...   \n",
       "\n",
       "                 MRK      MSFT       NKE        PG       TRV       UNH  \\\n",
       "2003-09-30       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2003-12-31       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2004-03-31       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2004-06-30       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "2004-09-30       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2020-03-31  0.067092  0.071684 -0.010617  0.042059 -0.079265  0.055494   \n",
       "2020-06-30  0.030525  0.122910 -0.005052 -0.013421 -0.080922  0.045550   \n",
       "2020-09-30  0.053685  0.106936  0.165900 -0.022128 -0.146659  0.076968   \n",
       "2020-12-31  0.031781  0.160039  0.236566  0.145938  0.095168  0.095972   \n",
       "2021-03-31       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                  VZ       WMT       WBA       DIS  \n",
       "2003-09-30       NaN       NaN       NaN       NaN  \n",
       "2003-12-31       NaN       NaN       NaN       NaN  \n",
       "2004-03-31       NaN       NaN       NaN       NaN  \n",
       "2004-06-30       NaN       NaN       NaN       NaN  \n",
       "2004-09-30       NaN       NaN       NaN       NaN  \n",
       "...              ...       ...       ...       ...  \n",
       "2020-03-31 -0.041490 -0.027629 -0.114633 -0.089830  \n",
       "2020-06-30 -0.006380  0.076205 -0.216030 -0.126801  \n",
       "2020-09-30  0.044651  0.085244  0.016860  0.132230  \n",
       "2020-12-31  0.033116  0.095196  0.084698  0.148307  \n",
       "2021-03-31       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[71 rows x 91 columns]"
      ]
     },
     "execution_count": 591,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = pd.DataFrame()\n",
    "temp = pd.concat([out.result.iloc[:,0].pct_change().shift(-1),Lagged_Set_offset.iloc[:,1:].pct_change()],axis=1).copy()\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "29c83e18-ed0b-446d-89a3-86ea42e5672c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^SP500TR</th>\n",
       "      <th>DGS10</th>\n",
       "      <th>DTB3</th>\n",
       "      <th>DGS3MO</th>\n",
       "      <th>MORTGAGE30US</th>\n",
       "      <th>DFII10</th>\n",
       "      <th>T5YIFR</th>\n",
       "      <th>BAMLHYH0A0HYM2TRIV</th>\n",
       "      <th>BAMLCC0A1AAATRIV</th>\n",
       "      <th>DGS1</th>\n",
       "      <th>...</th>\n",
       "      <th>MRK</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NKE</th>\n",
       "      <th>PG</th>\n",
       "      <th>TRV</th>\n",
       "      <th>UNH</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WBA</th>\n",
       "      <th>DIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-12-31</th>\n",
       "      <td>0.029813</td>\n",
       "      <td>0.146408</td>\n",
       "      <td>0.177641</td>\n",
       "      <td>0.173951</td>\n",
       "      <td>-0.034697</td>\n",
       "      <td>-0.074253</td>\n",
       "      <td>0.071443</td>\n",
       "      <td>0.047325</td>\n",
       "      <td>0.018302</td>\n",
       "      <td>0.453585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139617</td>\n",
       "      <td>0.047395</td>\n",
       "      <td>0.136556</td>\n",
       "      <td>0.022376</td>\n",
       "      <td>-0.129715</td>\n",
       "      <td>0.206465</td>\n",
       "      <td>0.073766</td>\n",
       "      <td>0.017156</td>\n",
       "      <td>0.135452</td>\n",
       "      <td>0.145218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-31</th>\n",
       "      <td>-0.003927</td>\n",
       "      <td>-0.064321</td>\n",
       "      <td>0.379480</td>\n",
       "      <td>0.381139</td>\n",
       "      <td>-0.026828</td>\n",
       "      <td>-0.107335</td>\n",
       "      <td>-0.058061</td>\n",
       "      <td>0.022953</td>\n",
       "      <td>0.004509</td>\n",
       "      <td>0.168695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040049</td>\n",
       "      <td>0.061480</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>-0.005265</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.134324</td>\n",
       "      <td>-0.099969</td>\n",
       "      <td>-0.021408</td>\n",
       "      <td>-0.016190</td>\n",
       "      <td>0.087865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-06-30</th>\n",
       "      <td>0.040415</td>\n",
       "      <td>-0.029200</td>\n",
       "      <td>0.351635</td>\n",
       "      <td>0.352408</td>\n",
       "      <td>0.005632</td>\n",
       "      <td>0.016103</td>\n",
       "      <td>-0.028520</td>\n",
       "      <td>-0.014754</td>\n",
       "      <td>0.012871</td>\n",
       "      <td>0.191683</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010308</td>\n",
       "      <td>-0.024908</td>\n",
       "      <td>-0.047000</td>\n",
       "      <td>-0.001620</td>\n",
       "      <td>0.071592</td>\n",
       "      <td>0.094748</td>\n",
       "      <td>-0.031815</td>\n",
       "      <td>-0.080337</td>\n",
       "      <td>-0.002811</td>\n",
       "      <td>-0.043581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-09-30</th>\n",
       "      <td>0.009843</td>\n",
       "      <td>0.030605</td>\n",
       "      <td>0.264461</td>\n",
       "      <td>0.264607</td>\n",
       "      <td>-0.010401</td>\n",
       "      <td>-0.023224</td>\n",
       "      <td>0.017754</td>\n",
       "      <td>0.036117</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.242531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033318</td>\n",
       "      <td>-0.013385</td>\n",
       "      <td>0.007584</td>\n",
       "      <td>0.021780</td>\n",
       "      <td>-0.007422</td>\n",
       "      <td>0.066643</td>\n",
       "      <td>-0.033054</td>\n",
       "      <td>-0.015528</td>\n",
       "      <td>0.069797</td>\n",
       "      <td>-0.074613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-12-31</th>\n",
       "      <td>0.048335</td>\n",
       "      <td>-0.033696</td>\n",
       "      <td>0.127029</td>\n",
       "      <td>0.130365</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>0.086422</td>\n",
       "      <td>-0.026112</td>\n",
       "      <td>-0.006528</td>\n",
       "      <td>-0.003505</td>\n",
       "      <td>0.086213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.335677</td>\n",
       "      <td>0.046546</td>\n",
       "      <td>0.038692</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>0.161476</td>\n",
       "      <td>0.137236</td>\n",
       "      <td>-0.060045</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>0.030966</td>\n",
       "      <td>-0.021323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-31</th>\n",
       "      <td>-0.004005</td>\n",
       "      <td>-0.118382</td>\n",
       "      <td>-0.034936</td>\n",
       "      <td>-0.034350</td>\n",
       "      <td>-0.087840</td>\n",
       "      <td>-0.701053</td>\n",
       "      <td>-0.021767</td>\n",
       "      <td>0.013810</td>\n",
       "      <td>0.013844</td>\n",
       "      <td>-0.110559</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>0.086946</td>\n",
       "      <td>0.102210</td>\n",
       "      <td>0.115945</td>\n",
       "      <td>0.036481</td>\n",
       "      <td>0.089712</td>\n",
       "      <td>0.058122</td>\n",
       "      <td>0.055645</td>\n",
       "      <td>0.149192</td>\n",
       "      <td>0.010897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-31</th>\n",
       "      <td>-0.035486</td>\n",
       "      <td>-0.231272</td>\n",
       "      <td>-0.139409</td>\n",
       "      <td>-0.139576</td>\n",
       "      <td>0.011354</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>-0.056758</td>\n",
       "      <td>-0.014645</td>\n",
       "      <td>0.026680</td>\n",
       "      <td>-0.183227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.067092</td>\n",
       "      <td>0.071684</td>\n",
       "      <td>-0.010617</td>\n",
       "      <td>0.042059</td>\n",
       "      <td>-0.079265</td>\n",
       "      <td>0.055494</td>\n",
       "      <td>-0.041490</td>\n",
       "      <td>-0.027629</td>\n",
       "      <td>-0.114633</td>\n",
       "      <td>-0.089830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-30</th>\n",
       "      <td>0.137470</td>\n",
       "      <td>-0.003356</td>\n",
       "      <td>-0.205011</td>\n",
       "      <td>-0.206562</td>\n",
       "      <td>-0.048233</td>\n",
       "      <td>-1.412198</td>\n",
       "      <td>-0.042913</td>\n",
       "      <td>-0.044535</td>\n",
       "      <td>0.048502</td>\n",
       "      <td>-0.145904</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030525</td>\n",
       "      <td>0.122910</td>\n",
       "      <td>-0.005052</td>\n",
       "      <td>-0.013421</td>\n",
       "      <td>-0.080922</td>\n",
       "      <td>0.045550</td>\n",
       "      <td>-0.006380</td>\n",
       "      <td>0.076205</td>\n",
       "      <td>-0.216030</td>\n",
       "      <td>-0.126801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-30</th>\n",
       "      <td>0.075139</td>\n",
       "      <td>-0.238254</td>\n",
       "      <td>-0.314105</td>\n",
       "      <td>-0.313234</td>\n",
       "      <td>-0.080166</td>\n",
       "      <td>6.574263</td>\n",
       "      <td>-0.104170</td>\n",
       "      <td>0.074650</td>\n",
       "      <td>0.035055</td>\n",
       "      <td>-0.322416</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053685</td>\n",
       "      <td>0.106936</td>\n",
       "      <td>0.165900</td>\n",
       "      <td>-0.022128</td>\n",
       "      <td>-0.146659</td>\n",
       "      <td>0.076968</td>\n",
       "      <td>0.044651</td>\n",
       "      <td>0.085244</td>\n",
       "      <td>0.016860</td>\n",
       "      <td>0.132230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-31</th>\n",
       "      <td>0.091881</td>\n",
       "      <td>-0.496250</td>\n",
       "      <td>-0.868509</td>\n",
       "      <td>-0.871179</td>\n",
       "      <td>-0.088578</td>\n",
       "      <td>0.959941</td>\n",
       "      <td>-0.057046</td>\n",
       "      <td>0.037628</td>\n",
       "      <td>-0.005473</td>\n",
       "      <td>-0.836772</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031781</td>\n",
       "      <td>0.160039</td>\n",
       "      <td>0.236566</td>\n",
       "      <td>0.145938</td>\n",
       "      <td>0.095168</td>\n",
       "      <td>0.095972</td>\n",
       "      <td>0.033116</td>\n",
       "      <td>0.095196</td>\n",
       "      <td>0.084698</td>\n",
       "      <td>0.148307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ^SP500TR     DGS10      DTB3    DGS3MO  MORTGAGE30US    DFII10  \\\n",
       "2004-12-31  0.029813  0.146408  0.177641  0.173951     -0.034697 -0.074253   \n",
       "2005-03-31 -0.003927 -0.064321  0.379480  0.381139     -0.026828 -0.107335   \n",
       "2005-06-30  0.040415 -0.029200  0.351635  0.352408      0.005632  0.016103   \n",
       "2005-09-30  0.009843  0.030605  0.264461  0.264607     -0.010401 -0.023224   \n",
       "2005-12-31  0.048335 -0.033696  0.127029  0.130365      0.009702  0.086422   \n",
       "...              ...       ...       ...       ...           ...       ...   \n",
       "2019-12-31 -0.004005 -0.118382 -0.034936 -0.034350     -0.087840 -0.701053   \n",
       "2020-03-31 -0.035486 -0.231272 -0.139409 -0.139576      0.011354 -0.005393   \n",
       "2020-06-30  0.137470 -0.003356 -0.205011 -0.206562     -0.048233 -1.412198   \n",
       "2020-09-30  0.075139 -0.238254 -0.314105 -0.313234     -0.080166  6.574263   \n",
       "2020-12-31  0.091881 -0.496250 -0.868509 -0.871179     -0.088578  0.959941   \n",
       "\n",
       "              T5YIFR  BAMLHYH0A0HYM2TRIV  BAMLCC0A1AAATRIV      DGS1  ...  \\\n",
       "2004-12-31  0.071443            0.047325          0.018302  0.453585  ...   \n",
       "2005-03-31 -0.058061            0.022953          0.004509  0.168695  ...   \n",
       "2005-06-30 -0.028520           -0.014754          0.012871  0.191683  ...   \n",
       "2005-09-30  0.017754            0.036117          0.007615  0.242531  ...   \n",
       "2005-12-31 -0.026112           -0.006528         -0.003505  0.086213  ...   \n",
       "...              ...                 ...               ...       ...  ...   \n",
       "2019-12-31 -0.021767            0.013810          0.013844 -0.110559  ...   \n",
       "2020-03-31 -0.056758           -0.014645          0.026680 -0.183227  ...   \n",
       "2020-06-30 -0.042913           -0.044535          0.048502 -0.145904  ...   \n",
       "2020-09-30 -0.104170            0.074650          0.035055 -0.322416  ...   \n",
       "2020-12-31 -0.057046            0.037628         -0.005473 -0.836772  ...   \n",
       "\n",
       "                 MRK      MSFT       NKE        PG       TRV       UNH  \\\n",
       "2004-12-31 -0.139617  0.047395  0.136556  0.022376 -0.129715  0.206465   \n",
       "2005-03-31  0.040049  0.061480  0.022334 -0.005265  0.001892  0.134324   \n",
       "2005-06-30  0.010308 -0.024908 -0.047000 -0.001620  0.071592  0.094748   \n",
       "2005-09-30 -0.033318 -0.013385  0.007584  0.021780 -0.007422  0.066643   \n",
       "2005-12-31 -0.335677  0.046546  0.038692  0.012642  0.161476  0.137236   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "2019-12-31  0.115591  0.086946  0.102210  0.115945  0.036481  0.089712   \n",
       "2020-03-31  0.067092  0.071684 -0.010617  0.042059 -0.079265  0.055494   \n",
       "2020-06-30  0.030525  0.122910 -0.005052 -0.013421 -0.080922  0.045550   \n",
       "2020-09-30  0.053685  0.106936  0.165900 -0.022128 -0.146659  0.076968   \n",
       "2020-12-31  0.031781  0.160039  0.236566  0.145938  0.095168  0.095972   \n",
       "\n",
       "                  VZ       WMT       WBA       DIS  \n",
       "2004-12-31  0.073766  0.017156  0.135452  0.145218  \n",
       "2005-03-31 -0.099969 -0.021408 -0.016190  0.087865  \n",
       "2005-06-30 -0.031815 -0.080337 -0.002811 -0.043581  \n",
       "2005-09-30 -0.033054 -0.015528  0.069797 -0.074613  \n",
       "2005-12-31 -0.060045  0.006618  0.030966 -0.021323  \n",
       "...              ...       ...       ...       ...  \n",
       "2019-12-31  0.058122  0.055645  0.149192  0.010897  \n",
       "2020-03-31 -0.041490 -0.027629 -0.114633 -0.089830  \n",
       "2020-06-30 -0.006380  0.076205 -0.216030 -0.126801  \n",
       "2020-09-30  0.044651  0.085244  0.016860  0.132230  \n",
       "2020-12-31  0.033116  0.095196  0.084698  0.148307  \n",
       "\n",
       "[65 rows x 91 columns]"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Lagged_Differenced_Set_offset = pd.DataFrame()\n",
    "Lagged_Differenced_Set_offset = temp.copy()\n",
    "Lagged_Differenced_Set_offset.dropna(inplace= True)\n",
    "Lagged_Differenced_Set_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "id": "8cfbc252-b567-48e9-86f9-9e57df0ac45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0  0         0\n",
      "0    1  3 -0.002200\n",
      "1    2  3 -0.008412\n",
      "2    3  3  0.027732\n",
      "3    4  2 -0.020862\n",
      "4    5  2 -0.115653\n",
      "..  .. ..       ...\n",
      "85  86  0  0.873368\n",
      "86  87  0  0.849360\n",
      "87  88  1  0.443876\n",
      "88  89  0  0.172700\n",
      "89  90  0  0.907287\n",
      "\n",
      "[90 rows x 3 columns]\n",
      "     0  0         0\n",
      "0    1  2 -0.086328\n",
      "1    2  2 -0.078614\n",
      "2    3  2 -0.073269\n",
      "3    4  1 -0.084382\n",
      "4    5  1 -0.178404\n",
      "..  .. ..       ...\n",
      "85  86  0  0.835386\n",
      "86  87  0  0.791905\n",
      "87  88  0  0.590050\n",
      "88  89  4 -0.012591\n",
      "89  90  0  0.761578\n",
      "\n",
      "[90 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(ls_stats)\n",
    "print(lso_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "53aa28e7-3712-466f-96fd-7bbf115714ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>^SP500TR</th>\n",
       "      <th>DGS10</th>\n",
       "      <th>DTB3</th>\n",
       "      <th>DGS3MO</th>\n",
       "      <th>MORTGAGE30US</th>\n",
       "      <th>DFII10</th>\n",
       "      <th>T5YIFR</th>\n",
       "      <th>BAMLHYH0A0HYM2TRIV</th>\n",
       "      <th>BAMLCC0A1AAATRIV</th>\n",
       "      <th>DGS1</th>\n",
       "      <th>...</th>\n",
       "      <th>MRK</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>NKE</th>\n",
       "      <th>PG</th>\n",
       "      <th>TRV</th>\n",
       "      <th>UNH</th>\n",
       "      <th>VZ</th>\n",
       "      <th>WMT</th>\n",
       "      <th>WBA</th>\n",
       "      <th>DIS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 91 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [^SP500TR, DGS10, DTB3, DGS3MO, MORTGAGE30US, DFII10, T5YIFR, BAMLHYH0A0HYM2TRIV, BAMLCC0A1AAATRIV, DGS1, BAMLCC0A4BBBTRIV, IC4WSA, WILLMICROCAPPR, WILLLRGCAPVAL, T5YIE, WTB3MS, WGS3MO, TWEXB, DEXCHUS, DEXUSUK, TEDRATE, VIXCLS, NFCI, BAA10Y, BAMLC0A0CM, BAMLH0A3HYC, DCOILBRENTEU, DCOILWTICO, DFF, DGS1MO, DGS30, DGS5, ICSA, M1, STLFSI2, T10Y2Y, T10Y3M, TREAST, QQQ, DIA, IYR, EWG, EWU, EWJ, EZU, EWZ, ILF, EWW, LQD, SHY, IEF, TLT, ETH, BCH, LTC, XLY, XLP, XLE, XLF, XLV, XLI, XLB, XLK, XLU, MMM, AXP, AMGN, AAPL, BA, CAT, CVX, CSCO, KO, GS, HD, HON, IBM, INTC, JNJ, JPM, MCD, MRK, MSFT, NKE, PG, TRV, UNH, VZ, WMT, WBA, DIS]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 91 columns]"
      ]
     },
     "execution_count": 576,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "abb40926-6979-4f9f-ad09-ed1f93c25a80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The null hypothesis can be rejected\n",
      "6.885443339653454\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "-0.22878674517176567\n",
      "The null hypothesis can be rejected\n",
      "-0.22668581022971535\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "0.8166023951541367\n",
      "The null hypothesis can be rejected\n",
      "-1.356968426245824\n",
      "The null hypothesis can be rejected\n",
      "3.9086252681754066\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "2.985358088288716\n",
      "The null hypothesis can be rejected\n",
      "-5.469768379607016\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "8.484683150329378\n",
      "The null hypothesis can be rejected\n",
      "1.085147786860383\n",
      "The null hypothesis can be rejected\n",
      "-0.19752277327746892\n",
      "The null hypothesis can be rejected\n",
      "-0.19540646400242231\n",
      "The null hypothesis can be rejected\n",
      "-11.350137693395583\n",
      "The null hypothesis can be rejected\n",
      "-10.803383014781412\n",
      "The null hypothesis can be rejected\n",
      "6.827288645565231\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "-1.801218733495958\n",
      "The null hypothesis can be rejected\n",
      "0.4736829751734114\n",
      "The null hypothesis can be rejected\n",
      "-1.2673958159124152\n",
      "The null hypothesis can be rejected\n",
      "-0.9991836486100991\n",
      "The null hypothesis can be rejected\n",
      "-1.510244291652497\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "-0.15354499346566336\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "-6.7043623611013095\n",
      "The null hypothesis can be rejected\n",
      "-23.482646643742633\n",
      "The null hypothesis can be rejected\n",
      "0.9381039668172806\n",
      "The null hypothesis can be rejected\n",
      "0.6593492805299226\n",
      "The null hypothesis can be rejected\n",
      "0.9801735656703824\n",
      "The null hypothesis can be rejected\n",
      "-1.824773303199229\n",
      "The null hypothesis can be rejected\n",
      "4.363691334332785\n",
      "The null hypothesis can be rejected\n",
      "7.3694502650770515\n",
      "The null hypothesis can be rejected\n",
      "5.430732792128845\n",
      "The null hypothesis can be rejected\n",
      "3.8224067792471628\n",
      "The null hypothesis can be rejected\n",
      "4.709130225086085\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "4.303075661720292\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "-72.15665088653053\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "3.305097355019508\n",
      "The null hypothesis can be rejected\n",
      "8.402102204160961\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "3.1683116532762274\n",
      "The null hypothesis can be rejected\n",
      "7.5384081378780525\n",
      "The null hypothesis can be rejected\n",
      "6.0234834589308335\n",
      "The null hypothesis can be rejected\n",
      "3.898561333087972\n",
      "The null hypothesis can be rejected\n",
      "4.367810442718166\n",
      "The null hypothesis can be rejected\n",
      "9.855150778004198\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "0.8554532110001581\n",
      "The null hypothesis can be rejected\n",
      "-2.578207697944365\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "4.3230640491044365\n",
      "The null hypothesis can be rejected\n",
      "3.899662845781629\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis can be rejected\n",
      "5.144006036950101\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n",
      "The null hypothesis cannot be rejected\n"
     ]
    }
   ],
   "source": [
    "transformed, lambdas = transform_boxcox(Lagged_Differenced_Set_offset.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1422b2e-d8e4-43aa-a691-8c54392a6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Difference (only if not using differenced)\n",
    "#transformed = (transformed - transformed.shift(-1)).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72722f-fbff-4bff-92b8-26b19888e415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "8cc90237-f851-4afb-86f8-2dca0b41123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cbd = True\n",
    "ic = True\n",
    "\n",
    "# evaluate an elastic net model on the dataset\n",
    "tsize = .20\n",
    "#train, test = train_test_split(Lagged_Differenced_Set_offset.iloc[:,0:], test_size=tsize, shuffle=False)\n",
    "train, test = train_test_split(transformed.iloc[:,0:], test_size=tsize, shuffle=False)\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=ic)\n",
    "\n",
    "#train_t, lambdas_t = transform_boxcox(train)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    train_t = train\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_t)\n",
    "\n",
    "#\n",
    "train_s = pd.DataFrame(scaler.transform(train_t))\n",
    "train_s.columns = train.columns\n",
    "train_s.index = train.index  \n",
    "\n",
    "train_t = train_s\n",
    "\n",
    "#test_t = transform_boxcox_l(test, lambdas_t)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    test_t = test\n",
    "\n",
    "test_s = pd.DataFrame(scaler.transform(test_t))\n",
    "test_s.columns = test.columns\n",
    "test_s.index = test.index\n",
    "\n",
    "test_t = test_s\n",
    "\n",
    "y_train = pd.DataFrame(train_t.iloc[:,0])\n",
    "\n",
    "#exclude y\n",
    "\n",
    "X_inter_train = pd.DataFrame(interaction.fit_transform(train_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_t.iloc[:,1:]).columns))\n",
    "\n",
    "    #apply ZCA each time a set of factors are removed (i.e. iteratively)\n",
    " #trf = zca.ZCA().fit(X_inter_train)\n",
    "  #trf = zca.ZCA().fit(X_train)\n",
    "\n",
    " #X_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "  #X_train = pd.DataFrame(trf.transform(X_train))\n",
    " #X_train.columns=X_inter_train.columns\n",
    "  #X_train.columns=X_train.columns\n",
    "  #X_train.index = train.index\n",
    "\n",
    "#X_inter_alt = X_train.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#print(X_inter_alt.head(3))\n",
    "\n",
    "y_test = pd.DataFrame(test_t.iloc[:,0])\n",
    "\n",
    "X_inter_test = pd.DataFrame(interaction.fit_transform(test_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(test_t.iloc[:,1:]).columns))\n",
    "\n",
    " #X_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "  #X_test = pd.DataFrame(trf.transform(X_test))\n",
    "\n",
    " #X_test.columns=X_inter_test.columns\n",
    "  #X_test.columns=X_test.columns\n",
    "  #X_test.index = test.index\n",
    "\n",
    "#X_inter_t_alt = X_test.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#X_inter_t_alt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf46d0-1162-46d6-a166-0461b321e0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "\n",
    " #model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=4, verbose=0, precompute='auto')\n",
    "\n",
    "model = ElasticNet()\n",
    "grid = dict()\n",
    "# fit model\n",
    "\n",
    "grid['alpha'] = alphas\n",
    "grid['l1_ratio'] = ratios\n",
    "\n",
    "#search = HalvingRandomSearchCV(model, grid,resource='n_samples',max_resources=10,random_state=0)\n",
    "\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    " #results = search.fit(X_inter_train, y_train)\n",
    "#results = search.fit(X_train, y_train)\n",
    "\n",
    "results = search.fit(X_inter_train, y_train)\n",
    "\n",
    " #model.fit(X_inter_train, y_train)\n",
    "# summarize chosen configuration\n",
    "\n",
    "print('MAE: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n",
    "\n",
    "print(results.best_estimator_)\n",
    "best_model = ElasticNet(alpha=results.best_estimator_.alpha, l1_ratio = results.best_estimator_.l1_ratio)\n",
    "\n",
    "#pd.concat([all_data[Y],all_data_int],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb7965-c50b-4f1e-bd5e-05504a69192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_inter_train,train_t.iloc[:,0])\n",
    "\n",
    "trainScore = best_model.score(X_inter_train, y_train, sample_weight=None)\n",
    "testScore = best_model.score(X_inter_test, y_test, sample_weight=None)\n",
    "print(trainScore)\n",
    "print(testScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb694fa2-0e07-4a01-8648-2d6615a70b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "093a64f9-522d-4761-adf1-f55d566a1b70",
   "metadata": {},
   "source": [
    "pd.DataFrame(best_model.predict(X_inter_test))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b12fd-2b69-4243-909a-6f828aa43b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "0e86999a-16ba-4505-b5ec-843d219ac62e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 724,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "id": "14400790-a56a-4084-8c56-cf73697546e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43 accuracy with a standard deviation of 0.20\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "#works best\n",
    "#class balanced, no zca\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE \n",
    "\n",
    "#pd.DataFrame(Lagged_Set.iloc[:,0].shift(-1)).hist()\n",
    "#Lagged_Differenced_Set = (all_data - all_data.shift(-1)).dropna()\n",
    "#Lagged_Differenced_Set_f = pd.concat([Lagged_Differenced_Set.iloc[:,0].shift(-1),Lagged_Differenced_Set.iloc[:,1:]],axis=1)\n",
    "\n",
    "set_ = pd.DataFrame()\n",
    "set_ = Lagged_Differenced_Set_offset.copy()\n",
    "#set_ = transformed.copy()\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "a = set_.iloc[:,0].loc[X_train.index]\n",
    "b = set_.iloc[:,0].loc[X_test.index]\n",
    "plt.hist(a)\n",
    "plt.show()\n",
    "plt.hist(b)\n",
    "plt.show()\n",
    "print(stats.ttest_ind(a,b, equal_var = False))\n",
    "\n",
    "print(mean(y))\n",
    "\n",
    "X, y = set_.iloc[:,1:],  set_.iloc[:,0]\n",
    "\n",
    "y = pd.DataFrame(np.where(y > mean(y), 1, 0))\n",
    "y.index = set_.index\n",
    "\n",
    "train_size = .7\n",
    "test_size = 1-train_size\n",
    "\n",
    "#test_I = random.sample(list(X.index), int(np.round(test_size*len(X))))\n",
    "#train_I set(test_I) ! set()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
    "\n",
    "one = y_train[y_train==0].dropna().sample(int(len(X_train)*2), replace=True, random_state=1)\n",
    "two = y_train[y_train==1].dropna().sample(int(len(X_train)*2), replace=True, random_state=1)\n",
    "\n",
    "train_index = np.append(one.index,two.index)\n",
    "\n",
    "\n",
    "#destroyed my data\n",
    "sm = SVMSMOTE(random_state=42)\n",
    "    #smote = SMOTE()\n",
    "Xsm_train, ysm_train = sm.fit_sample(np.array(X_train),np.array(y_train))\n",
    "    \n",
    "Xsm_train = X_train.copy()\n",
    "ysm_train = y_train.copy()\n",
    "\n",
    "#df.sample(frac=0.5, replace=True, random_state=1)\n",
    "#class balance\n",
    "#train_index = []\n",
    "#train_index = np.append(one.index,two.index)\n",
    "\n",
    "#train_index = X_train.index\n",
    "\n",
    "#sm_train = sm_train[train_index]\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(Xsm_train)\n",
    "\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(Xsm_train))\n",
    "X_train_transformed.columns = X_train.columns\n",
    "#X_train_transformed.index = \n",
    "\n",
    "X_inter_train = X_train_transformed.copy()\n",
    "\n",
    "#X_inter_train = pd.DataFrame(interaction.fit_transform(X_train_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_train_transformed).columns))\n",
    "\n",
    "trf = zca.ZCA().fit(X_inter_train)\n",
    "\n",
    "#X_inter_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "X_inter_train.columns=pd.DataFrame(X_inter_train).columns\n",
    "#X_inter_train.index = X_train.loc[train_index].index\n",
    "\n",
    "clf = svm.SVC(kernel='rbf', C=1, random_state=42)\n",
    "\n",
    "scores = cross_val_score(clf, X_inter_train, ysm_train, cv=10)\n",
    "\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "clf = svm.SVC(C=1).fit(X_inter_train, ysm_train)\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_transformed.columns = X_test.columns\n",
    "X_test_transformed.index = X_test.index\n",
    "\n",
    "X_inter_test = X_test_transformed\n",
    "\n",
    "#X_inter_test = pd.DataFrame(interaction.fit_transform(X_test_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_test_transformed).columns))\n",
    "\n",
    "#X_inter_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "id": "e560215f-9b7d-4d76-ac61-dbf96d2cfd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "[1 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "predicteds = clf.predict(X_inter_test)\n",
    "#predicted.index = y_test.index\n",
    "\n",
    "print(clf.score(X_inter_test, y_test))\n",
    "\n",
    "#clf.predict(X_test)\n",
    "print(predicteds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "id": "0c39fe4e-d604-4fa9-ad81-ee7a53a06784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.553846\n",
       "dtype: float64"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "3909eb8a-645f-47c5-933a-98d61f3d6ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7692307692307693"
      ]
     },
     "execution_count": 498,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('svc', svm.SVC(kernel='sigmoid', C=1, random_state=42))])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)\n",
    "\n",
    "#search = cross_val_score(pipe, X_inter_train, y_train.loc[train_index], cv=10)\n",
    "#GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "#search.fit(X_digits, y_digits)\n",
    "#print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "#print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b4466-cd31-4f19-85e0-c32db62dbe01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672895b-f442-45b4-8cb1-3257e8c830a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b63d8-9f6e-453e-b896-0ee262d1242e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "id": "6ce0671f-8931-4f7e-936b-51adcb52dab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 23}\n",
      "0.6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#knn\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV#create new a knn model\n",
    "knn2 = KNeighborsClassifier(metric='euclidean',weights='distance')#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "knn_gscv = GridSearchCV(knn2, param_grid, cv=5)#fit model to data\n",
    "knn_gscv.fit(X_inter_train, ysm_train)\n",
    "#check top performing n_neighbors value\n",
    "print(knn_gscv.best_params_)\n",
    "#check mean score for the top performing value of n_neighbors\n",
    "print(knn_gscv.best_score_)\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = knn_gscv.best_params_['n_neighbors'],metric='euclidean',weights='distance')\n",
    "knn.fit(X_inter_train,ysm_train)\n",
    "predictedk = knn.predict(X_inter_test)\n",
    "knn.score(X_inter_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43107e46-54bb-4837-a798-af32efaccadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "id": "e55c7caf-0c3b-4911-ba58-8313a7ba85ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: -0.356\n",
      "Config: {'alpha': 10.0, 'l1_ratio': 0.35000000000000003}\n"
     ]
    }
   ],
   "source": [
    "#logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = (10 ** np.linspace(-1, 1, 10))/10\n",
    "alphas = np.append(alphas,[10.0, 100.0])\n",
    "\n",
    "model = SGDClassifier(loss=\"log\", penalty=\"elasticnet\")\n",
    "grid = dict()\n",
    "# fit model\n",
    "\n",
    "grid['alpha'] = alphas\n",
    "grid['l1_ratio'] = ratios\n",
    "\n",
    "search_l = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "results_l = search_l.fit(X_inter_train, ysm_train)\n",
    "\n",
    " #model.fit(X_inter_train, y_train)\n",
    "# summarize chosen configuration\n",
    "\n",
    "print('MAE: %.3f' % results_l.best_score_)\n",
    "print('Config: %s' % results_l.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b9087-fe22-4d54-b152-a40e48784774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "id": "7015d3fd-f35c-4f6c-b8e9-f66c92514715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDClassifier(alpha=10.0, l1_ratio=0.35000000000000003, loss='log',\n",
      "              penalty='elasticnet')\n",
      "                     0\n",
      "DGS10         0.002531\n",
      "DTB3         -0.012167\n",
      "DGS3MO       -0.012140\n",
      "MORTGAGE30US -0.006606\n",
      "DFII10       -0.020393\n",
      "...                ...\n",
      "UNH           0.010486\n",
      "VZ            0.002110\n",
      "WMT           0.007844\n",
      "WBA           0.002396\n",
      "DIS          -0.008269\n",
      "\n",
      "[90 rows x 1 columns]\n",
      "0.5333333333333333\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "print(results_l.best_estimator_)\n",
    "best_model_l = SGDClassifier(alpha=results_l.best_estimator_.alpha, l1_ratio = results_l.best_estimator_.l1_ratio)\n",
    "\n",
    "#calibrator = CalibratedClassifierCV(best_model_l, cv='prefit')\n",
    "#model=calibrator.fit(X_inter_train, y_train)\n",
    "\n",
    "best_model_l.fit(X_inter_train,ysm_train)\n",
    "\n",
    "#sgd = SGDClassifier()\n",
    "\n",
    "#sgd.partial_fit(X_inter_train,y_train, classes=[0,1])\n",
    "\n",
    "print(pd.DataFrame(np.transpose(best_model_l.coef_)).set_index(X_inter_train.columns))\n",
    "\n",
    "trainScore_l = best_model_l.score(X_inter_train, ysm_train, sample_weight=None)\n",
    "testScore_l = best_model_l.score(X_inter_test, y_test, sample_weight=None)\n",
    "print(trainScore_l)\n",
    "print(testScore_l)\n",
    "\n",
    "#print(help(KNeighborsClassifier))\n",
    "predictedl = best_model_l.predict(X_inter_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "id": "a6dd1212-7817-4f74-95d6-0493d68c9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    0  0  0\n",
      "0   1  0  0\n",
      "1   1  1  0\n",
      "2   0  0  0\n",
      "3   1  1  0\n",
      "4   1  1  0\n",
      "5   1  1  0\n",
      "6   1  1  0\n",
      "7   1  0  0\n",
      "8   1  1  0\n",
      "9   0  0  0\n",
      "10  1  1  0\n",
      "11  1  1  0\n",
      "12  1  1  0\n",
      "13  1  1  0\n",
      "14  1  1  0\n",
      "15  1  0  0\n",
      "16  1  1  0\n",
      "17  1  0  0\n",
      "18  1  1  0\n",
      "19  1  1  0\n",
      "[[3 5]\n",
      " [3 9]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.38      0.43         8\n",
      "           1       0.64      0.75      0.69        12\n",
      "\n",
      "    accuracy                           0.60        20\n",
      "   macro avg       0.57      0.56      0.56        20\n",
      "weighted avg       0.59      0.60      0.59        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#predictedf = pd.concat([pd.DataFrame(predicteds),pd.DataFrame(predictedk),pd.DataFrame(predictedl)],axis=1)\n",
    "predictedf = pd.concat([pd.DataFrame(predicteds),pd.DataFrame(predictedk)],axis=1)\n",
    "print(predictedf)\n",
    "print(confusion_matrix(y_test, predictedf.mode(axis=1)))\n",
    "print(classification_report(y_test, predictedf.mode(axis=1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dfc37-ab80-427f-bc37-edb9a212c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate output multi-step 1d cnn example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Lagged_Differenced_Set = (all_data - all_data.shift(-1)).dropna()\n",
    "\n",
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r-\", label=\"Validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out, xcolumns, ycolumns):\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, xcolumns], sequences[end_ix:out_end_ix, ycolumns]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return array(X), array(y)\n",
    "\n",
    "#n_steps_in = int(np.floor(len(train)*.8))\n",
    "print(len(Lagged_Differenced_Set))\n",
    "n_steps_in = int(np.round(len(Lagged_Differenced_Set)/2))\n",
    "print(len(Lagged_Differenced_Set) - n_steps_in)\n",
    "print(n_steps_in)\n",
    "#n_steps_out = int(len(train)-n_steps_in)\n",
    "n_steps_out = 1\n",
    "print(n_steps_out)\n",
    "\n",
    "#ycolumns = range(0,len(transformed.columns))\n",
    "ycolumns = range(0,len(Lagged_Differenced_Set.columns[0:1].values))\n",
    "xcolumns = range(1,len(Lagged_Differenced_Set.columns[1:]))\n",
    "\n",
    "#trainInner, valid = train_test_split(trainOuter, test_size=0.15, shuffle=False)\n",
    "#trainInner_whitened = pd.DataFrame(trf.transform(trainInner.iloc[:,1:])).set_index(trainInner.index)\n",
    "#valid_whitened = pd.DataFrame(trf.transform(valid.iloc[:,1:])).set_index(valid.index)\n",
    "#test_whitened = pd.DataFrame(trf.transform(test.iloc[:,1:])).set_index(test.index)\n",
    "\n",
    "trainOuter_whitened=pd.DataFrame(trf.transform(trainOuter.iloc[:,1:])).set_index(trainOuter.index)\n",
    "\n",
    "X, y = split_sequences(np.array(Lagged_Differenced_Set), n_steps_in, n_steps_out, xcolumns, ycolumns)\n",
    "\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "    \n",
    "#flatten output\n",
    "\n",
    "if len(ycolumns) > 0:\n",
    "    n_output = y.shape[1] * y.shape[2]\n",
    "    y = y.reshape((y.shape[0], n_output))\n",
    "\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "modelCNN = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(n_output)\n",
    "])\n",
    "\n",
    "# fit model\n",
    "#model.fit(X, y, epochs=7000, verbose=0)\n",
    "\n",
    "epochs_ = 200\n",
    "batch_size_ = 25\n",
    "\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mse')\n",
    "modelCNN.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "X_train_transformed = pd.DataFrame(scaler.transform(X_train))\n",
    "X_train_transformed.columns = X_train.columns\n",
    "\n",
    "X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "X_test_transformed.columns = X_test.columns\n",
    "X_test_transformed.index = X_test.index\n",
    "\n",
    "    #new_series = pd.DataFrame(ts_train_scaled).append(pd.DataFrame(ts_valid_scaled)).append(pd.DataFrame(ts_test_scaled))\n",
    "\n",
    "    #series_reshaped = np.array([new_series[i:i + (n_steps+n_ahead)].copy() for i in range(len(data) - (n_steps+n_ahead))])  \n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                    test_size=0.33,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)    \n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "historyCNN = modelCNN.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history6 = model6.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history = model.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f7e2d-3603-4ba1-8457-4fd9918498a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "plot_learning_curves(historyCNN.history[\"loss\"], history.history[\"val_loss\"])\n",
    "plt.show()\n",
    "#plot_learning_curves(history6.history[\"loss\"], history.history[\"val_loss\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f6d94-9757-4baa-b095-8937fbba5857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4c952-450d-43d4-86c1-2a37673a58d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc3da-fa1f-4e57-9d84-f4e5f8872af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#multi\n",
    "yhat = modelCNN.predict(X_test, verbose=0)\n",
    "\n",
    "predicted = []\n",
    "original = []\n",
    "if len(ycolumns) > 1:\n",
    "    results = yhat.reshape(len(y_test), int(yhat.shape[1]/X_test.shape[2]), len(transformed.columns))\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(pd.DataFrame(results[i])[0][0])\n",
    "        original.append(pd.DataFrame(y_test[i])[0][0])\n",
    "        #print(pd.DataFrame(results[i])[0][0])\n",
    "        #print(pd.DataFrame(y_test[i])[0][0])\n",
    "else:\n",
    "    results = yhat\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(results[i])\n",
    "        original.append(y_test[i])\n",
    "        #print(pd.DataFrame(results[i]))\n",
    "        #print(pd.DataFrame(y_test[i]))\n",
    "        \n",
    "plt.scatter(predicted,original)\n",
    "\n",
    "pd.concat([pd.DataFrame(predicted),pd.DataFrame(original)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229628-eed4-4b76-831d-1de033240347",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(best_model.coef_).set_index(X_inter_train.columns)\n",
    "\n",
    "a_coef = abs(coef)\n",
    "a_coef.sort_values(by=[0],ascending=False,inplace=True)\n",
    "chosen_few = a_coef[a_coef>0].dropna().index.values\n",
    "\n",
    "scaler_ = preprocessing.StandardScaler().fit(transformed)\n",
    "\n",
    "#\n",
    "train_ = pd.DataFrame(scaler_.transform(transformed))\n",
    "train_.columns = transformed.columns\n",
    "train_.index = transformed.index  \n",
    "\n",
    "X_inter_train_ = pd.DataFrame(interaction.fit_transform(train_.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_.iloc[:,1:]).columns))\n",
    "\n",
    "max_pvalue = 1\n",
    "New_Names = X_inter_train.columns\n",
    "X_b = X_inter_train_[chosen_few]\n",
    "while (max_pvalue > .05):\n",
    "        \n",
    "    trf = zca.ZCA().fit(X_b)\n",
    "        \n",
    "    X_b_z = pd.DataFrame(trf.transform(X_b))\n",
    "    X_b_z.columns=pd.DataFrame(X_b).columns\n",
    "    X_b_z.index = train_.index\n",
    "\n",
    "    model_ = sm.OLS(train_.iloc[:,0],sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    #model_ = sm.OLS(pd.DataFrame(pd.concat([train_.iloc[:,0],X_b_z],axis=1),sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    results_ = model_.fit()\n",
    "\n",
    "    set_ = X_b.columns.tolist()\n",
    "    \n",
    "    max_pvalue = max(results_.pvalues[1:])\n",
    "    if (max_pvalue > .05):\n",
    "        print(max_pvalue)\n",
    "        max_pname = (results_.pvalues[1:]).idxmax(axis=1)\n",
    "        set_.remove(max_pname)\n",
    "        New_Names = set_\n",
    "    \n",
    "        X_b = X_inter_train_[New_Names]\n",
    "        X_b.index = X_inter_train_.index\n",
    "\n",
    "#from statsmodels.formula.api import ols\n",
    "#lm = ols(pd.DataFrame(train_.iloc[:,0]) ~ sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip')).fit()\n",
    "#table = sm.stats.anova_lm(model_, type=3)\n",
    "#print(table)\n",
    "print(results_.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e428c-cce9-46e3-b145-754fc9e4c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNaNCols(df_):\n",
    "    for col in df_:        \n",
    "        num_NaNs = df_[col].isnull().sum()\n",
    "        if num_NaNs > 0:\n",
    "            print(f\"Column: {col}\")\n",
    "            print(f\"Number of NaNs: {num_NaNs}\")\n",
    "\n",
    "findNaNCols(X_b_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4e220-4ac2-4945-9e3a-4b8ff1355a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940860-86d6-4768-81a0-63f9127143a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_plot = Plot.LinearRegressionResidualPlot(X_b_z, pd.DataFrame(train_.iloc[:,0]))\n",
    "lm = linear_plot.fit()\n",
    "summary, diag_res = linear_plot.diagnostic_plots(lm)\n",
    "print(\"Summary of Regression\\n:{}\".format(summary))\n",
    "print(\"Diagnostic Tests of Regression\\n:{}\".format(diag_res))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#sns.pairplot(pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1), hue=Y, height=2);\n",
    "\n",
    "pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1).hist()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model_s = sklearn.linear_model.LinearRegression()\n",
    "model_s.fit(X_b_z, pd.DataFrame(train_[Y]))\n",
    "\n",
    "shap.initjs()\n",
    "e = shap.explainers.Linear(model_s, X_b_z)\n",
    "\n",
    "shap_values = e.shap_values(X_b_z)\n",
    "shap.summary_plot(shap_values, X_b_z)\n",
    "shap.plots.heatmap(e(X_b_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b64b8-ca00-46d1-ba16-91f3adb00065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4871c2-4418-491a-8e5c-5776d42e4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f\n",
    "\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "residuals_normalized = lm.get_influence().resid_studentized_internal\n",
    "cooks = lm.get_influence().cooks_distance[0]\n",
    "cooks = np.round(f.pdf(cooks,len(lm.tvalues)+1, len(lm.fittedvalues)-len(lm.tvalues)-1),2)\n",
    "\n",
    "res_std = lm.get_influence().resid_std\n",
    "\n",
    "leverage = lm.get_influence().hat_matrix_diag\n",
    "\n",
    "plt.hist(pd.DataFrame(leverage))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(cooks))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(residuals_normalized))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "testNormal(residuals_normalized)\n",
    "\n",
    "w = res_std\n",
    "x = cooks\n",
    "y = leverage\n",
    "z = residuals_normalized\n",
    "\n",
    "fitted_y = lm.fittedvalues\n",
    "\n",
    "labels_ = fitted_y.index\n",
    "\n",
    "outlier_check = pd.concat([pd.DataFrame(x),pd.DataFrame(y),pd.DataFrame(z),pd.DataFrame(w)],axis=1).set_index(labels_)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres']\n",
    "\n",
    "qq = ProbPlot(residuals_normalized)\n",
    "\n",
    "c_thresh = .1\n",
    "l_thresh = (2*(len(lm.tvalues)-1)/len(lm.fittedvalues))\n",
    "s_thresh = max(qq.theoretical_quantiles)\n",
    "\n",
    "print(\"Outlier threshold's\")\n",
    "print(\"Cooks distance: > .1+\")\n",
    "print(\"Leverage: > \" + str(l_thresh) + \" to \" + str(3 * (len(lm.tvalues)-1)/len(fitted_y)))\n",
    "print(\"Studentized residuals: > \" + str(s_thresh))\n",
    "print()\n",
    "\n",
    "flag = []\n",
    "\n",
    "for i in range(0,len(outlier_check)):\n",
    "    if( (outlier_check.iloc[i][0] >= c_thresh) or (outlier_check.iloc[i][1] >= l_thresh) or (abs(outlier_check.iloc[i][2]) >= s_thresh) ):\n",
    "        print(outlier_check.iloc[i])\n",
    "        print()\n",
    "        flag.append(True)\n",
    "    else:\n",
    "        flag.append(False)\n",
    "\n",
    "outlier_check = pd.concat([outlier_check,pd.DataFrame(flag).set_index(labels_)],axis=1)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres', 'flagged']\n",
    "\n",
    "print(np.flip(np.argsort(cooks), 0))\n",
    "#print(outlier_check)\n",
    "\n",
    "search = outlier_check[outlier_check['flagged']==1].index.to_list()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in search:\n",
    "    v = outlier_check.index.to_list().index(i)\n",
    "    rows.append(v)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2b196-7761-4420-a145-a5411053a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_style(row):\n",
    "\n",
    "    color = 'white'\n",
    "    if bool(row.flagged) == True:\n",
    "        color = 'orange'\n",
    "\n",
    "    return ['background-color: %s' % color]*len(row.values)\n",
    "\n",
    "all_data[set(all_data.columns) & set(New_Names)]\n",
    "\n",
    "outlier_check.style.apply(custom_style, axis=1).apply(custom_style, axis=1).background_gradient(cmap ='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da39b86-0ac2-4470-9457-8e999918d47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b8430-ac88-4358-88c1-e6c5400fe7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = y_train.columns.to_list()\n",
    "df2 = list(set(set(all_data.columns) & set(New_Names)))\n",
    "\n",
    "flattened = [] \n",
    "for sublist in df1,df2: \n",
    "    for val in sublist: \n",
    "        flattened.append(val) \n",
    "\n",
    "all_data.iloc[rows][flattened].style.background_gradient(cmap ='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
