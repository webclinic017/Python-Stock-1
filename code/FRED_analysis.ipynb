{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9b3139d-1ac0-4486-80f0-e54d08259ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from fracdiff import fdiff\n",
    "#import urbangrammar-graphics as ugg\n",
    "from clustergram import Clustergram\n",
    "from concurrent.futures import wait, ALL_COMPLETED\n",
    "from numpy import absolute\n",
    "from numpy import arange\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from pandas_profiling import ProfileReport\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.conversion import localconverter\n",
    "from rpy2.robjects.packages import importr\n",
    "from scipy import stats\n",
    "from scipy.cluster.vq import vq\n",
    "from scipy.spatial.distance import cdist, pdist\n",
    "from scipy.special import boxcox, inv_boxcox\n",
    "from scipy.stats import f\n",
    "from sklearn import preprocessing\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.preprocessing import scale\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import IPython\n",
    "import concurrent.futures\n",
    "import dtale\n",
    "import matplotlib.pyplot as plt_reverse\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import rpy2\n",
    "import rpy2.robjects as ro\n",
    "import scipy\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d897e90-58fb-4a17-8afe-2aa8e7df0358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa42158-3b08-46e1-9d9e-b7cbd99cf723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee726465-0b88-46f7-877f-5aee84616ef2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#if not sys.warnoptions:\n",
    "#    import warnings\n",
    "#    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "wd = os.getcwd()\n",
    "\n",
    "if (os.defpath==\".;C:\\\\bin\"):\n",
    "    os.environ['R_HOME'] = 'C:/Users/User/Documents/R/R-4.1.2'\n",
    "    os.environ['R_LIBS'] = 'C:/Users/User/Documents/R/R-4.1.2/library'\n",
    "    from OLS_LR_DiagnosticPlots.ModelDiagnostics import Plot\n",
    "else:\n",
    "    os.environ['R_HOME'] = '/mnt/distvol/R/4.0.5/lib64/R/'\n",
    "\n",
    "pandas2ri.activate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205d0cf4-d8be-4d70-a00e-49eb0e0bc01a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31771f1a-52e9-4361-8881-12fc3f053b39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78bf8827-4f34-413b-9a4d-bd6565e33372",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def testNormal (x):    \n",
    "    \n",
    "    k2, p = stats.normaltest(x)\n",
    "    alpha = .001\n",
    "    #print(\"p = {:g}\".format(p))    \n",
    "    if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "        #print(p)\n",
    "        #print(alpha)\n",
    "        print(\"The null hypothesis can be rejected\")\n",
    "        xt, _ = stats.yeojohnson(x)\n",
    "        #xt, _ = stats.boxcox(x)        \n",
    "        print(_)\n",
    "        xt = pd.DataFrame(xt)\n",
    "        \n",
    "        return _, pd.DataFrame(xt).set_index(x.index)\n",
    "    else:\n",
    "        print(\"The null hypothesis cannot be rejected\")    \n",
    "        return 1, pd.DataFrame(x)\n",
    "\n",
    "def inverse_boxcox (data, lambdas):\n",
    "    power = PowerTransformer(method='yeo-johnson')\n",
    "    power.lambdas_ = lambdas.values\n",
    "    return(power.inverse_transform([data]))\n",
    "    #return inv_boxcox(data, lambdas.values)\n",
    "    \n",
    "def transform_boxcox_l(data, l_):\n",
    "    transformed = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        #print(i)\n",
    "        if l_.iloc[i].values == 1:\n",
    "            inner_scale = data.iloc[:,i]            \n",
    "        else:\n",
    "            inner_scale = pd.DataFrame(stats.yeojohnson((data.iloc[:,i]), lmbda=l_.iloc[i].values))\n",
    "            \n",
    "        inner_scale.index = data.index\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed\n",
    "\n",
    "def transform_boxcox (data):\n",
    "    transformed = pd.DataFrame()\n",
    "    transformed_lambdas = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data.columns)):\n",
    "        l, inner_scale = testNormal(data.iloc[:,i])\n",
    "        inner_scale.set_index(data.index)\n",
    "\n",
    "        transformed_lambdas = pd.concat([transformed_lambdas,pd.DataFrame(pd.Series(l))],axis=0)\n",
    "        transformed = pd.concat([transformed,inner_scale],axis=1)\n",
    "        \n",
    "    transformed.columns = data.columns\n",
    "    return transformed, transformed_lambdas\n",
    "\n",
    "def inverse_yeo(og, data_, lambda_):\n",
    "    values = []\n",
    "    for i in range(0,len(og)):\n",
    "        X = og[i]\n",
    "        X_trans = data_[i]\n",
    "        if X >= 0 and lambda_ == 0:\n",
    "            X = exp(X_trans) - 1\n",
    "        elif X >= 0 and lambda_ != 0:\n",
    "            X = (X_trans * lambda_ + 1) ** (1 / lambda_) - 1\n",
    "        elif X < 0 and lambda_ != 2:\n",
    "            X = 1 - (-(2 - lambda_) * X_trans + 1) ** (1 / (2 - lambda_))\n",
    "        elif X < 0 and lambda_ == 2:\n",
    "            X = 1 - exp(-X_trans)\n",
    "        \n",
    "        values.append(X)\n",
    "    return(pd.DataFrame(values))\n",
    "\n",
    "\n",
    "def revert_yeo (og, data_, lambdas):\n",
    "    reverted = pd.DataFrame()\n",
    "\n",
    "    for i in range(0,len(data_.columns)):        \n",
    "        if lambdas.iloc[i].values == 1 :\n",
    "            revert = data_.iloc[:,i]\n",
    "        else:\n",
    "            p#ower = PowerTransformer(method='yeo-johnson')\n",
    "            #power.lambdas_ = lambdas.iloc[i].values\n",
    "            #revert = pd.DataFrame(power.inverse_transform([data.iloc[:,i].values]))\n",
    "            #return inv_boxcox(data, lambdas.values)\n",
    "            revert = pd.DataFrame(inverse_yeo(og.iloc[:,i].values,data_.iloc[:,i].values, lambdas.iloc[i].values))            \n",
    "        revert.index = data_.index\n",
    "        reverted = pd.concat([reverted,revert],axis=1)\n",
    "        \n",
    "    reverted.columns = data_.columns\n",
    "    return reverted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d53ac0c-299b-4f11-b9c1-38834b5d088e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ec9d431-186e-4171-8a8a-378ab2ab2e53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_deltas(data):\n",
    "\n",
    "    R.r('''\n",
    "               f <- function(values) {\n",
    "                        #system(\"which openssl\")\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              \n",
    "                              r = residuals(varvefd, reg = TRUE)\n",
    "                              #print(r)\n",
    "                              return(d)\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f = R.globalenv['f']\n",
    "    d=R.conversion.rpy2py((r_f(R.conversion.py2rpy(data.dropna()))))\n",
    "    return(d)\n",
    "\n",
    "def get_residuals(data):\n",
    "\n",
    "    R.r('''\n",
    "               f <- function(values) {\n",
    "                        #system(\"which openssl\")\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              \n",
    "                              #return(d)\n",
    "                              r = residuals(varvefd, reg = TRUE)\n",
    "                              #print(r)\n",
    "                              return(r)\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f = R.globalenv['f']\n",
    "    d=R.conversion.rpy2py((r_f(R.conversion.py2rpy(data.dropna()))))\n",
    "    return(d)\n",
    "\n",
    "def arfima_predict(data_, ahead):\n",
    "\n",
    "    R.r('''\n",
    "               f_ <- function(values, ahead) {\n",
    "                        #system(\"which openssl\")\n",
    "                        print(nrow(values))\n",
    "\n",
    "                        library(snpEnrichment)\n",
    "                        library(arfima)\n",
    "                        library(parallel)\n",
    "                        library(forecast)                    \n",
    "\n",
    "                        dset <- lapply(1:ncol(values),function(x)\n",
    "                        {\n",
    "                            column = values[,x,drop=FALSE]\n",
    "\n",
    "\n",
    "                            #tryCatch(invisible(capture.output(suppressMessages(suppressWarnings(\n",
    "                            #{\n",
    "                              varvefd = arfima(column)\n",
    "                              d = summary(varvefd)$coef[[1]][1]\n",
    "                              \n",
    "                              #return(d)\n",
    "                              r = residuals(varvefd, reg = TRUE)\n",
    "                              #print(r)\n",
    "                              #return(r)\n",
    "                              \n",
    "                              pred <- predict(varvefd, ahead)\n",
    "                              #print(ahead)\n",
    "                              #print(pred)\n",
    "                              #print(pred)\n",
    "                              return(as.data.frame(pred)[,1,drop=TRUE])\n",
    "                            #}\n",
    "                           #)\n",
    "                           #))),\n",
    "                            #error=function(e)\n",
    "                              #{\n",
    "                                #d = 1\n",
    "                                #return(d)\n",
    "                              #})\n",
    "\n",
    "                        })    \n",
    "\n",
    "                        unlist(dset)\n",
    "\n",
    "                }\n",
    "                ''')\n",
    "\n",
    "    r_f_ = R.globalenv['f_']\n",
    "    d_=R.conversion.rpy2py((r_f_(R.conversion.py2rpy(data_),ahead)))\n",
    "    return(d_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8191525a-36ba-428a-96fb-0dbf4ac4d375",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e43598e7-17dd-4ced-b588-e8870d3df933",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def arima_adjust(y_train, y_test, train_predicted, test_predicted):\n",
    "    \n",
    "    #arima residual adjusted\n",
    "    \n",
    "    window = 15\n",
    "    train_resid = y_train - pd.DataFrame(train_predicted).set_index(y_train.index)\n",
    "\n",
    "    model = AutoReg(train_resid.values, lags=15)\n",
    "    model_fit = model.fit()\n",
    "    coef = model_fit.params\n",
    "    # walk forward over time steps in test\n",
    "    history = train_resid[len(train_resid)-window:]\n",
    "    history = [history.loc[history.index[i]] for i in range(len(history))]\n",
    "    predictions = list()\n",
    "    for t in range(len(y_test)):\n",
    "        # persistence\n",
    "        yhat = pd.DataFrame(test_predicted).set_index(y_test.index).loc[y_test.index[t]]    \n",
    "        error = y_test.loc[y_test.index[t]] - yhat\n",
    "        # predict error\n",
    "        length = len(history)\n",
    "        lag = [history[i] for i in range(length-window,length)]\n",
    "        pred_error = coef[0]\n",
    "        for d in range(window):\n",
    "            pred_error += coef[d+1] * lag[window-d-1]\n",
    "        # correct the prediction\n",
    "        yhat = yhat + pred_error\n",
    "        #print(yhat)\n",
    "        predictions.append(np.round(yhat,0).astype(int))\n",
    "        history.append(error)\n",
    "        #print('predicted=%f, expected=%f' % ((np.round(yhat,0)), y_test.loc[y_test.index[t]]))\n",
    "\n",
    "    print(metrics.classification_report(y_test,predictions))\n",
    "    print(metrics.confusion_matrix(y_test,predictions))\n",
    "    \n",
    "    return(pd.DataFrame(predictions).set_index(y_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3af9904-08a5-4d5f-90f7-1b4c5d4652df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb9bb80-d23a-4a60-964b-41dad96d6411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1235085d-f98a-4b9d-a118-29a6f154af93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom statsmodels.tsa.ar_model import AutoReg\\n\\nresiduals_train = y_train - pd.DataFrame(clf.predict(X_train)).set_index(y_train.index)\\n\\ny_adjusted = predicteds.copy()\\n\\nfor i in range(0,len(y_test)):    \\n    \\n    if i == 0:\\n        forecast_r = arfima_predict(residuals_train.values)\\n        \\n    else:\\n        residuals_test = y_test.loc[y_test.index[0:i]] - pd.DataFrame(predicteds).set_index(y_test.index).loc[y_test.index[0:i]]\\n        \\n        residuals = pd.concat([residuals_train,residuals_test],axis=0)\\n        \\n        forecast_r = arfima_predict(residuals.values)\\n        \\n    print(forecast_r)\\n    adjusted = pd.DataFrame(y_adjusted).set_index(y_test.index).loc[y_test.index[i]]\\n    #print(adjusted)\\n    \\n    \\n    \\n    #y_test.loc[0:i]-pd.DataFrame(predicteds).set_index(y_test.loc[0:i].index)\\n    #ar_s_model = AutoReg(residuals, lags=15)\\n    #model_fit = model.fit()\\n    #plt.plot(np.round(arfima_predict(residuals.values, len(y_test)),0))\\n\\n    #improved_forecast = forecast + estimated error\\n\\n    #print('Coef=%s' % (model_fit.params))\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#arfima residual adjusted\n",
    "def arfima_adjust(y_train, y_test, train_predicted, test_predicted):\n",
    "    \n",
    "    #train_resid = get_deltas(y_train)\n",
    "    train_resid = y_train - pd.DataFrame(train_predicted).set_index(y_train.index)\n",
    "\n",
    "    y_adjusted = predicted.copy()\n",
    "\n",
    "    forecast_r = arfima_predict(train_resid.values, len(y_test))  \n",
    "    print(len(forecast_r))\n",
    "    \n",
    "    # walk forward over time steps in test\n",
    "    #history = train_resid[len(train_resid):-1]\n",
    "    #history = [history.loc[history.index[i]] for i in range(len(history))]\n",
    "    predictions = list()\n",
    "    for t in range(len(y_test)):\n",
    "        # persistence\n",
    "        yhat = pd.DataFrame(test_predicted).set_index(y_test.index).loc[y_test.index[t]]\n",
    "        #error = y_test.loc[y_test.index[t]] - yhat\n",
    "        # predict error\n",
    "        #length = len(history)\n",
    "        #lag = [history[i] for i in range(length-window,length)]\n",
    "        #pred_error = coef[0]\n",
    "        #for d in range(window):\n",
    "            #pred_error += coef[d+1] * lag[window-d-1]\n",
    "        # correct the prediction\n",
    "        #print(yhat)\n",
    "        #print(pd.DataFrame(forecast_r).set_index(y_test.index).loc[y_test.index[t]] )\n",
    "        yhat = yhat + pd.DataFrame(forecast_r).set_index(y_test.index).loc[y_test.index[t]] \n",
    "        #print(yhat)\n",
    "        predictions.append(np.round(yhat,0).astype(int))\n",
    "        #history.append(error)\n",
    "        #print('predicted=%f, expected=%f' % ((np.round(yhat,0)), y_test.loc[y_test.index[t]]))\n",
    "\n",
    "    print(metrics.classification_report(y_test,predictions))\n",
    "    print(metrics.confusion_matrix(y_test,predictions))\n",
    "    \n",
    "    return(pd.DataFrame(predictions).set_index(y_test.index))\n",
    "\n",
    "'''\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "residuals_train = y_train - pd.DataFrame(clf.predict(X_train)).set_index(y_train.index)\n",
    "\n",
    "y_adjusted = predicteds.copy()\n",
    "\n",
    "for i in range(0,len(y_test)):    \n",
    "    \n",
    "    if i == 0:\n",
    "        forecast_r = arfima_predict(residuals_train.values)\n",
    "        \n",
    "    else:\n",
    "        residuals_test = y_test.loc[y_test.index[0:i]] - pd.DataFrame(predicteds).set_index(y_test.index).loc[y_test.index[0:i]]\n",
    "        \n",
    "        residuals = pd.concat([residuals_train,residuals_test],axis=0)\n",
    "        \n",
    "        forecast_r = arfima_predict(residuals.values)\n",
    "        \n",
    "    print(forecast_r)\n",
    "    adjusted = pd.DataFrame(y_adjusted).set_index(y_test.index).loc[y_test.index[i]]\n",
    "    #print(adjusted)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #y_test.loc[0:i]-pd.DataFrame(predicteds).set_index(y_test.loc[0:i].index)\n",
    "    #ar_s_model = AutoReg(residuals, lags=15)\n",
    "    #model_fit = model.fit()\n",
    "    #plt.plot(np.round(arfima_predict(residuals.values, len(y_test)),0))\n",
    "\n",
    "    #improved_forecast = forecast + estimated error\n",
    "\n",
    "    #print('Coef=%s' % (model_fit.params))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c789bcbc-a66e-422f-89d4-8e2e02089af4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a133da3-b0fd-4741-a0aa-db8cc75f878d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_weights(d, num_k):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    \n",
    "    for k in range(1, num_k):\n",
    "        w_k = np.append(w_k, -w_k[-1] * ((d - k + 1)) / k)\n",
    "        \n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def get_weights_floored(d, num_k, floor=1e-3):\n",
    "    r\"\"\"Calculate weights ($w$) for each lag ($k$) through\n",
    "    $w_k = -w_{k-1} \\frac{d - k + 1}{k}$ provided weight above a minimum value\n",
    "    (floor) for the weights to prevent computation of weights for the entire\n",
    "    time series.\n",
    "    \n",
    "    Args:\n",
    "        d (int): differencing value.\n",
    "        num_k (int): number of lags (typically length of timeseries) to calculate w.\n",
    "        floor (float): minimum value for the weights for computational efficiency.\n",
    "    \"\"\"\n",
    "    w_k = np.array([1])\n",
    "    k = 1\n",
    "    \n",
    "    while k < num_k:\n",
    "        w_k_latest = -w_k[-1] * ((d - k + 1)) / k\n",
    "        if abs(w_k_latest) <= floor:\n",
    "            break\n",
    "\n",
    "        w_k = np.append(w_k, w_k_latest)\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    w_k = w_k.reshape(-1, 1) \n",
    "    \n",
    "    return w_k\n",
    "\n",
    "def frac_diff(df, d, floor=1e-3):\n",
    "    r\"\"\"Fractionally difference time series via CPU.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): dataframe of raw time series values.\n",
    "        d (float): differencing value from 0 to 1 where > 1 has no FD.\n",
    "        floor (float): minimum value of weights, ignoring anything smaller.\n",
    "    \"\"\"\n",
    "    # Get weights window\n",
    "    weights = get_weights_floored(d=d, num_k=len(df), floor=floor)\n",
    "    weights_window_size = len(weights)\n",
    "    \n",
    "    # Reverse weights\n",
    "    weights = weights[::-1]\n",
    "    \n",
    "    # Blank fractionally differenced series to be filled\n",
    "    df_fd = []\n",
    "\n",
    "    # Slide window of time series, to calculated fractionally differenced values\n",
    "    # per window\n",
    "    for idx in range(weights_window_size, df.shape[0]):\n",
    "        # Dot product of weights and original values\n",
    "        # to get fractionally differenced values\n",
    "        date_idx = df.index[idx]\n",
    "        df_fd.append(np.dot(weights.T, df.iloc[idx - weights_window_size:idx]).item())\n",
    "    \n",
    "    # Return FD values and weights\n",
    "    df_fd = pd.DataFrame(df_fd)\n",
    "    \n",
    "    return df_fd, weights\n",
    "\n",
    "def quarterly_annual_return(values):\n",
    "    return((1 + values)**4 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222e53d3-2864-4d69-a45a-ce48217e4fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d949ef06-ae96-4828-b02b-21a92a11b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxl = 5\n",
    "train_size = .7\n",
    "t_size = 1-train_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e6233b-8abf-4f78-83f5-a3e7a2e26fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if (os.defpath==\".;C:\\\\bin\"):\n",
    "    all_data = pd.read_csv(re.sub('code', 'data', wd)+\"\\combined_set.csv\",parse_dates=['Date'])\n",
    "    deltas = pd.read_csv(re.sub('code', 'data', wd)+\"\\deltas.csv\",parse_dates=['Date'])\n",
    "else:\n",
    "    all_data = pd.read_csv('/mnt/distvol/combined_set.csv',parse_dates=['Date'])\n",
    "    deltas = pd.read_csv('/mnt/distvol/deltas.csv',parse_dates=['Date'])\n",
    "        \n",
    "all_data.index = all_data.iloc[:,0]\n",
    "deltas.index = deltas.iloc[:,0]\n",
    "\n",
    "#compare = 'SPY'\n",
    "compare = '^SP500TR'\n",
    "#compare = 'T10Y3M'\n",
    "#compare = 'T10Y2Y'\n",
    "target = \"MSPUS\"\n",
    "\n",
    "#why did I do this?, to zero it out\n",
    "#deltas[target] = deltas[target].diff(1).copy()\n",
    "\n",
    "deltas = deltas.dropna().copy()\n",
    "#target = '^SP500TR'\n",
    "#target = pd.DataFrame(vetted_symbols).sample(n=1).values[0][0]\n",
    "#target = etf_metals[0]\n",
    "#target = crypto[1]\n",
    "#target = '^GSPC'\n",
    "\n",
    "pd.concat([deltas[compare].pct_change(),deltas[target].pct_change()],axis=1).dropna().corr()\n",
    "\n",
    "all_data = all_data.iloc[:,1:]\n",
    "\n",
    "filter_ = all_data.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7adfd73d-0eed-4b7a-8e80-411de637ebb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2007-03-31', '2008-06-30', '2009-09-30', '2010-12-31',\n",
       "               '2012-03-31', '2013-06-30', '2014-09-30', '2015-12-31',\n",
       "               '2017-03-31', '2018-06-30', '2019-09-30', '2020-12-31'],\n",
       "              dtype='datetime64[ns]', name='Date', freq=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.index[np.arange(0, len(all_data.index), 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2fd162ef-f682-48fa-9514-37d7ce7d7bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSPUS 1.5722610722610724\n",
      "^SP500TR 4.188325752194344\n",
      "MSPUS 0.06614916024382911\n",
      "^SP500TR 0.11808912344215372\n"
     ]
    }
   ],
   "source": [
    "plt.plot(pd.concat([deltas[compare].cumsum(),deltas[target].cumsum()],axis=1),label=[compare,target])\n",
    "#plt.xticks(np.arange(0, len(deltas), 5),rotation = 45)\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "\n",
    "quarters = len(all_data.index)\n",
    "\n",
    "target_total_return = all_data[target][-1]/all_data[target][0]\n",
    "print(target,target_total_return)\n",
    "compare_total_return = all_data[compare][-1]/all_data[compare][0]\n",
    "print(compare,compare_total_return)\n",
    "\n",
    "target_annualized_return = ((1 + target_total_return)**(4/quarters))-1\n",
    "compare_annualized_return = ((1 + compare_total_return)**(4/quarters))-1\n",
    "\n",
    "print(target,target_annualized_return)\n",
    "print(compare,compare_annualized_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a0af0e-3462-4594-be5f-452297533220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "27fea3c4-a7c7-4928-9bc4-e9f1cb892076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2007-03-31 00:00:00\n",
      "2021-09-30 00:00:00\n",
      "90% range and center of 3 year moving average over time span, annualized return\n",
      "^SP500TR [0.04126413 0.10077235 0.11469634 0.14016705 0.15797676 0.18482247\n",
      " 0.20877366]\n",
      "MSPUS [-0.00365503  0.01324764  0.02279176  0.04623384  0.06384595  0.07062792\n",
      "  0.08691822]\n"
     ]
    }
   ],
   "source": [
    "print(all_data.index[0])\n",
    "print(all_data.index[-1])\n",
    "axes = plt.axes()\n",
    "\n",
    "print(\"90% range and center of 3 year moving average over time span, annualized return\")\n",
    "\n",
    "range_=[.02,.09,.25,.5,.75,.91,.98]\n",
    "#[.01,.05,.10,.25,.33,.5,.66,.75,.90,.95,.99]\n",
    "\n",
    "MA_compare = deltas[compare].rolling(window=12).mean()\n",
    "MA_compare_quantiles = np.quantile([MA_compare.dropna()],range_)\n",
    "print(compare,quarterly_annual_return(MA_compare_quantiles))\n",
    "\n",
    "MA_target = deltas[target].rolling(window=12).mean()\n",
    "MA_target_quantiles = np.quantile([MA_target.dropna()],range_)\n",
    "print(target,quarterly_annual_return(MA_target_quantiles))\n",
    "\n",
    "MA_M30US = all_data['MORTGAGE30US'][deltas[compare].index].rolling(window=12).mean()/100\n",
    "MA_FFUNDS = all_data['FEDFUNDS'][deltas[compare].index].rolling(window=12).mean()/100\n",
    "\n",
    "MA_M30US_quantiles = np.quantile([MA_M30US.dropna()],range_)\n",
    "MA_FFUNDS_quantiles = np.quantile([MA_FFUNDS.dropna()],range_)\n",
    "\n",
    "axes.set_ylim([0, max(pd.concat([quarterly_annual_return(MA_target.dropna()),quarterly_annual_return(MA_compare.dropna())],axis=0))])\n",
    "\n",
    "x = range(0,len(deltas[compare]))\n",
    "m, b = np.polyfit(x, deltas[compare].dropna(), 1)\n",
    "plt.plot(pd.DataFrame(quarterly_annual_return(m*x + b)).set_index(deltas[compare].dropna().index),c=\"blue\",label=compare)\n",
    "plt.scatter(deltas[compare].index,quarterly_annual_return(deltas[compare]),c=\"blue\")\n",
    "plt.plot(quarterly_annual_return(MA_compare),c=\"blue\")\n",
    "plt.hlines(quarterly_annual_return(deltas[compare].mean()),xmin=deltas.index[0],xmax=deltas.index[-1],color=\"blue\")\n",
    "\n",
    "x = range(0,len(deltas[target]))\n",
    "m, b = np.polyfit(x, deltas[target].dropna(), 1)\n",
    "plt.plot(pd.DataFrame(quarterly_annual_return(m*x + b)).set_index(deltas[target].dropna().index),c=\"orange\",label=target)\n",
    "plt.scatter(deltas[target].index,quarterly_annual_return(deltas[target]),c=\"orange\")\n",
    "plt.plot(quarterly_annual_return(MA_target),c=\"orange\")\n",
    "plt.hlines(quarterly_annual_return(deltas[target].mean()),xmin=deltas.index[0],xmax=deltas.index[-1],color=\"orange\")\n",
    "\n",
    "plt.plot(all_data['MORTGAGE30US'][deltas[compare].index]/100,label='MORTGAGE30US')\n",
    "\n",
    "plt.plot(all_data['FEDFUNDS'][deltas[compare].index]/100,label='FEDFUNDS')\n",
    "\n",
    "#plt.xticks(np.arange(0, len(all_data), 5),rotation = 45)\n",
    "plt.legend(loc=2)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb3fbca-2fef-4c87-8608-23b53bb247ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8fecbd3-b193-42d7-a8f2-efd34a59751d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2011-06-30 00:00:00 - 2021-09-30 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(MA_compare.dropna().index[0],\"-\",MA_compare.dropna().index[-1])\n",
    "\n",
    "ranged_data = pd.concat([pd.DataFrame(quarterly_annual_return(MA_compare_quantiles)),pd.DataFrame(quarterly_annual_return(MA_target_quantiles)),pd.DataFrame(MA_M30US_quantiles),pd.DataFrame(MA_FFUNDS_quantiles)],axis=1)\n",
    "ranged_data.columns= [compare,target,'M30US','FFUNDS']\n",
    "ranged_data.index = range_\n",
    "\n",
    "plt.plot(ranged_data,label=ranged_data.columns)\n",
    "plt.scatter(ranged_data.index,ranged_data[compare])\n",
    "plt.scatter(ranged_data.index,ranged_data[target])\n",
    "plt.scatter(ranged_data.index,ranged_data['M30US'])\n",
    "plt.scatter(ranged_data.index,ranged_data['FFUNDS'])\n",
    "plt.hlines(np.median(ranged_data[compare]),xmin=ranged_data.index[0],xmax=ranged_data.index[-1],color=\"blue\")\n",
    "plt.hlines(np.median(ranged_data[target]),xmin=ranged_data.index[0],xmax=ranged_data.index[-1],color=\"orange\")\n",
    "plt.hlines(np.median(ranged_data['M30US']),xmin=ranged_data.index[0],xmax=ranged_data.index[-1],color=\"green\")\n",
    "plt.hlines(np.median(ranged_data['FFUNDS']),xmin=ranged_data.index[0],xmax=ranged_data.index[-1],color=\"red\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b059908b-4d34-4574-a1c1-05f98f3057d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_data.melt(ignore_index=False).reset_index()['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4d80c7-99c2-4070-b6f4-1b19d28520f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"C:/Users/User/Documents/R/R-4.1.2/library\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#robjects.ro['version']\n",
    "\n",
    "base = importr('base')\n",
    "#grdevices = importr('grDevices')\n",
    "print(base._libPaths())\n",
    "\n",
    "timetk = importr('timetk')\n",
    "magrittr = importr('magrittr')\n",
    "dplyr = importr('dplyr')\n",
    "tidyverse = importr('tidyverse')\n",
    "nbclust = importr('NbClust')\n",
    "grdevices = importr('grDevices')\n",
    "\n",
    "with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "      r_from_pd_df = ro.conversion.py2rpy(all_data.melt(ignore_index=False).reset_index())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98d62cec-5870-47f5-a494-03afd87dabc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ts_frequency            286.0\n",
       "ts_nperiods             286.0\n",
       "ts_seasonal_period      286.0\n",
       "ts_trend                286.0\n",
       "ts_spike                286.0\n",
       "ts_linearity            286.0\n",
       "ts_curvature            286.0\n",
       "ts_e_acf1               286.0\n",
       "ts_e_acf10              286.0\n",
       "ts_seasonal_strength    286.0\n",
       "ts_peak                 286.0\n",
       "ts_trough               286.0\n",
       "ts_entropy              286.0\n",
       "ts_x_acf1               286.0\n",
       "ts_x_acf10              286.0\n",
       "ts_diff1_acf1           286.0\n",
       "ts_diff1_acf10          286.0\n",
       "ts_diff2_acf1           286.0\n",
       "ts_diff2_acf10          286.0\n",
       "ts_seas_acf1            286.0\n",
       "ts_my_mean              286.0\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11283e1f-d042-4b61-af9d-6f295e485dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;246m# A tibble: 286 x 22\u001b[39m\n",
      "   variable        ts_frequency ts_nperiods ts_seasonal_period ts_trend ts_spike\n",
      "   \u001b[3m\u001b[38;5;246m<chr>\u001b[39m\u001b[23m                  \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m       \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m              \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m    \u001b[3m\u001b[38;5;246m<dbl>\u001b[39m\u001b[23m\n",
      "\u001b[38;5;250m 1\u001b[39m T10YIE                     4           1                  4    0.848  4.65\u001b[38;5;246me\u001b[39m\u001b[31m-5\u001b[39m\n",
      "\u001b[38;5;250m 2\u001b[39m WDFUELLA                   4           1                  4    0.936  4.75\u001b[38;5;246me\u001b[39m\u001b[31m-6\u001b[39m\n",
      "\u001b[38;5;250m 3\u001b[39m DTWEXBGS                   4           1                  4    0.985  1.65\u001b[38;5;246me\u001b[39m\u001b[31m-7\u001b[39m\n",
      "\u001b[38;5;250m 4\u001b[39m GASREGW                    4           1                  4    0.925  7.03\u001b[38;5;246me\u001b[39m\u001b[31m-6\u001b[39m\n",
      "\u001b[38;5;250m 5\u001b[39m DGS2                       4           1                  4    0.985  4.48\u001b[38;5;246me\u001b[39m\u001b[31m-7\u001b[39m\n",
      "\u001b[38;5;250m 6\u001b[39m CPALTT01USQ657N            4           1                  4    0.548  1.64\u001b[38;5;246me\u001b[39m\u001b[31m-4\u001b[39m\n",
      "\u001b[38;5;250m 7\u001b[39m PAYEMS                     4           1                  4    0.961  1.17\u001b[38;5;246me\u001b[39m\u001b[31m-5\u001b[39m\n",
      "\u001b[38;5;250m 8\u001b[39m MSPUS                      4           1                  4    0.994  2.34\u001b[38;5;246me\u001b[39m\u001b[31m-8\u001b[39m\n",
      "\u001b[38;5;250m 9\u001b[39m ASPUS                      4           1                  4    0.989  6.84\u001b[38;5;246me\u001b[39m\u001b[31m-8\u001b[39m\n",
      "\u001b[38;5;250m10\u001b[39m IRLTLT01USM156N            4           1                  4    0.970  5.81\u001b[38;5;246me\u001b[39m\u001b[31m-7\u001b[39m\n",
      "\u001b[38;5;246m# ... with 276 more rows, and 16 more variables: ts_linearity <dbl>,\u001b[39m\n",
      "\u001b[38;5;246m#   ts_curvature <dbl>, ts_e_acf1 <dbl>, ts_e_acf10 <dbl>,\u001b[39m\n",
      "\u001b[38;5;246m#   ts_seasonal_strength <dbl>, ts_peak <dbl>, ts_trough <dbl>,\u001b[39m\n",
      "\u001b[38;5;246m#   ts_entropy <dbl>, ts_x_acf1 <dbl>, ts_x_acf10 <dbl>, ts_diff1_acf1 <dbl>,\u001b[39m\n",
      "\u001b[38;5;246m#   ts_diff1_acf10 <dbl>, ts_diff2_acf1 <dbl>, ts_diff2_acf10 <dbl>,\u001b[39m\n",
      "\u001b[38;5;246m#   ts_seas_acf1 <dbl>, ts_my_mean <dbl>\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ro.X11()\n",
    "#ro.windows()\n",
    "ro.r('''\n",
    "\n",
    "my_mean <- function(x, na.rm=TRUE) {\n",
    "  mean(x, na.rm = na.rm)\n",
    "}\n",
    "\n",
    "f <- function(y) {\n",
    "#print(y)\n",
    "\n",
    "#library(arfima)\n",
    "#varvefd = arfima(y)\n",
    "#d = summary(varvefd)$coef[[1]][1]\n",
    "#return(d)\n",
    "\n",
    "tsfeature_tbl <- y %>%\n",
    "group_by(variable) %>%\n",
    "tk_tsfeatures(\n",
    "  .date_var = Date,\n",
    "  .value    = value,\n",
    "  .period   = 4,\n",
    "  .features = c(\"frequency\", \"stl_features\", \"entropy\", \"acf_features\", \"my_mean\"),\n",
    "  .scale    = TRUE,\n",
    "  .prefix   = \"ts_\"\n",
    ") %>%\n",
    "ungroup()\n",
    "    \n",
    "print(tsfeature_tbl)\n",
    "\n",
    "set.seed(123)\n",
    "\n",
    "cluster_tbl <- tibble(\n",
    "    cluster = tsfeature_tbl %>% \n",
    "        select(-variable) %>%\n",
    "        as.matrix() %>%\n",
    "        kmeans(centers = 3, nstart = 100) %>%\n",
    "        pluck(\"cluster\")\n",
    ") %>%\n",
    "    bind_cols(\n",
    "        tsfeature_tbl\n",
    "    )\n",
    "\n",
    "cluster_tbl\n",
    "\n",
    "cluster_tbl %>%\n",
    "    select(cluster, variable) %>%\n",
    "    right_join(y, by = \"variable\") %>%\n",
    "    group_by(variable) %>%\n",
    "    plot_time_series(\n",
    "      Date, value, \n",
    "      .color_var   = cluster, \n",
    "      .facet_ncol  = 2, \n",
    "      .interactive = FALSE\n",
    "    )\n",
    "plot(cluster_tbl)\n",
    "\n",
    "return(tsfeature_tbl)\n",
    "}\n",
    "''')\n",
    "grdevices.png(file=\"temp.png\", width=4096, height=1024)\n",
    "r_f = ro.globalenv['f']\n",
    "d=(r_f(r_from_pd_df))\n",
    "#rprint(pp)\n",
    "\n",
    "time.sleep(15)\n",
    "#grdevices.dev_copy(device = r.png, filename = \"plot.png\", width = 1000, height = 500)\n",
    "grdevices.dev_off()\n",
    "\n",
    "#From here optional, if you want a waiting time\n",
    "#Elsewise close the plot manually afterwards with grdevices.dev_off()\n",
    "\n",
    "#grdevices.dev_off()\n",
    "#grdevices.dev_off()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afc5dfbd-7731-469a-82f3-8671485be163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAACAAAAAIAAQMAAAAsGR49AAAAA1BMVEX///+nxBvIAAAAlElEQVR4nO3BAQEAAACAkP6v7ggKAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAGoCHgABRYr8gAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=P size=2048x512 at 0x2449DC0E100>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "path=r\"temp.png\"\n",
    "display(Image.open(path))\n",
    "\n",
    "#print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168f13bd-1dc6-4d1b-905f-878bcc96b30a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea610df0-f15b-4243-8bfd-88e99775f655",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Training, Holdout = split_sequences(np.array(pd.DataFrame(deltas.index.strftime('%Y-%m-%d'))), 1009, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d71edfb-170e-44bd-8797-a6472f681c0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefdfeb3-8066-41b8-aae1-ab07a7ac8b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, valid = train_test_split(deltas.index,  test_size=0.3, random_state=0, shuffle=False)\n",
    "#valid, test = train_test_split(valid,  test_size=0.5, random_state=0, shuffle=False)\n",
    "\n",
    "train, valid = train_test_split(deltas.index,  test_size=0.3, random_state=0, shuffle=True)\n",
    "valid, test = train_test_split(valid,  test_size=0.5, random_state=0, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87b9794-4919-4b9a-a349-b4257814d263",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655da4b-6a02-4e99-9239-42b854196ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65a2def-07e4-4fbf-9b51-a567c5d0b902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from scipy import stats # For in-built method to get PCC\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b1f1c-c9da-4b7b-9747-cc0302c0d038",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8663d-fdda-4ab1-9651-217abe197d70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ad7d0-a233-4120-9b5f-3ce29d3c3bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "lagn=9\n",
    "p_threshold = .05\n",
    "threshold = .5\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "final = pd.DataFrame()\n",
    "\n",
    "#target = '^SP500TR'\n",
    "#print()\n",
    "#print(target)\n",
    "#print()\n",
    "#print(f\"target: {target}\")\n",
    "\n",
    "#sets = range(0,len(Training),252)\n",
    "#move this outside\n",
    "\n",
    "X = deltas[set(deltas.columns).difference(target)].copy()\n",
    "newX = pd.DataFrame()\n",
    "y = pd.DataFrame(deltas[target].copy())\n",
    "\n",
    "for m in X.columns:\n",
    "    lagged = pd.DataFrame()\n",
    "    newX = pd.concat([newX,X[m]],axis=1)\n",
    "    \n",
    "    for lag in range(1,lagn+1):\n",
    "        temp = pd.DataFrame(X[m].shift(lag).copy())\n",
    "        temp.columns = [m+\"_\"+str(lag)]\n",
    "        lagged = pd.concat([lagged,temp],axis=1)\n",
    "    #print(lagged)\n",
    "    newX = pd.concat([newX,lagged],axis=1)\n",
    "\n",
    "#newX = newX[set(newX.columns).difference(newX)].copy()\n",
    "#newX = newX.dropna().copy()\n",
    "#y = y.loc[newX.index]\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "kfold.get_n_splits(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c10b5f-0d90-46fa-a20a-42df119ec37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "newX.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23b2f6a-e1db-4769-afeb-9c045555bec0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36af4f6-44a4-4b85-b17c-275455cdeb04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89e278f-1cbf-4c42-9195-93db105518c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#for m in X.columns:\n",
    "#print(m)\n",
    "#X_train = newX[newX.columns[newX.columns.str.contains(m)]].loc[train].copy()\n",
    "#X_valid = newX[newX.columns[newX.columns.str.contains(m)]].loc[valid].copy()\n",
    "X_train = newX.drop(X.columns, axis=1, inplace=False).loc[train].dropna().copy()\n",
    "X_valid = newX.drop(X.columns, axis=1, inplace=False).loc[valid].dropna().copy()\n",
    "X_test = newX.drop(X.columns, axis=1, inplace=False).loc[test].dropna().copy()\n",
    "\n",
    "y_train = y.loc[X_train.index][target].copy()\n",
    "y_valid = y.loc[X_valid.index][target].copy()\n",
    "y_test = y.loc[X_test.index][target].copy()\n",
    "\n",
    "exclude = ''\n",
    "\n",
    "sig_table = np.zeros(shape=(len(all_data.columns)))\n",
    "signs_table = np.zeros(shape=(len(all_data.columns)))\n",
    "\n",
    "#this is for internal cross validation\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=num_folds)\n",
    "train_ = []\n",
    "test_ = []\n",
    "for train_indices, test_indices in k_fold.split(X_train.index):\n",
    "    train_.append(train_indices)\n",
    "    test_.append(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d081b41c-d328-4df6-a89f-66a75d51cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=0.99, svd_solver='full')\n",
    "pca.fit(X_train)\n",
    "pca.explained_variance_\n",
    "print(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "#pca = PCA(n_components=1)\n",
    "#pca.fit(X)\n",
    "X_pca = pd.DataFrame(pca.transform(X_train))\n",
    "X_pca.index = X_train.index\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f033cccb-a1ae-4c46-828d-cb9027167452",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dabdf8c-110d-479e-8fdb-55d487b66b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from visuals import visuals as vs\n",
    "\n",
    "# Train the supervised model on the training \n",
    "model = AdaBoostRegressor().fit(pd.DataFrame(X_pca), y_train)\n",
    "\n",
    "# Extract the feature importances using .feature_importances_ \n",
    "importances = model.feature_importances_\n",
    "\n",
    "# Plot\n",
    "vs.feature_plot(importances, pd.DataFrame(X_pca), y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b8b894-7de5-4546-8bd6-4b3e7b85aa19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4567a2-1890-431e-90c3-9305afef20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sffs\n",
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "#lr = LinearRegression()\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "sffs1 = sffs(lr, \n",
    "          k_features=len(X_train)+1, \n",
    "          forward=True, \n",
    "          floating=True, \n",
    "          scoring='neg_mean_squared_error',\n",
    "          #scoring='accuracy',\n",
    "          n_jobs=-1,\n",
    "          cv=5)\n",
    "\n",
    "y_ = pd.DataFrame(np.where(y_train>0,1,0)))\n",
    "sffs1 = sffs1.fit(X_train, pd.DataFrame(y_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8af0f5-a98f-40d1-a964-f3095affbf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_sfs(sffs1.get_metric_dict(), kind='std_err')\n",
    "\n",
    "plt.title('Sequential Forward Selection (w. StdErr)')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "print('Selected features:', sffs1.k_feature_idx_)\n",
    "\n",
    "position = list(pd.DataFrame(sffs1.subsets_).loc[\"avg_score\"]).index(pd.DataFrame(sffs1.subsets_).loc[\"avg_score\"].max())\n",
    "\n",
    "print(sffs1.subsets_[position]['feature_names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c41a28-c754-4d1e-bde1-627a0b305e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_CV.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa1a23a-2e32-42e5-bad8-24ee1b4cfb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(pd.DataFrame(sffs1.subsets_).loc['feature_names'].values)\n",
    "\n",
    "#feature_idx = pd.DataFrame(sfbs1.subsets_).loc['feature_idx'].tolist()\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.linear_model import ElasticNet \n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "\n",
    "\n",
    "CV_results = pd.DataFrame()#[]#np.zeros(shape=(len(features),5))\n",
    "\n",
    "for f in features:\n",
    "\n",
    "    print(f)\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1, )\n",
    "    ratios = np.arange(0, 1, 0.05)\n",
    "    alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "\n",
    "    EN_CV = LogisticRegressionCV(l1_ratios=ratios, cv=cv, random_state=0, penalty='l2', n_jobs=-1)#.fit(X, y)\n",
    "    #EN_CV = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1)\n",
    "    # fit model\n",
    "\n",
    "    X_subset = pd.DataFrame(X_train[list(np.asarray(f))])\n",
    "    #y_ = pd.DataFrame(y_train)\n",
    "    y_ = pd.DataFrame(np.where(y_train>0,1,0))\n",
    "    EN_CV.fit(X_train, y_)\n",
    "    \n",
    "    \n",
    "    #print('alpha: %f' % EN_CV.alpha_)\n",
    "    #print('l1_ratio_: %f' % EN_CV.l1_ratio_)\n",
    "    \n",
    "    # evaluate model\n",
    "    scores = cross_val_score(EN_CV, X_subset, y_, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "    # force scores to be positive\n",
    "    #scores = absolute(scores)\n",
    "    #print('Mean MAE: %.3f (%.3f)' % (mean(scores[~np.isnan(scores)]), std(scores[~np.isnan(scores)])))\n",
    "    \n",
    "    #temp = pd.concat([pd.DataFrame({\"features\": [np.array(f)]}),pd.DataFrame(np.array(mean(scores[~np.isnan(scores)])).reshape(-1,1)),pd.DataFrame(np.array(std(scores[~np.isnan(scores)])).reshape(-1,1)),pd.DataFrame(np.array(EN_CV.alpha_).reshape(-1,1)),pd.DataFrame(np.array(EN_CV.l1_ratio_).reshape(-1,1))],axis=1)\n",
    "    #logistic\n",
    "    temp = pd.concat([pd.DataFrame({\"features\": [np.array(f)]}),pd.DataFrame(np.array(mean(scores[~np.isnan(scores)])).reshape(-1,1)),pd.DataFrame(np.array(std(scores[~np.isnan(scores)])).reshape(-1,1)),pd.DataFrame(np.array(EN_CV.l1_ratio_).reshape(-1,1))],axis=1)\n",
    "    \n",
    "    CV_results = pd.concat([CV_results,temp],axis=0)\n",
    "                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f3f6c1-78fe-43ad-85a3-fdfbb5528300",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFeatures = list(np.array(pd.DataFrame(sffs1.subsets_).loc['feature_names'])[position])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce356d39-fb0a-405f-b4eb-1d69b92a07ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CV_results.columns = ['features','mean error','std','alpha','lambda']\n",
    "#logistic\n",
    "CV_results.columns = ['features','mean error','std','lambda']\n",
    "CV_results.index = np.array(range(0,len(features)))#range(1,len(features)+1)\n",
    "plt.plot(CV_results['mean error'])\n",
    "plt.show()\n",
    "\n",
    "best = CV_results.iloc[CV_results['mean error'].idxmin()]\n",
    "print(best)\n",
    "\n",
    "parse = CV_results[CV_results['mean error']<=(np.min(CV_results['mean error'])+np.std(CV_results['mean error']))].iloc[0]\n",
    "print(parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0ce9e9-0472-40a6-a285-b9ec8ab8ae49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model3.predict(X_test[parse['features']]))\n",
    "print(np.array(np.where(y_test>0,1,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d0e400-193a-407b-8bd4-46a59b50c3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([pd.concat([pd.DataFrame(np.where(y_valid<0,-1,1)),pd.DataFrame(model3.predict_proba(X_valid[parse['features']])).loc[:,1]],axis=1),pd.concat([pd.DataFrame(np.where(y_test<0,-1,1)),pd.DataFrame(model3.predict_proba(X_test[parse['features']])).loc[:,1]],axis=1)],axis=0)\n",
    "#pd.DataFrame(model3.predict_proba(X_train[parse['features']])).loc[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb85c91-e888-41e0-8bd6-1145efb3c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LogisticRegression(l1_ratio=parse['lambda'])\n",
    "model3.fit(X_train[parse['features']],y_)\n",
    "#print(MAPE(np.array(y_), model3.predict(X_train[parse['features']])))\n",
    "print(mean(np.where([i[0] for i in np.where(y_==0,-1,1)]*np.where(model3.predict(X_train[parse['features']])==0,-1,1)<0,0,1)))\n",
    "print(mean(np.where(np.where(y_valid<0,-1,1)*np.where(model3.predict(X_valid[parse['features']])==0,-1,1)<0,0,1)))\n",
    "print(mean(np.where(np.where(y_test<0,-1,1)*np.where(model3.predict(X_test[parse['features']])==0,-1,1)<0,0,1)))\n",
    "\n",
    "pd.concat([pd.DataFrame(np.where(y_valid<0,-1,1)),pd.DataFrame(model3.predict(X_valid[parse['features']]))],axis=1)\n",
    "#print(MAPE(np.array(y_), model3.predict(X_train[parse['features']])))\n",
    "#print(MAPE(np.array(np.where(y_valid>0,1,0)), model3.predict(X_valid[parse['features']])))\n",
    "#print(MAPE(np.array(np.where(y_test>0,1,0)), model3.predict(X_test[parse['features']])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37da019-d329-4dca-82a4-8b5065c2cb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "model0 = LinearRegression()\n",
    "model1 = ElasticNet(alpha=parse['alpha'], l1_ratio=parse['lambda'])\n",
    "model2 = ElasticNet(alpha=best['alpha'], l1_ratio=best['lambda'])\n",
    "\n",
    "\n",
    "model0.fit(X_train[sortedFeatures],y_train)\n",
    "model1.fit(X_train[parse['features']],y_train)\n",
    "model2.fit(X_train[best['features']],y_train)\n",
    "\n",
    "#linear\n",
    "#parse\n",
    "print(sortedFeatures)\n",
    "#print(\"EN parse: \" + str(mean_squared_error(y_valid, model1.predict(X_valid[parse['features']]))))\n",
    "print(MAPE(y_train, model0.predict(X_train[sortedFeatures])))\n",
    "print(MAPE(y_valid, model0.predict(X_valid[sortedFeatures])))\n",
    "print(MAPE(y_test, model0.predict(X_test[sortedFeatures])))\n",
    "\n",
    "\n",
    "#parse\n",
    "print(parse['features'])\n",
    "#print(\"EN parse: \" + str(mean_squared_error(y_valid, model1.predict(X_valid[parse['features']]))))\n",
    "print(MAPE(y_train,model1.predict(X_train[parse['features']])))\n",
    "print(MAPE(y_valid, model1.predict(X_valid[parse['features']])))\n",
    "print(MAPE(y_test, model1.predict(X_test[parse['features']])))\n",
    "\n",
    "#best\n",
    "#print(\"EN best: \" + str(mean_squared_error(y_valid, model2.predict(X_valid[best['features']]))))\n",
    "print(best['features'])\n",
    "print(MAPE(y_train,model2.predict(X_train[best['features']])))\n",
    "print(MAPE(y_valid, model2.predict(X_valid[best['features']])))\n",
    "print(MAPE(y_test, model2.predict(X_test[best['features']])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe173818-871c-4d2c-a08e-ebb5e652a566",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ead074-a93d-446e-abef-6c0b10ca4a0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbe2436-63ed-40e4-8ca0-160f32e912d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = ''\n",
    "\n",
    "sig_table = np.zeros(shape=(len(sortedFeatures)))\n",
    "signs_table = np.zeros(shape=(len(sortedFeatures)))\n",
    "\n",
    "p_threshold = .05\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "k_fold = KFold(n_splits=num_folds)\n",
    "train_ = []\n",
    "test_ = []\n",
    "for train_indices, test_indices in k_fold.split(X_train.index):\n",
    "    train_.append(train_indices)\n",
    "    test_.append(test_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b77fa-33ab-4c23-b59e-3b8f25b97e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e054b77-9508-4e3f-8d30-48806189d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "sig_table = np.zeros(len(X_train[sortedFeatures].columns))\n",
    "signs_table = np.zeros(len(X_train[sortedFeatures].columns))\n",
    "significance = np.zeros(len(X_train[sortedFeatures].columns))\n",
    "sign = np.zeros(len(X_train[sortedFeatures].columns))\n",
    "purity = np.zeros(len(X_train[sortedFeatures].columns))\n",
    "\n",
    "for it in range(0,len(train_)):\n",
    "    max_pvalue = 1\n",
    "    train_index = train_[it]\n",
    "    test_index = test_[it]\n",
    "    #display(all_data.iloc[test_index].describe())\n",
    "    \n",
    "    subset = pd.concat([X_train[sortedFeatures].iloc[train_index].loc[:, ~X_train[sortedFeatures].columns.isin([exclude])],y_train],axis=1)\n",
    "    \n",
    "    #skip y and states\n",
    "    set_ = subset.loc[:, ~subset.columns.isin([target])].columns.tolist()\n",
    "    \n",
    "    n=len(subset)\n",
    "    \n",
    "    while(max_pvalue>=.05):\n",
    "\n",
    "        dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
    "        p_values = pd.DataFrame(2*dist.cdf(-abs(subset.pcorr()[target]))).T\n",
    "        p_values.columns = list(subset.columns)\n",
    "        \n",
    "        max_pname = p_values.idxmax(axis=1)[0]\n",
    "        max_pvalue = p_values[max_pname].values[0]\n",
    "        \n",
    "        if (max_pvalue > .05):\n",
    "\n",
    "            set_.remove(max_pname)\n",
    "            temp = [target]\n",
    "            temp.extend(set_)\n",
    "            subset = subset[temp]\n",
    "    \n",
    "    winners = p_values.loc[:, ~p_values.columns.isin([target])].columns.tolist()\n",
    "    sig_table = (sig_table + np.where(X_train[sortedFeatures].columns.isin(winners),1,0)).copy()\n",
    "    signs_table[X_train[sortedFeatures].columns.get_indexer(winners)]+=np.where(subset.pcorr()[target][winners]<0,-1,1)\n",
    "    \n",
    "significance = pd.DataFrame(sig_table).T\n",
    "significance.columns = list(X_train[sortedFeatures].columns)\n",
    "display(significance)\n",
    "\n",
    "sign = pd.DataFrame(signs_table).T\n",
    "sign.columns = list(X_train[sortedFeatures].columns)\n",
    "display(sign)\n",
    "\n",
    "purity = abs((sign/num_folds)*(sign/significance)).T.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "display(purity.T)\n",
    "\n",
    "threshold = .5\n",
    "\n",
    "chosen = list(purity.T.columns.values[np.array(purity.T>threshold).reshape(len(X_train[sortedFeatures].columns,))])\n",
    "display(chosen)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea8f2f8-3e0b-4e23-8b05-60c52205df79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8179e66-a887-45fd-97ad-521afc05ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from zca import zca\n",
    "\n",
    "zca = zca.ZCA()\n",
    "#chosen = sortedFeatures\n",
    "#chosen = parse['features']\n",
    "chosen = best['features']\n",
    "\n",
    "data_zca = zca.fit_transform(X_train[chosen])\n",
    "\n",
    "#zca.fit(X_train[chosen])\n",
    "\n",
    "#data_zca = zca.transform(X_train[chosen])\n",
    "\n",
    "model = sm.OLS(y_train,X_train[chosen])\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())\n",
    "\n",
    "reg = LinearRegression().fit(X_train[chosen], y_train)\n",
    "#reg.score(X, y)\n",
    "\n",
    "#reg.coef_\n",
    "\n",
    "#reg.intercept_\n",
    "\n",
    "#import statsmodels as ssm\n",
    "#X=sm.add_constant(X_train[sortedFeatures])        #to add constant value in the model\n",
    "model= sm.OLS(y_train,X_train[chosen]).fit()         #fitting the model\n",
    "summary = model.summary()      #summary of the model\n",
    "display(summary)\n",
    "print(MAPE(y_train,model.predict(X_train[chosen])))\n",
    "print(MAPE(y_valid,model.predict(X_valid[chosen])))\n",
    "print(MAPE(y_test,model.predict(X_test[chosen])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f116db-e04a-4ab9-bd95-508552f9648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parse['alpha'], parse['lambda'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e3bed-56a0-4486-a166-7367503831b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d911b4b-ef30-472c-8a24-435c2ad47ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.stats.proportion as smp\n",
    "\n",
    "correct = np.where(pd.concat([y_valid*model2.predict(X_valid[chosen]),y_test*model2.predict(X_test[chosen])],axis=0) > 0, 1 ,0)\n",
    "\n",
    "ci_low, ci_upp = smp.proportion_confint(sum(correct), len(correct), alpha=0.05, method='normal')\n",
    "\n",
    "print(ci_low)\n",
    "print(ci_upp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2bd50-7d41-46e8-a582-bfe6a001f9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed7f20f-838e-4108-864f-0782a824c74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(y_train, model.predict(X_train[sortedFeatures]))\n",
    "plt.scatter(y_valid, model.predict(X_valid[sortedFeatures]))\n",
    "plt.scatter(y_test, model.predict(X_test[sortedFeatures]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734ec73c-3a2d-4628-a4f5-d086cd113644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "lagn=9\n",
    "p_threshold = .05\n",
    "threshold = .5\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "final = pd.DataFrame()\n",
    "\n",
    "target = \"SPCS20RSA\"\n",
    "#print()\n",
    "#print(target)\n",
    "#print()\n",
    "#print(f\"target: {target}\")\n",
    "\n",
    "#sets = range(0,len(Training),252)\n",
    "#move this outside\n",
    "X_train = deltas.loc[train][set(deltas.columns).difference(target)].copy()\n",
    "X_valid = deltas.loc[test][set(deltas.columns).difference(target)].copy()\n",
    "y_train = deltas.loc[train][target].copy()\n",
    "y_valid = deltas.loc[test][target].copy()\n",
    "\n",
    "#X_train = deltas.loc[train][X_train.columns[~X_train.columns.isin([target])]].copy()\n",
    "#X_valid = deltas.loc[test][X_train.columns[~X_train.columns.isin([target])]].copy()\n",
    "#y_train = deltas.loc[train][target].copy()\n",
    "#y_valid = deltas.loc[test][target].copy()\n",
    "\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "kfold.get_n_splits(X_train)\n",
    "\n",
    "for m in X_train.columns:\n",
    "    #print(m)\n",
    "\n",
    "    corrs = []\n",
    "    ps = []\n",
    "    lags = []\n",
    "\n",
    "    sig_table = np.zeros(shape=(2))\n",
    "    signs_table = np.zeros(shape=(2))\n",
    "\n",
    "    sets = np.zeros(shape=(num_folds,lagn))\n",
    "\n",
    "    iterator = 0\n",
    "\n",
    "    for train_index, test_index in kfold.split(X_train):\n",
    "\n",
    "        subsetX = X_train.iloc[train_index]\n",
    "        subsetY = y_train.iloc[train_index]\n",
    "\n",
    "        #skip y and states\n",
    "\n",
    "        n=len(subsetX)\n",
    "\n",
    "        temp = pd.concat([subsetX[m].shift((lag-lagn)[0]),subsetY], axis=1).dropna()\n",
    "\n",
    "        dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
    "\n",
    "        chosen = pd.DataFrame()\n",
    "\n",
    "        #train\n",
    "        setc = crosscorrelation(np.array(temp[m]),np.array(temp.iloc[:,1]), lagn)\n",
    "        p_values = pd.DataFrame(2*dist.cdf(-abs(setc))).T\n",
    "\n",
    "        fields = np.argwhere(np.array(p_values)[0] <= p_threshold)\n",
    "\n",
    "        #print(fields[fields>lagn+1]-lagn-1)\n",
    "        #print(setc[fields[fields>lagn+1]])\n",
    "\n",
    "        #sets[iterator] = (np.where(p_values <= p_threshold, True, 0)*setc)[0][lagn+1:]\n",
    "        sets[iterator] = (np.array(p_values)*setc)[0][lagn+1:]             \n",
    "\n",
    "        iterator = iterator + 1\n",
    "\n",
    "    csets_avg = np.sum(sets, axis=0)/num_folds       \n",
    "\n",
    "    lag = pd.DataFrame(abs(csets_avg)).idxmax()\n",
    "    #print(f\"correlation: {setc[lag]}\")\n",
    "\n",
    "    #pearson_coef, p_value = stats.pearsonr(np.array(temp.iloc[:,0]),np.array(temp.iloc[:,1])) #define the columns to perform calculations on\n",
    "\n",
    "    corrs.append(csets_avg[lag])\n",
    "    p_value = (2*dist.cdf(-abs(csets_avg[lag])))\n",
    "    ps.append(p_value)\n",
    "    lags.append(lag)    \n",
    "\n",
    "    print(p_value)\n",
    "    if (p_value < .05):\n",
    "        for p in range(0,len(sets)):\n",
    "            plt.plot(sets[p])   \n",
    "        plt.show()\n",
    "\n",
    "        print(target)\n",
    "        print(m)\n",
    "        #print(i)\n",
    "        #print(f\"lag: {lag-lagn}\")\n",
    "        #print(\"Pearson Correlation Coefficient: \", pearson_coef, \"and a P-value of:\", p_value) # Results\n",
    "        temp1 = pd.DataFrame([target, m,(lag-lagn)[0],pearson_coef]).T\n",
    "        temp1.columns = [\"target\",\"name\",\"lag\",\"pearson\"]\n",
    "\n",
    "        #p_values = pd.DataFrame(2*dist.cdf(-abs(temp.corr()[target]))).T\n",
    "        #p_values.columns = list(temp.columns)\n",
    "\n",
    "        #if(p_values[p_values.loc[:, ~p_values.columns.isin([target])].columns.tolist()]<=p_threshold):\n",
    "        #winners = p_values.loc[:, ~p_values.columns.isin([target])].columns.tolist()\n",
    "        #winners = m\n",
    "\n",
    "        #sig_table = (sig_table + np.where(temp.columns.isin([winners]),1,0)).copy()\n",
    "        #print(sig_table)\n",
    "        #signs_table[temp.columns.get_indexer([winners])]+=np.where(temp.corr()[target][winners]<0,-1,1)                 \n",
    "        #chosen = pd.concat([temp1,chosen],axis=0)\n",
    "        chosen.append(m)\n",
    "\n",
    "        c_threshold = dist.ppf(.05)\n",
    "        plt.plot(np.sum(sets, axis=0)/num_folds)\n",
    "        plt.axhline(y = 0, color = 'r', linestyle = '-')\n",
    "        plt.axhline(y = c_threshold, color = 'y', linestyle = '-')\n",
    "        plt.axhline(y = abs(c_threshold), color = 'y', linestyle = '-')\n",
    "\n",
    "        plt.show()\n",
    "        x=range(0,lagn)\n",
    "        plt.stackplot(x,sets[0],sets[1],sets[2],sets[3],sets[4], labels=['A','B','C','D','E'])\n",
    "        plt.axhline(y = 0, color = 'r', linestyle = '-')\n",
    "        plt.axhline(y = c_threshold, color = 'y', linestyle = '-')\n",
    "        plt.axhline(y = abs(c_threshold), color = 'y', linestyle = '-')        \n",
    "        plt.show()\n",
    "\n",
    "    #significance = pd.DataFrame(sig_table).T\n",
    "    #significance.columns = list(temp.columns)\n",
    "    #print(significance)\n",
    "\n",
    "    #sign = pd.DataFrame(signs_table).T\n",
    "    #sign.columns = list(temp.columns)\n",
    "\n",
    "    #purity = abs((sign/num_folds)*(sign/significance)).T.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "    #print(purity)\n",
    "\n",
    "    #ichosen = list(purity.T.columns.values[np.array(purity.T>=threshold).reshape(len(temp.columns,))])\n",
    "    #print(ichosen)\n",
    "    #chosen.append(ichosen)\n",
    "\n",
    "    #print(chosen)\n",
    "\n",
    "    #print(lags)\n",
    "\n",
    "    #print(m)\n",
    "    #print(ps)          \n",
    "\n",
    "    for i in range(0,len(chosen)):\n",
    "        values = chosen.reset_index().iloc[i]\n",
    "        name = values['name']\n",
    "        target = values['target']\n",
    "        lag = values['lag']\n",
    "        #print(lag)\n",
    "        aggregate = pd.DataFrame()\n",
    "\n",
    "        #test\n",
    "        innerSet = pd.concat([X_valid[name].shift(lag),y_valid],axis=1).dropna()\n",
    "\n",
    "        for p in range(0,len(innerSet)):     \n",
    "            #print(innerSet.iloc[p][name] )\n",
    "            if(innerSet.iloc[p][name] < 0):\n",
    "                d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),innerSet.iloc[p][target], np.nan, 'l'])\n",
    "                #print(\"lower\")\n",
    "            elif (innerSet.iloc[p][name] > 0):\n",
    "                d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),np.nan, innerSet.iloc[p][target], 'u'])\n",
    "                #print(\"upper\")\n",
    "            else:\n",
    "                d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),np.nan, np.nan, np.nan])\n",
    "\n",
    "            aggregate = pd.concat([d.T,aggregate],axis=0)\n",
    "        #print(aggregate.loc[:,['l','u']].replace([np.inf, -np.inf, np.NaN], 0).cumsum().iloc[-1])\n",
    "\n",
    "        aggregate.columns = \"Date\",\"l\",\"u\",\"class\"\n",
    "\n",
    "        pearson_coef, p_value = stats.pearsonr(np.array(innerSet.iloc[:,0]),np.array(innerSet.iloc[:,1])) #define the columns to perform calculations on\n",
    "\n",
    "        #print(f\"name: {name}\", f\"lag: {lag}\", f\"corr: {innerSet.corr().iloc[0][1]}\")\n",
    "        if p_value < .05 and (values['pearson']*pearson_coef) > 0:\n",
    "\n",
    "                newData = pd.concat([pd.DataFrame(innerSet[target]).set_index(innerSet[target].index.strftime('%Y-%m-%d')),aggregate.set_index(\"Date\")],axis=1).replace([np.inf, -np.inf, np.NaN], 0)\n",
    "\n",
    "                if newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][target] < newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]['l'] or newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][target] < newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]['u']:\n",
    "                    print(f\"target: {target}\")\n",
    "                    print(f\"name: {name}\", f\"lag: {lag}\", f\"corr: {pearson_coef}\", f\"p-value: {p_value}\")\n",
    "                    print(pd.DataFrame(newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]).T)\n",
    "\n",
    "                    x_ticks = newData.index[np.arange(0, len(newData.index), 200)]\n",
    "\n",
    "                    plt.plot(newData[[target,\"l\",\"u\"]].cumsum())\n",
    "                    plt.legend(loc='upper left', fancybox=True, ncol=5, labels=[target,\"l\",\"u\"])\n",
    "                    #plt.legend(loc=\"upper left\",fontsize=8)\n",
    "\n",
    "                    plt.xticks(x_ticks, rotation = 45)\n",
    "                    plt.show()\n",
    "\n",
    "                    temp1 = pd.DataFrame([target, name, pearson_coef, p_value, lag, max(newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][['l','u']]), newData[[target]].cumsum().iloc[-1][target] ]).T\n",
    "                    final = pd.concat([temp1,final],axis=0)\n",
    "\n",
    "final.columns = [\"target\",\"correlate\",\"pearson\",\"p-value\",\"lag\",\"TCR\",\"holdTCR\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ffb8cc-851a-41ba-96fb-ea6219e1a116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fa6caa-63e6-4902-aa8d-fcd376647055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "lagn=1\n",
    "p_threshold = .05\n",
    "threshold = .5\n",
    "\n",
    "num_folds = 5\n",
    "kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "\n",
    "final = pd.DataFrame()\n",
    "\n",
    "for target in set(deltas.columns) & set(vetted_symbols):\n",
    "    #print()\n",
    "    #print(target)\n",
    "    #print()\n",
    "    #print(f\"target: {target}\")\n",
    "    \n",
    "    #sets = range(0,len(Training),252)\n",
    "    #move this outside\n",
    "    X_train = deltas.loc[train][X_train.columns[~X_train.columns.isin([target])]].copy()\n",
    "    X_valid = deltas.loc[test][X_train.columns[~X_train.columns.isin([target])]].copy()\n",
    "    y_train = deltas.loc[train][target].copy()\n",
    "    y_valid = deltas.loc[test][target].copy()\n",
    "    \n",
    "    kfold = KFold(n_splits=num_folds, shuffle=False)\n",
    "    kfold.get_n_splits(X_train)\n",
    "\n",
    "    for m in X_train.columns:\n",
    "        #print(m)\n",
    "        \n",
    "        corrs = []\n",
    "        ps = []\n",
    "        lags = []\n",
    "\n",
    "        sig_table = np.zeros(shape=(2))\n",
    "        signs_table = np.zeros(shape=(2))\n",
    "\n",
    "        sets = np.zeros(shape=(num_folds,lagn))\n",
    "        \n",
    "        iterator = 0\n",
    "        \n",
    "        for train_index, test_index in kfold.split(X_train):\n",
    "\n",
    "            subsetX = X_train.iloc[train_index]\n",
    "            subsetY = y_train.iloc[train_index]\n",
    "\n",
    "            #skip y and states\n",
    "\n",
    "            n=len(subset)\n",
    "            \n",
    "            temp = pd.concat([subsetX[m].shift((lag-lagn)[0]),subsetY], axis=1).dropna()\n",
    "\n",
    "            dist = scipy.stats.beta(n/2 - 1, n/2 - 1, loc=-1, scale=2)\n",
    "\n",
    "            chosen = pd.DataFrame()\n",
    "\n",
    "            #train\n",
    "            setc = crosscorrelation(np.array(temp[m]),np.array(temp.iloc[:,1]), lagn)\n",
    "            p_values = pd.DataFrame(2*dist.cdf(-abs(setc))).T\n",
    "            \n",
    "            fields = np.argwhere(np.array(p_values)[0] <= p_threshold)\n",
    "           \n",
    "            #print(fields[fields>lagn+1]-lagn-1)\n",
    "            #print(setc[fields[fields>lagn+1]])\n",
    "            \n",
    "            #sets[iterator] = (np.where(p_values <= p_threshold, True, 0)*setc)[0][lagn+1:]\n",
    "            sets[iterator] = (np.array(p_values)*setc)[0][lagn+1:]             \n",
    "            \n",
    "            iterator = iterator + 1\n",
    "        \n",
    "        csets_avg = np.sum(sets, axis=0)/num_folds       \n",
    "        \n",
    "        lag = pd.DataFrame(abs(csets_avg)).idxmax()\n",
    "        #print(f\"correlation: {setc[lag]}\")\n",
    "\n",
    "        #pearson_coef, p_value = stats.pearsonr(np.array(temp.iloc[:,0]),np.array(temp.iloc[:,1])) #define the columns to perform calculations on\n",
    "\n",
    "        corrs.append(csets_avg[lag])\n",
    "        p_value = (2*dist.cdf(-abs(csets_avg[lag])))\n",
    "        ps.append(p_value)\n",
    "        lags.append(lag)    \n",
    "\n",
    "        #print(p_value)\n",
    "        if (p_value < .05):\n",
    "            for p in range(0,len(sets)):\n",
    "                plt.plot(sets[p])   \n",
    "            plt.show()\n",
    "            \n",
    "            print(target)\n",
    "            print(m)\n",
    "            #print(i)\n",
    "            #print(f\"lag: {lag-lagn}\")\n",
    "            #print(\"Pearson Correlation Coefficient: \", pearson_coef, \"and a P-value of:\", p_value) # Results\n",
    "            temp1 = pd.DataFrame([target, m,(lag-lagn)[0],pearson_coef]).T\n",
    "            temp1.columns = [\"target\",\"name\",\"lag\",\"pearson\"]\n",
    "\n",
    "            #p_values = pd.DataFrame(2*dist.cdf(-abs(temp.corr()[target]))).T\n",
    "            #p_values.columns = list(temp.columns)\n",
    "\n",
    "            #if(p_values[p_values.loc[:, ~p_values.columns.isin([target])].columns.tolist()]<=p_threshold):\n",
    "            #winners = p_values.loc[:, ~p_values.columns.isin([target])].columns.tolist()\n",
    "            winners = m\n",
    "\n",
    "            #sig_table = (sig_table + np.where(temp.columns.isin([winners]),1,0)).copy()\n",
    "            #print(sig_table)\n",
    "            #signs_table[temp.columns.get_indexer([winners])]+=np.where(temp.corr()[target][winners]<0,-1,1)                 \n",
    "            #chosen = pd.concat([temp1,chosen],axis=0)\n",
    "            chosen.append(m)\n",
    "\n",
    "            c_threshold = dist.ppf(.05)\n",
    "            plt.plot(np.sum(sets, axis=0)/num_folds)\n",
    "            plt.axhline(y = 0, color = 'r', linestyle = '-')\n",
    "            plt.axhline(y = c_threshold, color = 'y', linestyle = '-')\n",
    "            plt.axhline(y = abs(c_threshold), color = 'y', linestyle = '-')\n",
    "\n",
    "            plt.show()\n",
    "            x=range(0,lagn)\n",
    "            plt.stackplot(x,sets[0],sets[1],sets[2],sets[3],sets[4], labels=['A','B','C','D','E'])\n",
    "            plt.axhline(y = 0, color = 'r', linestyle = '-')\n",
    "            plt.axhline(y = c_threshold, color = 'y', linestyle = '-')\n",
    "            plt.axhline(y = abs(c_threshold), color = 'y', linestyle = '-')        \n",
    "            plt.show()\n",
    "\n",
    "        #significance = pd.DataFrame(sig_table).T\n",
    "        #significance.columns = list(temp.columns)\n",
    "        #print(significance)\n",
    "\n",
    "        #sign = pd.DataFrame(signs_table).T\n",
    "        #sign.columns = list(temp.columns)\n",
    "\n",
    "        #purity = abs((sign/num_folds)*(sign/significance)).T.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "        #print(purity)\n",
    "\n",
    "        #ichosen = list(purity.T.columns.values[np.array(purity.T>=threshold).reshape(len(temp.columns,))])\n",
    "        #print(ichosen)\n",
    "        #chosen.append(ichosen)\n",
    "        \n",
    "        #print(chosen)\n",
    "        \n",
    "        #print(lags)\n",
    "        \n",
    "        #print(m)\n",
    "        #print(ps)          \n",
    "        \n",
    "        for i in range(0,len(chosen)):\n",
    "            values = chosen.reset_index().iloc[i]\n",
    "            name = values['name']\n",
    "            target = values['target']\n",
    "            lag = values['lag']\n",
    "            #print(lag)\n",
    "            aggregate = pd.DataFrame()\n",
    "\n",
    "            #test\n",
    "            innerSet = pd.concat([X_valid[name].shift(lag),y_valid],axis=1).dropna()\n",
    "\n",
    "            for p in range(0,len(innerSet)):     \n",
    "                #print(innerSet.iloc[p][name] )\n",
    "                if(innerSet.iloc[p][name] < 0):\n",
    "                    d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),innerSet.iloc[p][target], np.nan, 'l'])\n",
    "                    #print(\"lower\")\n",
    "                elif (innerSet.iloc[p][name] > 0):\n",
    "                    d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),np.nan, innerSet.iloc[p][target], 'u'])\n",
    "                    #print(\"upper\")\n",
    "                else:\n",
    "                    d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),np.nan, np.nan, np.nan])\n",
    "\n",
    "                aggregate = pd.concat([d.T,aggregate],axis=0)\n",
    "            #print(aggregate.loc[:,['l','u']].replace([np.inf, -np.inf, np.NaN], 0).cumsum().iloc[-1])\n",
    "\n",
    "            aggregate.columns = \"Date\",\"l\",\"u\",\"class\"\n",
    "\n",
    "            pearson_coef, p_value = stats.pearsonr(np.array(innerSet.iloc[:,0]),np.array(innerSet.iloc[:,1])) #define the columns to perform calculations on\n",
    "\n",
    "            #print(f\"name: {name}\", f\"lag: {lag}\", f\"corr: {innerSet.corr().iloc[0][1]}\")\n",
    "            if p_value < .05 and (values['pearson']*pearson_coef) > 0:\n",
    "\n",
    "                    newData = pd.concat([pd.DataFrame(innerSet[target]).set_index(innerSet[target].index.strftime('%Y-%m-%d')),aggregate.set_index(\"Date\")],axis=1).replace([np.inf, -np.inf, np.NaN], 0)\n",
    "\n",
    "                    if newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][target] < newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]['l'] or newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][target] < newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]['u']:\n",
    "                        print(f\"target: {target}\")\n",
    "                        print(f\"name: {name}\", f\"lag: {lag}\", f\"corr: {pearson_coef}\", f\"p-value: {p_value}\")\n",
    "                        print(pd.DataFrame(newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]).T)\n",
    "\n",
    "                        x_ticks = newData.index[np.arange(0, len(newData.index), 200)]\n",
    "\n",
    "                        plt.plot(newData[[target,\"l\",\"u\"]].cumsum())\n",
    "                        plt.legend(loc='upper left', fancybox=True, ncol=5, labels=[target,\"l\",\"u\"])\n",
    "                        #plt.legend(loc=\"upper left\",fontsize=8)\n",
    "\n",
    "                        plt.xticks(x_ticks, rotation = 45)\n",
    "                        plt.show()\n",
    "\n",
    "                        temp1 = pd.DataFrame([target, name, pearson_coef, p_value, lag, max(newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][['l','u']]), newData[[target]].cumsum().iloc[-1][target] ]).T\n",
    "                        final = pd.concat([temp1,final],axis=0)\n",
    "\n",
    "final.columns = [\"target\",\"correlate\",\"pearson\",\"p-value\",\"lag\",\"TCR\",\"holdTCR\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a0652-040f-4e7c-9290-d39157eeb5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd03d72-19e8-424a-bc05-6d788ca05385",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8812ce-ac82-4e32-9169-f7c89fffe51a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c8e09f-335f-4167-9eec-470aba8361cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3f1cf3-e8e0-41df-9e21-e39402bc9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2 = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(final)):\n",
    "    values = final.reset_index().iloc[i]\n",
    "    name = values['correlate']\n",
    "    target = values['target']\n",
    "    lag = values['lag']\n",
    "    #print(lag)\n",
    "    aggregate = pd.DataFrame()\n",
    "\n",
    "    X_test = deltas.loc[test][set(deltas.columns).difference(target)]\n",
    "    y_test = deltas.loc[test][target]\n",
    "    \n",
    "    #test\n",
    "    innerSet = pd.concat([X_test[name].shift(lag),y_test],axis=1).dropna()\n",
    "\n",
    "    for p in range(0,len(innerSet)):     \n",
    "        #print(innerSet.iloc[p][name] )\n",
    "        if(innerSet.iloc[p][name] < 0):\n",
    "            d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),innerSet.iloc[p][target], np.nan, 'l'])\n",
    "            #print(\"lower\")\n",
    "        elif (innerSet.iloc[p][name] > 0):\n",
    "            d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),np.nan, innerSet.iloc[p][target], 'u'])\n",
    "            #print(\"upper\")\n",
    "        else:\n",
    "            d = pd.DataFrame([innerSet.index[p].strftime('%Y-%m-%d'),np.nan, np.nan, np.nan])\n",
    "\n",
    "        aggregate = pd.concat([d.T,aggregate],axis=0)\n",
    "    #print(aggregate.loc[:,['l','u']].replace([np.inf, -np.inf, np.NaN], 0).cumsum().iloc[-1])\n",
    "\n",
    "    aggregate.columns = \"Date\",\"l\",\"u\",\"class\"\n",
    "\n",
    "    pearson_coef, p_value = stats.pearsonr(np.array(innerSet.iloc[:,0]),np.array(innerSet.iloc[:,1])) #define the columns to perform calculations on\n",
    "\n",
    "    #print(f\"name: {name}\", f\"lag: {lag}\", f\"corr: {innerSet.corr().iloc[0][1]}\")\n",
    "    if p_value < .05:\n",
    "\n",
    "            newData = pd.concat([pd.DataFrame(innerSet[target]).set_index(innerSet[target].index.strftime('%Y-%m-%d')),aggregate.set_index(\"Date\")],axis=1).replace([np.inf, -np.inf, np.NaN], 0)\n",
    "\n",
    "            if newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][target] < newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]['l'] or newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][target] < newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]['u']:\n",
    "                print(f\"target: {target}\")\n",
    "                print(f\"name: {name}\", f\"lag: {lag}\", f\"corr: {pearson_coef}\", f\"p-value: {p_value}\")\n",
    "                print(pd.DataFrame(newData[[target,\"l\",\"u\"]].cumsum().iloc[-1]).T)\n",
    "\n",
    "                x_ticks = newData.index[np.arange(0, len(newData.index), 200)]\n",
    "\n",
    "                plt.plot(newData[[target,\"l\",\"u\"]].cumsum())\n",
    "                plt.legend(loc='upper left', fancybox=True, ncol=5, labels=[target,\"l\",\"u\"])\n",
    "                #plt.legend(loc=\"upper left\",fontsize=8)\n",
    "\n",
    "                plt.xticks(x_ticks, rotation = 45)\n",
    "                plt.show()\n",
    "\n",
    "                temp1 = pd.DataFrame([target, name, pearson_coef, p_value, lag, max(newData[[target,\"l\",\"u\"]].cumsum().iloc[-1][['l','u']]), newData[[target]].cumsum().iloc[-1][target] ]).T\n",
    "                results2 = pd.concat([temp1,results2],axis=0)\n",
    "                \n",
    "results2.columns = [\"target\",\"correlate\",\"pearson\",\"p-value\",\"lag\",\"TCR\",\"holdTCR\"]                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33164a3e-d6f7-4dfa-acc0-22cc61e12ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61851809-5bb6-4fe0-8a85-825e0b6c7e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c9dbe-45c4-4412-94e9-78bd649fc503",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([final['pearson'],results2['pearson']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df62dd61-3005-47e9-af92-05ca6780f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([final['pearson']*results2['pearson']],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bbc290-8cad-491e-9aaf-981ec6311c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = pd.DataFrame()\n",
    "\n",
    "for s in Training:\n",
    "    #print(s)\n",
    "    set_dates = s\n",
    "    #print(set_dates[1])\n",
    "    #print(set_dates[-1])\n",
    "    \n",
    "    #rate of change compare\n",
    "    filtered2 =  pd.concat([truncatedData.loc[[i[0] for i in set_dates]][compare],truncatedData.loc[[i[0] for i in set_dates]][target]],axis=1).pct_change().dropna().replace([np.inf, -np.inf, np.NaN], 0)\n",
    "    #quantiles2 = filtered2[compare].quantile(q=[0, .25, .5, .75, 1], interpolation='linear')\n",
    "\n",
    "    #prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    #colors = cycle(prop_cycle.by_key()['color'])\n",
    "    \n",
    "    #subset, Holdout = split_sequences(np.array(pd.DataFrame(filtered2.index.strftime('%Y-%m-%d'))), 126, 0)\n",
    "    \n",
    "    #for i in subset:\n",
    "        \n",
    "    #quantiles = pd.DataFrame(filtered2.iloc[0:-2][compare].quantile([0,.5,1]),columns=['min','median','max'])\n",
    "    #np.percentile((filtered2.iloc[0:-2][compare]),50)\n",
    "    \n",
    "    t = pd.DataFrame(filtered2.iloc[0:-3][compare].quantile([0,.5,1])).T\n",
    "    t.columns = ['min','median','max']\n",
    "    quantiles = t.reset_index(drop=True)\n",
    "\n",
    "    #lower = filtered2.iloc[0:-1][(filtered2.iloc[0:-2][compare]<=quantile)].index\n",
    "    #upper = ~filtered2.iloc[0:-2].index.isin(lower)\n",
    "\n",
    "\n",
    "\n",
    "    #my_dpi = 100\n",
    "    #fig, axes = plt.subplots(figsize=(12, 4),ncols=3, nrows=1)\n",
    "    #ax1, ax2, ax3 = axes.ravel()\n",
    "    #sns.set(style=\"ticks\")\n",
    "    #sns.despine(fig=fig)\n",
    "\n",
    "    #dataframe = filtered2.loc[dates]\n",
    "    #returns = dataframe[target].dropna()\n",
    "\n",
    "    #l = np.where(filtered2.iloc[0:-2][compare] <= quantile, filtered2.iloc[-1][target], 0)\n",
    "    #u = np.where(filtered2.iloc[upper][compare] > quantile, filtered2.iloc[-1][target], 0)\n",
    "    if((filtered2.iloc[-2][compare] <= quantiles['median'][0])):\n",
    "        d = pd.DataFrame([filtered2.iloc[-1][target], np.nan, 'l'])\n",
    "    else:\n",
    "        d = pd.DataFrame([np.nan, filtered2.iloc[-1][target], 'u'])\n",
    "    #d = pd.concat([pd.DataFrame(l.tolist()),pd.DataFrame(u.tolist())],axis=1)\n",
    "    #print(len(d))\n",
    "    #print(d)\n",
    "    \n",
    "    #print(pd.concat([d.T, quantiles.T.reset_index(drop=True)],axis=1))\n",
    "    #aggregate = pd.concat([d.T, quantiles.T.reset_index(drop=True), aggregate],axis=0)\n",
    "    aggregate = pd.concat([pd.concat([d.T, quantiles],axis=1),aggregate],axis=0)\n",
    "    #print(aggregate)\n",
    "    #print(len(aggregate))\n",
    "    #print(aggregate)\n",
    "    #d.index = filtered2.iloc[-1].name.strftime('%Y-%m-%d')\n",
    "   \n",
    "    #sns.regplot(x=compare, y=target, data=dataframe, ax=ax1)\n",
    "\n",
    "    #sns.violinplot(x=dataframe[compare].dropna(),color=next(colors), ax=ax2)\n",
    "\n",
    "    #sns.vioinplot(x=returns,color=next(colors), ax=ax3) \n",
    "    #plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7547d617-bd4e-4f9b-819c-a80234e52372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a474c002-5d5d-45ba-a289-461359c624e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Training)\n",
    "\n",
    "aggregate.columns = ['l','u','class','min','median','max']\n",
    "\n",
    "dates = []\n",
    "for item in Training:\n",
    "    last = item[-1]\n",
    "    dates.append(last[0])\n",
    "    \n",
    "aggregate.loc[:,['l','u']].replace([np.inf, -np.inf, np.NaN], 0).cumsum().iloc[-1]\n",
    "\n",
    "aggregate.index=dates\n",
    "\n",
    "#print(aggregate.cumsum().iloc[-1])\n",
    "#aggregate.reset_index()\n",
    "#ax = sns.regplot(x=aggregate.columns[1], y='index', data=aggregate.reset_index())\n",
    "\n",
    "x_ticks = aggregate.index[np.arange(0, len(aggregate.index), 200)]\n",
    "\n",
    "plt.plot(aggregate.loc[:,['l']].replace([np.inf, -np.inf, np.NaN], 0).cumsum())\n",
    "\n",
    "plt.xticks(x_ticks, rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014292dc-59d5-4f9e-81a4-69f017aa582c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c42a2-9ace-4d94-ab35-d68dd81d6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = pd.concat([truncatedData[compare].pct_change(),truncatedData[target]],axis=1).loc[dates]\n",
    "temp2 = pd.concat([truncatedData[compare].pct_change(),truncatedData[target].pct_change()],axis=1).loc[dates]\n",
    "temp1.index = temp1.index.copy().strftime('%Y-%m-%d')\n",
    "temp2.index = temp2.index.copy().strftime('%Y-%m-%d')\n",
    "temp = pd.concat([temp1,temp2,aggregate],axis=1)\n",
    "newNames = [compare,target,compare+\"_pct_change\",target+\"_pct_change\"]\n",
    "newNames.extend(temp.columns[4:10])\n",
    "temp.columns = newNames\n",
    "\n",
    "temp.to_csv('../data/processed/output.csv', index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bc91fb-1ac2-4166-83d9-d6ce9441e66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "#scores = np.where(temp[compare] > 1, 1, 0)\n",
    "\n",
    "#idk why this library uses 2, 1 vs 1 and 0\n",
    "fpr, tpr, thresholds = metrics.roc_curve(np.where(temp[target+\"_pct_change\"] > 0, 2, 1), np.array(temp[compare+\"_pct_change\"].shift(-1)) , pos_label=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8173adc-8622-4ca2-bc42-f73f83cac0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.plot_roc_curve(clf, X_test, y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b627888-e46d-4fcc-b34c-8a792899a70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "#plt.plot(fpr,tpr, label='AUC = ' + str(round(roc_auc_score(y,m.oob_decision_function_[:,1]), 2)))\n",
    "#plt.legend(loc='lower right')\n",
    "\n",
    "from plot_metric.functions import BinaryClassification\n",
    "# Visualisation with plot_metric\n",
    "bc = BinaryClassification(np.where(temp[target+\"_pct_change\"] > 0, 1, 0), np.array(temp[compare+\"_pct_change\"]), labels=[\"Class 1\", \"Class 2\"])\n",
    "\n",
    "# Figures\n",
    "plt.figure(figsize=(5,5))\n",
    "bc.plot_roc_curve()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f637ce-142d-4c36-acf2-382c4c9abe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"optimal cutoff\")\n",
    "#https://stackoverflow.com/questions/28719067/roc-curve-and-cut-off-point-python\n",
    "Find_Optimal_Cutoff(np.where(temp[target+\"_pct_change\"] > 0, 1, 0), np.array(temp[compare+\"_pct_change\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95d32a6-77c9-47e5-be41-e1410245a841",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import DataGenerator, KneeLocator\n",
    "kneedle = KneeLocator(fpr, tpr, S=1.0, curve=\"concave\", direction=\"increasing\")\n",
    "#kneedle.plot_knee_normalized()\n",
    "kneedle.plot_knee()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c1233d-050a-465c-b6ed-5218f71f2587",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8c093-8322-4813-863b-cf72a0cde583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3520671e-ee99-486a-9207-2ceb98d39960",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1817e-284a-4768-9f3b-a3f66c6941b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6627e-eeb0-4087-a751-e4193cbedd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(aggregate[aggregate['bifur']=='l'].iloc[:,0].cumsum(),  truncatedData[target].pct_change().loc[aggregate[aggregate['bifur']=='l'].iloc[:,0].index.tolist()])\n",
    "plt.show()\n",
    "aggregate[aggregate['bifur']=='l'].iloc[:,0].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efaeccd-d202-4a53-8781-730f772a03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(aggregate[aggregate['bifur']=='u'].iloc[:,1].cumsum(),  truncatedData[target].pct_change().loc[aggregate[aggregate['bifur']=='u'].iloc[:,1].index.tolist()])\n",
    "plt.show()\n",
    "aggregate[aggregate['bifur']=='u'].iloc[:,1].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c31cea-f47f-4c76-b8be-1178b7bf5b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#252 trading days a year\n",
    "#30 days = 21\n",
    "#60 = 42\n",
    "#90 = 63\n",
    "\n",
    "#return = current - prior / prior\n",
    "\n",
    "#for i in range(1,63): #[1,21,42,63]\n",
    "for i in [1,21,42,63,84]: #[1,21,42,84]\n",
    "    print(i)\n",
    "    rate_of_change = (truncatedData[compare]-truncatedData[compare].shift(i))/truncatedData[compare].shift(i)\n",
    "    \n",
    "    newDF = pd.concat([rate_of_change,truncatedData[target].pct_change()],axis=1).dropna()\n",
    "\n",
    "    set1 = newDF[compare]\n",
    "    set2 = newDF[target]\n",
    "    \n",
    "    lags = range(0,85) #[0,21,42,63]\n",
    "    mcorrs_ = []\n",
    "    mlags = []\n",
    "    for j in lags:\n",
    "        \n",
    "        newSet = pd.concat([set1.shift(j),set2],axis=1).dropna()\n",
    "      \n",
    "        mcorrs_.append(np.array(newSet.corr())[1,0])\n",
    "        \n",
    "    plt.plot(mcorrs_)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80acb6b9-23ed-44a3-8123-06137d83af4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([(truncatedData[compare]-truncatedData[compare].shift(1))/truncatedData[compare].shift(1),truncatedData[target].pct_change().shift(0)],axis=1).dropna().corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cfeec-663c-450e-a53b-5af669d8dccb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0618b1-bbc1-418a-a66d-84c96e9d32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter((truncatedData[compare]-truncatedData[compare].shift(1))/truncatedData[compare].shift(1),truncatedData[target].pct_change().shift(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ebcab-8ba5-4779-ac55-49dea38508f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570015c-e994-46fe-92d9-d9f34bddcd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib.pyplot as plt\n",
    "#plt.matshow(\n",
    "df = combined_set.loc[combined_set.index>=start_date.strftime('%Y-%m-%d')]\n",
    "\n",
    "r_ = df.corr()\n",
    "\n",
    "filter = r_[compare]\n",
    "                       \n",
    "filter = filter[filter<1]\n",
    "filter = filter.sort_values(kind=\"quicksort\", ascending=True)\n",
    "print(filter.head(10))\n",
    "print(filter.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe70b64d-f6b5-42aa-9b68-a2617214618b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c15a7a3-7138-46f2-930c-a9db9543c12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.99, svd_solver='full')\n",
    "\n",
    "X = np.array(d.iloc[:,1:])\n",
    "'''\n",
    "pca.fit(scale(X))\n",
    "#pca.explained_variance_\n",
    "pca.explained_variance_ratio_.cumsum()\n",
    "X_pca = pd.DataFrame(pca.transform(d.iloc[0:,1:]))\n",
    "X_pca.index = d.index\n",
    "X_pca.sort_values(by=[0],ascending=False,inplace=True)\n",
    "'''\n",
    "TSS_ = []\n",
    "BSS_ = []\n",
    "WSS_ = []\n",
    "silhouettes_ = []\n",
    "\n",
    "for k in range(2,22):\n",
    "    model = KMeans(n_clusters=k, random_state=0, n_init=100).fit(X)\n",
    "    #print(model.inertia_)\n",
    "\n",
    "    codebook = np.array(model.cluster_centers_)\n",
    "    partition, euc_distance_to_centroids = vq(X, codebook)\n",
    "    WSS = np.sum(euc_distance_to_centroids**2)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(X, model.labels_)\n",
    "\n",
    "    silhouettes_.append(silhouette_avg)\n",
    "    \n",
    "    TSS = np.sum((X-X.mean(0))**2)\n",
    "\n",
    "    BSS = TSS - WSS\n",
    "\n",
    "    TSS_.append(TSS)\n",
    "    BSS_.append(BSS)\n",
    "    WSS_.append(WSS)\n",
    "    \n",
    "    #print(TSS, WSS, BSS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e31c2d-f876-4a35-bc5e-dce9704dd156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d119ffe-ee16-43f4-be74-e98022255a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d568c59-e316-44a7-831e-040c118fea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(BSS_)/np.array(TSS_))\n",
    "plt.show()\n",
    "plt.plot(np.array(WSS_))\n",
    "plt.show()\n",
    "plt.plot(np.array(silhouettes_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6a8c9c-707a-4175-95a5-7a0b3dc9fa91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67420e63-59fe-45d6-90dd-ac2ee3fc24fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BSS_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#plt.plot(np.array(BSS_)/np.array(WSS_))\u001b[39;00m\n\u001b[0;32m      2\u001b[0m calinski_ \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;28mlen\u001b[39m(\u001b[43mBSS_\u001b[49m)):\n\u001b[0;32m      4\u001b[0m     new\u001b[38;5;241m=\u001b[39m(BSS_[i]\u001b[38;5;241m*\u001b[39m(i))\u001b[38;5;241m/\u001b[39m(WSS_[i]\u001b[38;5;241m/\u001b[39m((\u001b[38;5;28mlen\u001b[39m(d)\u001b[38;5;241m-\u001b[39m(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))))\n\u001b[0;32m      5\u001b[0m     calinski_\u001b[38;5;241m.\u001b[39mappend(new)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BSS_' is not defined"
     ]
    }
   ],
   "source": [
    "#plt.plot(np.array(BSS_)/np.array(WSS_))\n",
    "calinski_ = []\n",
    "for i in range(0,len(BSS_)):\n",
    "    new=(BSS_[i]*(i))/(WSS_[i]/((len(d)-(i+1))))\n",
    "    calinski_.append(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6193cabf-e50d-4d62-9345-973a4856df1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(calinski_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0db822b-df41-4682-8b8e-82613e2b3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from clustergram import Clustergram\n",
    "\n",
    "cgram = Clustergram(range(1, 8))\n",
    "cgram.fit(d.iloc[0:,1:])\n",
    "cgram.plot()\n",
    "cgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2b9636-4857-4aaa-b13d-e2077e957c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(Y):\n",
    "    \n",
    "    #Y = x\n",
    "    #output_slider_variable.value\n",
    "    internalFilter = filter_.copy()\n",
    "    internalFilter.remove(Y)\n",
    "    all_data_ = pd.concat([all_data[Y],all_data[internalFilter]], axis=1)    \n",
    "    #print(all_data_.describe())\n",
    "    display(all_data_.describe())\n",
    "    #x_ticks = all_data_.index[np.arange(0, len(all_data.index), int(len(internalFilter)/5))]\n",
    "    x_ticks  = []\n",
    "    for index, element in enumerate(all_data_.index):\n",
    "        if index % int(np.round(len(all_data_.index)/10)) == 0:\n",
    "            x_ticks.append(element)\n",
    "    plt.plot(all_data_[Y])\n",
    "    plt.xticks(x_ticks, rotation = 45)\n",
    "    plt.show()        \n",
    "    plt.hist(all_data_[Y], bins='auto')\n",
    "    plt.show()\n",
    "    diff = pd.DataFrame((all_data_[Y].pct_change())).dropna()\n",
    "    plt.hist(diff, bins='auto')\n",
    "    plt.show()\n",
    "    return(all_data_)\n",
    "    \n",
    "out = interactive(f3, Y=filter_)\n",
    "\n",
    "#output_slider_variable.observe(f4, 'value')\n",
    "\n",
    "print(\"choose Y\")\n",
    "display(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2cc18-8baf-4f8a-bcd2-9562b62237fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(out.result, test_size=tsize, shuffle=False)\n",
    "\n",
    "d = get_deltas(train)\n",
    "#differenced = pd.DataFrame(np.transpose(d.reshape(len(out.result.columns),len(out.result))))\n",
    "#differenced = pd.DataFrame(np.transpose(d.reshape(len(out.result.columns),len(out.result))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d87fad8-9555-4fe1-a82f-18243715d650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2b0f5-1e32-4e6a-8e54-ea4dbe225df2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e1c5c4-9718-4cbb-a820-9fd94e3fc482",
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = int(len(os.sched_getaffinity(0)))\n",
    "\n",
    "def getDifferenced(i, data):\n",
    "    #v = d[[i]]\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i], d=d[[i]], floor=5e-5)\n",
    "    #print(i)\n",
    "    a = np.array(data.iloc[:, i])\n",
    "    \n",
    "    return fdiff(a, n=d[i], axis=0)\n",
    "    #gquant_gpu, weights = frac_diff(all_data.iloc[:, i]), d=v, floor=5e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2070d863-3329-4f28-8b4e-a74b5480cc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "differenced = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(d)):\n",
    "    value = pd.DataFrame(getDifferenced(i, out.result))\n",
    "    #print(value)\n",
    "    differenced = pd.concat([differenced,value],axis=1)\n",
    "    \n",
    "differenced.columns = out.result.columns\n",
    "differenced.index = out.result.index    \n",
    "\n",
    "'''\n",
    "\n",
    "differenced_train = pd.DataFrame()\n",
    "\n",
    "for i in range(0,len(d)):\n",
    "    value = pd.DataFrame(getDifferenced(i, train))\n",
    "    #print(value)\n",
    "    differenced_train = pd.concat([differenced_train,value],axis=1)\n",
    "    \n",
    "differenced_train.columns = train.columns\n",
    "differenced_train.index = train.index    \n",
    "\n",
    "differenced_test = pd.DataFrame()\n",
    "for i in range(0,len(d)):\n",
    "    value = pd.DataFrame(getDifferenced(i, test))\n",
    "    differenced_test = pd.concat([differenced_test,value],axis=1)\n",
    "\n",
    "differenced_test.columns = test.columns\n",
    "differenced_test.index = test.index\n",
    "\n",
    "differenced = pd.DataFrame()\n",
    "differenced = pd.concat([differenced_train,differenced_test],axis=0)\n",
    "'''\n",
    "differenced = pd.concat([out.result.iloc[:,0],differenced.iloc[:,1:]],axis=1)\n",
    "\n",
    "#pool01 = concurrent.futures.ProcessPoolExecutor(cores)\n",
    "\n",
    "#futures01 = [pool01.submit(getDifferenced, args) for args in (range(0,len(d)))]\n",
    "\n",
    "#wait(futures01, timeout=None, return_when=ALL_COMPLETED)\n",
    "#'''\n",
    "\n",
    "'''\n",
    "differenced = pd.DataFrame()\n",
    "for f in range(0,len(futures01)):\n",
    "    value = pd.DataFrame(futures01[f].result())\n",
    "    differenced = pd.concat([differenced,value],axis=1)\n",
    "    \n",
    "differenced.columns = train.columns\n",
    "differenced.index = train.index\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2d32f-b9a1-4196-9d43-1359b65f1c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f492f94d-cba1-455a-b1d4-157a346f6d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_ = dtale.show(out.result)\n",
    "d_.open_browser()\n",
    "d_._url  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4273fdd8-d805-4928-b7dd-bda427cf51fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbfa758-3cf3-4410-bfc6-85105b37a19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0e2820-110a-4786-9035-98543c301b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for f in range(0,len(futures01)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "        plt.hist(Differenced_Set.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        Differenced_Set.iloc[:,1].plot()\n",
    "        plt.show()\n",
    "        plt.hist(all_data.iloc[:,f], bins='auto')  # arguments are passed to np.histogram\n",
    "        plt.show()\n",
    "        all_data.iloc[:,1].plot()\n",
    "        plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce862ad-f7fc-4f0b-b376-c29d32d01281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29b6b7d-d0c5-4077-b822-d79e626067b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49f670-bbdb-42a8-b9e3-e75e1b60e830",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = out.result.corr()\n",
    "#.abs()\n",
    "s = c.unstack()\n",
    "so = s.sort_values(kind=\"quicksort\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0016982-6857-424e-868a-da44b16ff66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ece4be0-8244-434e-b66e-9c1f39a7c2c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4a92b7-e5b3-49ed-b31f-5959f90e8fca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "def critical_r(n, alpha = .05 ):\n",
    "    df = n - 2\n",
    "    critical_t = scipy.stats.t.isf(alpha / 2, df)\n",
    "    critical_r = np.sqrt( (critical.t^2) / ( (critical.t^2) + df ) )\n",
    "    return(critical_r)\n",
    "\n",
    "def xcorr(x, y, maxlags=10):\n",
    "    Nx = len(x)\n",
    "    if Nx != len(y):\n",
    "        raise ValueError('x and y must be equal length')\n",
    "\n",
    "    c = np.correlate(x, y, mode=2)\n",
    "\n",
    "    if maxlags is None:\n",
    "        maxlags = Nx - 1\n",
    "\n",
    "    if maxlags >= Nx or maxlags < 1:\n",
    "        raise ValueError('maxlags must be None or strictly positive < %d' % Nx)\n",
    "\n",
    "    c = c[Nx - 1 - maxlags:Nx + maxlags]\n",
    "\n",
    "    return c\n",
    "\n",
    "def getLagged_Set(set_, og):\n",
    "\n",
    "    train, test = train_test_split(set_, test_size=tsize, shuffle=False)\n",
    "    \n",
    "    residuals = pd.DataFrame(np.transpose(get_residuals(train).reshape(len(train.columns),len(train))))\n",
    "    #print(residuals)\n",
    "\n",
    "    residuals.columns = train.columns\n",
    "    residuals.index = train.index\n",
    "\n",
    "    #print(len(train))\n",
    "    #set_.index = all_data.index\n",
    "    \n",
    "    Lagged_Differenced_Set = pd.DataFrame()\n",
    "    Lagged_Set = pd.DataFrame()\n",
    "    lags = []\n",
    "    lagcorrs = []\n",
    "    ogcorrs = []\n",
    "\n",
    "    for f in range(1,len(train.columns)):\n",
    "        #print(f)\n",
    "        #print(len(futures01[f].result()))\n",
    "\n",
    "        data_1 = residuals.iloc[:,0]\n",
    "        data_2 = residuals.iloc[:,f]\n",
    "\n",
    "        ogc = np.array(pd.concat([data_1 - np.mean(data_1),data_2 - np.mean(data_2)],axis=1).corr())[1,0]    \n",
    "        ogcorrs.append(ogc)\n",
    "\n",
    "        #corr = xcorr(data_1 - np.mean(data_1), data_2 - np.mean(data_2),maxlags=5)\n",
    "\n",
    "        set1 = data_1 - np.mean(data_1)\n",
    "        set2 = data_2 - np.mean(data_2)\n",
    "\n",
    "        corrs_ = []\n",
    "        for i in range(0,maxl):\n",
    "            c = np.array((pd.concat([set1,set2.shift(i)],axis=1).dropna()).corr())[0,1]\n",
    "            corrs_.append(c)\n",
    "\n",
    "        #corr = np.correlate(data_1 - np.mean(data_1), data_2 - np.mean(data_2),mode='full')\n",
    "        #plt.plot(corr)\n",
    "        #plt.show()\n",
    "\n",
    "        #lag = corr.argmax() - (len(data_1) - 1)\n",
    "        lag = abs(pd.Series(corrs_)).idxmax()\n",
    "\n",
    "        #print(corr)\n",
    "        lagc = np.array(pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1).corr())[1,0]\n",
    "\n",
    "        #print(lag)\n",
    "\n",
    "        #print(ogc)\n",
    "        #print(lagc)\n",
    "\n",
    "        #print(lag)\n",
    "        #plt.plot(data_1, 'r*')\n",
    "        #plt.plot(data_2, 'b*')\n",
    "\n",
    "        lag_merged = pd.concat([data_1 - np.mean(data_1),(data_2 - np.mean(data_2)).shift(lag)],axis=1)\n",
    "\n",
    "        #x_ticks = all_data.index[np.arange(0, len(all_data.index), 5)]\n",
    "        #plt.xticks(x_ticks, rotation = 45)    \n",
    "        #plt.show()\n",
    "\n",
    "        #plt.scatter(data_2.shift(lag),data_1)\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_acf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        #plot_pacf(data_2.shift(lag))\n",
    "        #plt.show()\n",
    "\n",
    "        #y = data_1\n",
    "        #X = data_2\n",
    "        #reg = LinearRegression().fit(X, y)\n",
    "\n",
    "        #print(reg.score(X, y),reg.coef_,reg.intercept_)\n",
    "\n",
    "        #model = sm.OLS(y,X)\n",
    "        #results = model.fit()\n",
    "        #print(results.summary())\n",
    "\n",
    "        #Lagged_Differenced_Set = pd.concat([Lagged_Differenced_Set,data_2.shift(lag)],axis=1)\n",
    "        #TrainO_Lagged_Set = pd.concat([TrainO_Lagged_Set,all_data.iloc[:,f].shift(lag)],axis=1)\n",
    "        #if lag>0:\n",
    "            #lag = 0\n",
    "\n",
    "        Lagged_Set = pd.concat([Lagged_Set,og.iloc[:,f].shift(lag)],axis=1)\n",
    "\n",
    "        lagcorrs.append(lagc)\n",
    "\n",
    "        lags.append(lag)\n",
    "\n",
    "    Lagged_Set.index = set_.index\n",
    "    #stats = pd.concat([pd.DataFrame(set_.columns[1:]),pd.DataFrame(lags),pd.DataFrame(lagcorrs),pd.DataFrame(ogcorrs)],axis=1)\n",
    "    stats = pd.concat([pd.DataFrame(set_.columns[1:]),pd.DataFrame(lags),pd.DataFrame(ogcorrs)],axis=1)\n",
    "    \n",
    "    #print(Lagged_Set)\n",
    "    #return stats, pd.concat([data_1,Lagged_Set],axis=1),  pd.concat([data_1,Lagged_Differenced_Set],axis=1)\n",
    "    return stats, pd.concat([set_.iloc[:,0],Lagged_Set],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb26a0-bb46-4fd9-b9b8-2de6a3a7f875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e80173c-962f-4b0e-b170-8186d5f9b635",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lagged_Set = pd.DataFrame()\n",
    "Lagged_Differenced_Set = pd.DataFrame()\n",
    "\n",
    "#'''\n",
    "#Lagged_Differenced_Set_offset = pd.concat([out.result.iloc[:,0].pct_change().shift(-1),out.result.iloc[:,1:].pct_change()],axis=1)\n",
    "differenced_set = out.result.pct_change()\n",
    "differenced_set = differenced_set.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "\n",
    "ls_stats, Lagged_Differenced_Set= getLagged_Set(differenced_set, out.result)\n",
    "lso_stats, Lagged_Differenced_Set_offset= getLagged_Set(pd.concat([differenced_set.iloc[:,0].shift(-1),differenced_set.iloc[:,1:]],axis=1), out.result)\n",
    "#add in last period of y\n",
    "Lagged_Differenced_Set_offset = pd.concat([Lagged_Differenced_Set_offset.iloc[:,0],differenced_set.iloc[:,0],Lagged_Differenced_Set_offset.iloc[:,1:]],axis=1)\n",
    "#Lagged_Set_offset.index = out.result.index\n",
    "Lagged_Differenced_Set.dropna(inplace= True)\n",
    "#Lagged_Set_offset.columns = out.result.columns\n",
    "\n",
    "Lagged_Differenced_Set_offset.dropna(inplace= True)\n",
    "\n",
    "#Lagged_Differenced_Set.dropna(inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b19b33-dfd6-4233-8e3c-4f266a9ce8f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541e943c-2c5b-4cc8-86ca-87a1dab05a44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c83e18-ed0b-446d-89a3-86ea42e5672c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "temp = pd.DataFrame()\n",
    "temp = pd.concat([out.result.iloc[:,0].pct_change().shift(-1),Lagged_Set_offset.iloc[:,1:].pct_change()],axis=1).copy()\n",
    "Lagged_Differenced_Set_offset = pd.DataFrame()\n",
    "Lagged_Differenced_Set_offset = temp.copy()\n",
    "Lagged_Differenced_Set_offset.dropna(inplace= True)\n",
    "Lagged_Differenced_Set_offset\n",
    "Lagged_Differenced_Set_offset = Lagged_Differenced_Set_offset.replace([np.inf, -np.inf, np.NaN], 0)\n",
    "#Lagged_Differenced_Set_offset.fillna(0)\n",
    "#preprocessing.StandardScaler().fit(Lagged_Differenced_Set_offset)\n",
    "'''\n",
    "np.sum(Lagged_Differenced_Set_offset.isin([np.inf, -np.inf, np.NaN])).sort_values(kind=\"quicksort\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfbc252-b567-48e9-86f9-9e57df0ac45c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ls_stats)\n",
    "print(lso_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53aa28e7-3712-466f-96fd-7bbf115714ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fb84f-111c-44f6-8fcc-7519b387e5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb40926-6979-4f9f-ad09-ed1f93c25a80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1422b2e-d8e4-43aa-a691-8c54392a6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skip correlation\n",
    "#Lagged_Differenced_Set_offset = pd.concat([our.result.iloc[:,0].pct_change().shift(-1),differenced_set.iloc[:,0],Lagged_Differenced_Set_offset.iloc[:,1:]],axis=1)\n",
    "#Lagged_Set_offset.index = out.result.index\n",
    "#Lagged_Differenced_Set.dropna(inplace= True)\n",
    "\n",
    "set_ = Lagged_Differenced_Set\n",
    "set_ = pd.concat([differenced_set.iloc[:,0].shift(-1),differenced_set.iloc[:,0],differenced_set.iloc[:,1:]],axis=1)\n",
    "set_.dropna(inplace=True)\n",
    "set_ = set_.tail(-1)\n",
    "transformed, lambdas = transform_boxcox(set_.dropna())\n",
    "\n",
    "#revert_yeo(Lagged_Differenced_Set_offset, transformed, lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c72722f-fbff-4bff-92b8-26b19888e415",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc90237-f851-4afb-86f8-2dca0b41123d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cbd = True\n",
    "ic = True\n",
    "\n",
    "# evaluate an elastic net model on the dataset\n",
    "#tsize = .20\n",
    "#train, test = train_test_split(Lagged_Differenced_Set_offset.iloc[:,0:], test_size=tsize, shuffle=False)\n",
    "train, test = train_test_split(transformed.iloc[:,0:], test_size=tsize, shuffle=False)\n",
    "\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=ic)\n",
    "\n",
    "#train_t, lambdas_t = transform_boxcox(train)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    train_t = train\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(train_t)\n",
    "\n",
    "#\n",
    "train_s = pd.DataFrame(scaler.transform(train_t))\n",
    "train_s.columns = train.columns\n",
    "train_s.index = train.index  \n",
    "\n",
    "train_t = train_s\n",
    "\n",
    "#test_t = transform_boxcox_l(test, lambdas_t)\n",
    "\n",
    "#disabled boxcox\n",
    "if cbd:\n",
    "    test_t = test\n",
    "\n",
    "test_s = pd.DataFrame(scaler.transform(test_t))\n",
    "test_s.columns = test.columns\n",
    "test_s.index = test.index\n",
    "\n",
    "test_t = test_s\n",
    "\n",
    "y_train = pd.DataFrame(train_t.iloc[:,0])\n",
    "\n",
    "#exclude y\n",
    "\n",
    "X_inter_train = pd.DataFrame(interaction.fit_transform(train_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_t.iloc[:,1:]).columns))\n",
    "\n",
    "    #apply ZCA each time a set of factors are removed (i.e. iteratively)\n",
    " #trf = zca.ZCA().fit(X_inter_train)\n",
    "  #trf = zca.ZCA().fit(X_train)\n",
    "\n",
    " #X_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "  #X_train = pd.DataFrame(trf.transform(X_train))\n",
    " #X_train.columns=X_inter_train.columns\n",
    "  #X_train.columns=X_train.columns\n",
    "  #X_train.index = train.index\n",
    "\n",
    "#X_inter_alt = X_train.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#print(X_inter_alt.head(3))\n",
    "\n",
    "y_test = pd.DataFrame(test_t.iloc[:,0])\n",
    "\n",
    "X_inter_test = pd.DataFrame(interaction.fit_transform(test_t.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(test_t.iloc[:,1:]).columns))\n",
    "\n",
    " #X_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "  #X_test = pd.DataFrame(trf.transform(X_test))\n",
    "\n",
    " #X_test.columns=X_inter_test.columns\n",
    "  #X_test.columns=X_test.columns\n",
    "  #X_test.index = test.index\n",
    "\n",
    "#X_inter_t_alt = X_test.iloc[:, np.array(range(0,len(all_data.iloc[:,2:].columns)))]\n",
    "#X_inter_t_alt.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf46d0-1162-46d6-a166-0461b321e0c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define model evaluation method\n",
    "from sklearn.experimental import enable_halving_search_cv  # noqa\n",
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "\n",
    " #model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=4, verbose=0, precompute='auto')\n",
    "\n",
    "model = ElasticNet()\n",
    "grid = dict()\n",
    "# fit model\n",
    "\n",
    "grid['alpha'] = alphas\n",
    "grid['l1_ratio'] = ratios\n",
    "\n",
    "#search = HalvingRandomSearchCV(model, grid,resource='n_samples',max_resources=10,random_state=0)\n",
    "\n",
    "search = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    " #results = search.fit(X_inter_train, y_train)\n",
    "#results = search.fit(X_train, y_train)\n",
    "\n",
    "results = search.fit(X_inter_train, y_train)\n",
    "\n",
    " #model.fit(X_inter_train, y_train)\n",
    "# summarize chosen configuration\n",
    "\n",
    "print('MAE: %.3f' % results.best_score_)\n",
    "print('Config: %s' % results.best_params_)\n",
    "\n",
    "print(results.best_estimator_)\n",
    "best_model = ElasticNet(alpha=results.best_estimator_.alpha, l1_ratio = results.best_estimator_.l1_ratio)\n",
    "\n",
    "#pd.concat([all_data[Y],all_data_int],axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebb7965-c50b-4f1e-bd5e-05504a69192f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model.fit(X_inter_train,train_t.iloc[:,0])\n",
    "\n",
    "trainScore = best_model.score(X_inter_train, y_train, sample_weight=None)\n",
    "testScore = best_model.score(X_inter_test, y_test, sample_weight=None)\n",
    "print(trainScore)\n",
    "print(testScore)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb694fa2-0e07-4a01-8648-2d6615a70b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "093a64f9-522d-4761-adf1-f55d566a1b70",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922b12fd-2b69-4243-909a-6f828aa43b36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e86999a-16ba-4505-b5ec-843d219ac62e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367906e7-5aec-4529-8f29-42040e9ac43d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14400790-a56a-4084-8c56-cf73697546e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#classification transforms\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "interaction = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import random\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import SVMSMOTE \n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "#set_ = Lagged_Differenced_Set_offset.copy()\n",
    "#set_ = transformed.copy()\n",
    "#DF_list = list()\n",
    "\n",
    "sets = list()\n",
    "sets.append(transformed.copy())\n",
    "#sets.append(Lagged_Differenced_Set_offset.copy())\n",
    "sets.append(set_.copy())\n",
    "\n",
    "def return_sets(sets):    \n",
    "    \n",
    "    sets_outer = list()\n",
    "    \n",
    "    for i in sets:\n",
    "        \n",
    "        sets_multiple = list()\n",
    "        \n",
    "        set_ = pd.DataFrame()\n",
    "        set_ = i\n",
    "\n",
    "        X, y = set_.iloc[:,1:],  set_.iloc[:,0]\n",
    "        \n",
    "        print(mean(y))\n",
    "\n",
    "        y = pd.DataFrame(np.where(y > mean(y), 1, 0))\n",
    "        y.index = set_.index\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=tsize, shuffle=False)\n",
    "\n",
    "        a = set_.iloc[:,0].loc[X_train.index]\n",
    "        b = set_.iloc[:,0].loc[X_test.index]\n",
    "        plt.hist(a)\n",
    "        plt.show()\n",
    "        plt.hist(b)\n",
    "        plt.show()\n",
    "        print(stats.ttest_ind(a,b, equal_var = False))\n",
    "\n",
    "        one = y_train[y_train==0].dropna().sample(int(len(X_train)*2), replace=True, random_state=1)\n",
    "        two = y_train[y_train==1].dropna().sample(int(len(X_train)*2), replace=True, random_state=1)\n",
    "\n",
    "        train_index = np.append(one.index,two.index)\n",
    "\n",
    "        #destroyed my data\n",
    "        #smote = SVMSMOTE(random_state=42)\n",
    "            #smote = SMOTE()\n",
    "        #Xsm_train, ysm_train = smote.fit_sample(np.array(X_train),np.array(y_train))\n",
    "\n",
    "        Xsm_train = X_train.copy()\n",
    "        ysm_train = y_train.copy()\n",
    "        \n",
    "        #use with optimal lags\n",
    "        #y_test = y_test.loc[y_test.index>y_test.index[maxl]]\n",
    "        #X_test = X_test.loc[X_test.index>X_test.index[maxl]]\n",
    "\n",
    "        scaler = preprocessing.StandardScaler().fit(Xsm_train)\n",
    "\n",
    "        X_train_transformed = pd.DataFrame(scaler.transform(Xsm_train))\n",
    "        X_train_transformed.columns = X_train.columns\n",
    "\n",
    "        X_inter_train = X_train_transformed.copy()\n",
    "\n",
    "        #X_inter_train = pd.DataFrame(interaction.fit_transform(X_train_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_train_transformed).columns))\n",
    "\n",
    "        trf = zca.ZCA().fit(X_inter_train)\n",
    "\n",
    "        #X_inter_train = pd.DataFrame(trf.transform(X_inter_train))\n",
    "        X_inter_train.columns=pd.DataFrame(X_inter_train).columns\n",
    "        #X_inter_train.index = X_train.loc[train_index].index\n",
    "\n",
    "        X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "        X_test_transformed.columns = X_test.columns\n",
    "        X_test_transformed.index = X_test.index\n",
    "\n",
    "        X_inter_test = X_test_transformed\n",
    "\n",
    "        #X_inter_test = pd.DataFrame(interaction.fit_transform(X_test_transformed), columns=interaction.get_feature_names(input_features=pd.DataFrame(X_test_transformed).columns))\n",
    "\n",
    "        #X_inter_test = pd.DataFrame(trf.transform(X_inter_test))\n",
    "\n",
    "        sets_multiple.append(X_inter_train)\n",
    "        sets_multiple.append(y_train)\n",
    "        sets_multiple.append(X_inter_test)\n",
    "        sets_multiple.append(y_test)        \n",
    "    \n",
    "        sets_outer.append(sets_multiple)\n",
    "        \n",
    "    return(sets_outer)\n",
    "\n",
    "values = return_sets(sets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568baa66-4a53-4f23-ba8a-c5f9408f1591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ec310d-7f86-4154-be9b-11c927a6a4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e560215f-9b7d-4d76-ac61-dbf96d2cfd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "#works best\n",
    "#class balanced, no zca\n",
    "from sklearn import svm\n",
    "\n",
    "for i in range(0,len(sets)):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    clf = svm.SVC(kernel='rbf', C=1, random_state=42)\n",
    "\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "\n",
    "    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n",
    "\n",
    "    clf = svm.SVC(C=1).fit(X_train, y_train)\n",
    "\n",
    "    train_predict = clf.predict(X_train)\n",
    "    predicteds = clf.predict(X_test)\n",
    "    #predicted.index = y_test.index\n",
    "\n",
    "    print(clf.score(X_train, y_train))\n",
    "    print(clf.score(X_test, y_test))\n",
    "\n",
    "    #clf.predict(X_test)\n",
    "    #print(predicteds)\n",
    "    print(metrics.classification_report(y_train,train_predict))\n",
    "    print(metrics.classification_report(y_test,predicteds))\n",
    "    print(metrics.confusion_matrix(y_train,train_predict))\n",
    "    print(metrics.confusion_matrix(y_test,predicteds))\n",
    "\n",
    "    predictions_s = arima_adjust(y_train, y_test, train_predict, predicteds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec38a51f-ede1-47bb-9af7-ee25b6106160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25944973-ee0b-4b74-be6c-f59463229bd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c39fe4e-d604-4fa9-ad81-ee7a53a06784",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3909eb8a-645f-47c5-933a-98d61f3d6ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),('svc', svm.SVC(kernel='sigmoid', C=1, random_state=42))])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)\n",
    "'''\n",
    "#search = cross_val_score(pipe, X_inter_train, y_train.loc[train_index], cv=10)\n",
    "#GridSearchCV(pipe, param_grid, n_jobs=-1)\n",
    "#search.fit(X_digits, y_digits)\n",
    "#print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "#print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83b6ec4-6148-41ba-9d1b-3765f913ee93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b4466-cd31-4f19-85e0-c32db62dbe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic1\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "for i in range(0,len(sets)):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    modell = LogisticRegression()\n",
    "    modell.fit(X_train, y_train)\n",
    "    print(modell)\n",
    "\n",
    "    #expected = y_test\n",
    "    train_predict = modell.predict(X_train)\n",
    "    predictedl = modell.predict(X_test)\n",
    "\n",
    "    print(modell.score(X_train, y_train))\n",
    "    print(modell.score(X_test, y_test))\n",
    "\n",
    "    #clf.predict(X_test)\n",
    "    #print(predicteds)\n",
    "    print(metrics.classification_report(y_train,train_predict))\n",
    "    print(metrics.classification_report(y_test,predictedl))\n",
    "    print(metrics.confusion_matrix(y_train,train_predict))\n",
    "    print(metrics.confusion_matrix(y_test,predictedl))\n",
    "    \n",
    "    predictions_l = arima_adjust(y_train, y_test, train_predict, predictedl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672895b-f442-45b4-8cb1-3257e8c830a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "for i in range(0,len(sets)):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    modeln = GaussianNB()\n",
    "    modeln.fit(X_train, y_train)\n",
    "    print(modeln)\n",
    "\n",
    "    #expected = y_test\n",
    "    train_predict = modeln.predict(X_train)\n",
    "    predictedn = modeln.predict(X_test)\n",
    "\n",
    "    print(modeln.score(X_train, y_train))\n",
    "    print(modeln.score(X_test, y_test))\n",
    "\n",
    "    #clf.predict(X_test)\n",
    "    #print(predicteds)\n",
    "    print(metrics.classification_report(y_train,train_predict))\n",
    "    print(metrics.classification_report(y_test,predictedn))\n",
    "    print(metrics.confusion_matrix(y_train,train_predict))\n",
    "    print(metrics.confusion_matrix(y_test,predictedn))\n",
    "    \n",
    "    predictions_n = arima_adjust(y_train, y_test, train_predict, predictedn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644b63d8-9f6e-453e-b896-0ee262d1242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DecisionTreeClassifier\n",
    "#works best with transformed\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "for i in reversed(range(0,len(sets))):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "\n",
    "    modeld = DecisionTreeClassifier()\n",
    "    modeld.fit(X_train, y_train)\n",
    "    print(modeld)\n",
    "\n",
    "    #expected = y_test\n",
    "    train_predict = modeld.predict(X_train)\n",
    "    predictedd = modeld.predict(X_test)\n",
    "\n",
    "    print(modeld.score(X_train, y_train))\n",
    "    print(modeld.score(X_test, y_test))\n",
    "\n",
    "    #clf.predict(X_test)\n",
    "    #print(predicteds)\n",
    "    print(metrics.classification_report(y_train,train_predict))\n",
    "    print(metrics.classification_report(y_test,predictedd))\n",
    "    print(metrics.confusion_matrix(y_train,train_predict))\n",
    "    print(metrics.confusion_matrix(y_test,predictedd))\n",
    "        \n",
    "    predictions_d = arima_adjust(y_train, y_test, train_predict, predictedd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0671f-8931-4f7e-936b-51adcb52dab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#knn\n",
    "#works best with transformed\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV#create new a knn model\n",
    "\n",
    "for i in range(0,len(sets)):\n",
    "    \n",
    "    X_train = values[i][0]\n",
    "    y_train = values[i][1]\n",
    "    X_test = values[i][2]\n",
    "    y_test = values[i][3]\n",
    "    \n",
    "    knn2 = KNeighborsClassifier(metric='euclidean',weights='distance')#create a dictionary of all values we want to test for n_neighbors\n",
    "    param_grid = {'n_neighbors': np.arange(1, 25)}#use gridsearch to test all values for n_neighbors\n",
    "    knn_gscv = GridSearchCV(knn2, param_grid, cv=5)#fit model to data\n",
    "    knn_gscv.fit(X_train, y_train)\n",
    "    #check top performing n_neighbors value\n",
    "    print(knn_gscv.best_params_)\n",
    "    #check mean score for the top performing value of n_neighbors\n",
    "    print(knn_gscv.best_score_)\n",
    "\n",
    "    modelk = KNeighborsClassifier(n_neighbors = knn_gscv.best_params_['n_neighbors'],metric='euclidean',weights='distance')\n",
    "    modelk.fit(X_train,y_train)\n",
    "    train_predict = modelk.predict(X_train)\n",
    "    predictedk = modelk.predict(X_test)\n",
    "    #modelk.score(X_test, y_test)\n",
    "\n",
    "    print(modelk.score(X_train, y_train))\n",
    "    print(modelk.score(X_test, y_test))\n",
    "\n",
    "    #clf.predict(X_test)\n",
    "    #print(predicteds)\n",
    "    print(metrics.classification_report(y_train,train_predict))\n",
    "    print(metrics.classification_report(y_test,predictedk))\n",
    "    print(metrics.confusion_matrix(y_train,train_predict))\n",
    "    print(metrics.confusion_matrix(y_test,predictedk))\n",
    "    \n",
    "    predictions_k = arima_adjust(y_train, y_test, train_predict, predictedk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43107e46-54bb-4837-a798-af32efaccadb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55c7caf-0c3b-4911-ba58-8313a7ba85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "#Elastic logistic\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "# define model\n",
    "ratios = arange(0, 1, 0.01)\n",
    "alphas = (10 ** np.linspace(-1, 1, 10))/10\n",
    "alphas = np.append(alphas,[10.0, 100.0])\n",
    "\n",
    "model = SGDClassifier(loss=\"log\", penalty=\"elasticnet\")\n",
    "grid = dict()\n",
    "# fit model\n",
    "\n",
    "grid['alpha'] = alphas\n",
    "grid['l1_ratio'] = ratios\n",
    "\n",
    "search_l = GridSearchCV(model, grid, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n",
    "\n",
    "results_l = search_l.fit(X_inter_train, ysm_train)\n",
    "\n",
    " #model.fit(X_inter_train, y_train)\n",
    "# summarize chosen configuration\n",
    "\n",
    "print('MAE: %.3f' % results_l.best_score_)\n",
    "print('Config: %s' % results_l.best_params_)\n",
    "\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "print(results_l.best_estimator_)\n",
    "best_model_l = SGDClassifier(alpha=results_l.best_estimator_.alpha, l1_ratio = results_l.best_estimator_.l1_ratio)\n",
    "\n",
    "#calibrator = CalibratedClassifierCV(best_model_l, cv='prefit')\n",
    "#model=calibrator.fit(X_inter_train, y_train)\n",
    "\n",
    "best_model_l.fit(X_inter_train,ysm_train)\n",
    "\n",
    "#sgd = SGDClassifier()\n",
    "\n",
    "#sgd.partial_fit(X_inter_train,y_train, classes=[0,1])\n",
    "\n",
    "print(pd.DataFrame(np.transpose(best_model_l.coef_)).set_index(X_inter_train.columns))\n",
    "\n",
    "trainScore_l = best_model_l.score(X_inter_train, ysm_train, sample_weight=None)\n",
    "testScore_l = best_model_l.score(X_inter_test, y_test, sample_weight=None)\n",
    "print(trainScore_l)\n",
    "print(testScore_l)\n",
    "\n",
    "#print(help(KNeighborsClassifier))\n",
    "predictedl = best_model_l.predict(X_inter_test)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b9087-fe22-4d54-b152-a40e48784774",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7015d3fd-f35c-4f6c-b8e9-f66c92514715",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dd1212-7817-4f74-95d6-0493d68c9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#predictedf = pd.concat([pd.DataFrame(predicteds),pd.DataFrame(predictedk),pd.DataFrame(predictedl)],axis=1)\n",
    "predictedf = pd.concat([pd.DataFrame(predictions_s),pd.DataFrame(predictions_k),pd.DataFrame(predictions_d),pd.DataFrame(predictions_l),pd.DataFrame(predictions_n)],axis=1)\n",
    "predictedf.columns = [\"s\",\"k\",\"d\",\"l\",\"n\"]\n",
    "predictedf.index = y_test.index\n",
    "print(predictedf)\n",
    "print(confusion_matrix(y_test, predictedf.mode(axis=1).iloc[:,0]))\n",
    "#print(confusion_matrix(y_test, predictedf.median(axis=1)))\n",
    "print(classification_report(y_test, predictedf.mode(axis=1).iloc[:,0]))\n",
    "#print(classification_report(y_test, predictedf.median(axis=1)))\n",
    "\n",
    "print(mean(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438843e-110f-404f-b1b1-47bfa41b560b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4dfc37-ab80-427f-bc37-edb9a212c4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate output multi-step 1d cnn example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Dense, SimpleRNN, GRU, LSTM\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Lagged_Differenced_Set = (all_data - all_data.shift(-1)).dropna()\n",
    "choice = pd.concat([out.result.iloc[:,0],differenced_set],axis=1)\n",
    "\n",
    "train, test = train_test_split(choice, test_size=tsize, shuffle=False)\n",
    "\n",
    "transformed_train, lambdas_ = transform_boxcox(train)\n",
    "transformed_test = transform_boxcox_l(test, lambdas_)\n",
    "scaler = preprocessing.StandardScaler().fit(transformed_train)\n",
    "train_s = pd.DataFrame(scaler.transform(transformed_train))\n",
    "test_s = pd.DataFrame(scaler.transform(transformed_test))\n",
    "\n",
    "#scaler = preprocessing.StandardScaler().fit(train)\n",
    "#train_s = pd.DataFrame(scaler.transform(train))\n",
    "#test_s = pd.DataFrame(scaler.transform(test))\n",
    "\n",
    "combined = pd.concat([train_s,test_s],axis=0)\n",
    "combined.index = choice.index\n",
    "combined.columns = choice.columns\n",
    "\n",
    "set_ = combined\n",
    "\n",
    "def plot_learning_curves(loss, val_loss):\n",
    "    plt.plot(np.arange(len(loss)) + 0.5, loss, \"b.\", label=\"Training loss\")\n",
    "    plt.plot(np.arange(len(val_loss)) + 1, val_loss, \"r-\", label=\"Validation loss\")\n",
    "    plt.gca().xaxis.set_major_locator(mpl.ticker.MaxNLocator(integer=True))\n",
    "    plt.legend(fontsize=14)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out, xcolumns, ycolumns):\n",
    "    \n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, xcolumns], sequences[end_ix:out_end_ix, ycolumns]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    \n",
    "    return array(X), array(y)\n",
    "\n",
    "#n_steps_in = int(np.floor(len(train)*.8))\n",
    "print(len(set_))\n",
    "n_steps_in = int(np.round(len(set_)/2))\n",
    "print(len(set_) - n_steps_in)\n",
    "print(n_steps_in)\n",
    "#n_steps_out = int(len(train)-n_steps_in)\n",
    "n_steps_out = 1\n",
    "print(n_steps_out)\n",
    "\n",
    "#ycolumns = range(0,len(transformed.columns))\n",
    "ycolumns = range(0,len(set_.columns[0:1].values))\n",
    "xcolumns = range(1,len(set_.columns[1:]))\n",
    "\n",
    "#trainInner, valid = train_test_split(trainOuter, test_size=0.15, shuffle=False)\n",
    "#trainInner_whitened = pd.DataFrame(trf.transform(trainInner.iloc[:,1:])).set_index(trainInner.index)\n",
    "#valid_whitened = pd.DataFrame(trf.transform(valid.iloc[:,1:])).set_index(valid.index)\n",
    "#test_whitened = pd.DataFrame(trf.transform(test.iloc[:,1:])).set_index(test.index)\n",
    "\n",
    "#trainOuter_whitened=pd.DataFrame(trf.transform(trainOuter.iloc[:,1:])).set_index(trainOuter.index)\n",
    "\n",
    "X, y = split_sequences(np.array(set_), n_steps_in, n_steps_out, xcolumns, ycolumns)\n",
    "\n",
    "#for i in range(len(X)):\n",
    "#    print(X[i], y[i])\n",
    "    \n",
    "#flatten output\n",
    "\n",
    "if len(ycolumns) > 0:\n",
    "    n_output = y.shape[1] * y.shape[2]\n",
    "    y = y.reshape((y.shape[0], n_output))\n",
    "\n",
    "n_features = X.shape[2]\n",
    "\n",
    "# define model\n",
    "modelCNN = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in, n_features)),\n",
    "    keras.layers.MaxPooling1D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(50, activation='relu'),\n",
    "    keras.layers.Dense(n_output)\n",
    "])\n",
    "\n",
    "# fit model\n",
    "#model.fit(X, y, epochs=7000, verbose=0)\n",
    "\n",
    "epochs_ = 2000\n",
    "batch_size_ = 25\n",
    "\n",
    "#np.random.seed(42)\n",
    "#tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "#model.compile(optimizer='adam', loss='mse')\n",
    "modelCNN.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=tsize,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)\n",
    "#scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "\n",
    "#X_train_transformed = pd.DataFrame(scaler.transform(X_train))\n",
    "#X_train_transformed.columns = X_train.columns\n",
    "\n",
    "#X_test_transformed = pd.DataFrame(scaler.transform(X_test))\n",
    "#X_test_transformed.columns = X_test.columns\n",
    "#X_test_transformed.index = X_test.index\n",
    "\n",
    "    #new_series = pd.DataFrame(ts_train_scaled).append(pd.DataFrame(ts_valid_scaled)).append(pd.DataFrame(ts_test_scaled))\n",
    "\n",
    "    #series_reshaped = np.array([new_series[i:i + (n_steps+n_ahead)].copy() for i in range(len(data) - (n_steps+n_ahead))])  \n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
    "                                                    test_size=tsize,\n",
    "                                                    shuffle = False)\n",
    "                                                    #random_state=42)    \n",
    "\n",
    "#model6.compile(loss=\"MAPE\", optimizer=\"rmsprop\",metrics=['MAPE'])\n",
    "historyCNN = modelCNN.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history6 = model6.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,validation_data=(X_valid, y_valid), verbose=0)\n",
    "#history = model.fit(X_train, y_train, epochs=epochs_,batch_size=batch_size_,verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1ce87-6c86-4308-8788-b01aa8256fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to apply to test\n",
    "#revert_boxcox(pd.DataFrame(transformed_.iloc[:,0]),pd.DataFrame(lambdas[0]))\n",
    "#Lagged_Differenced_Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8f7e2d-3603-4ba1-8457-4fd9918498a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "plot_learning_curves(historyCNN.history[\"loss\"], historyCNN.history[\"val_loss\"])\n",
    "plt.show()\n",
    "#plot_learning_curves(history6.history[\"loss\"], history.history[\"val_loss\"])\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799f6d94-9757-4baa-b095-8937fbba5857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4c952-450d-43d4-86c1-2a37673a58d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(revert_transformed_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5ad28-e151-4ccf-915f-a9797fa263cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cbc3da-fa1f-4e57-9d84-f4e5f8872af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "#multi\n",
    "yhat = modelCNN.predict(X_test, verbose=0)\n",
    "\n",
    "predicted = []\n",
    "original = []\n",
    "if len(ycolumns) > 1:\n",
    "    results = yhat.reshape(len(y_test), int(yhat.shape[1]/X_test.shape[2]), len(choice.columns))\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(pd.DataFrame(results[i])[0][0])\n",
    "        original.append(pd.DataFrame(y_test[i])[0][0])\n",
    "        #print(pd.DataFrame(results[i])[0][0])\n",
    "        #print(pd.DataFrame(y_test[i])[0][0])\n",
    "else:\n",
    "    results = yhat\n",
    "    for i in range(0,len(results)):\n",
    "        predicted.append(results[i])\n",
    "        original.append(y_test[i])\n",
    "        #print(pd.DataFrame(results[i]))\n",
    "        #print(pd.DataFrame(y_test[i]))\n",
    "        \n",
    "#revert_transformed_predicted = inverse_yeo((scaler.mean_[0]+scaler.scale_[0]*pd.DataFrame(predicted)).values,(scaler.mean_[0]+scaler.scale_[0]*pd.DataFrame(predicted)).values,lambdas_.values[0])\n",
    "#revert_transformed_original = inverse_yeo((scaler.mean_[0]+scaler.scale_[0]*pd.DataFrame(original)).values,(scaler.mean_[0]+scaler.scale_[0]*pd.DataFrame(original)).values,lambdas_.values[0])\n",
    "revert_transformed_predicted = (scaler.mean_[0]+scaler.scale_[0]*pd.DataFrame(predicted))\n",
    "revert_transformed_original = (scaler.mean_[0]+scaler.scale_[0]*pd.DataFrame(original))\n",
    "\n",
    "plt.scatter(revert_transformed_predicted,revert_transformed_original)\n",
    "temp = pd.concat([revert_transformed_predicted,revert_transformed_original],axis=1)\n",
    "temp.columns = [\"predicted\", \"original\"]\n",
    "display(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8229628-eed4-4b76-831d-1de033240347",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef = pd.DataFrame(best_model.coef_).set_index(X_inter_train.columns)\n",
    "\n",
    "a_coef = abs(coef)\n",
    "a_coef.sort_values(by=[0],ascending=False,inplace=True)\n",
    "chosen_few = a_coef[a_coef>0].dropna().index.values\n",
    "\n",
    "scaler_ = preprocessing.StandardScaler().fit(transformed)\n",
    "\n",
    "#\n",
    "train_ = pd.DataFrame(scaler_.transform(transformed))\n",
    "train_.columns = transformed.columns\n",
    "train_.index = transformed.index  \n",
    "\n",
    "X_inter_train_ = pd.DataFrame(interaction.fit_transform(train_.iloc[:,1:]), columns=interaction.get_feature_names(input_features=pd.DataFrame(train_.iloc[:,1:]).columns))\n",
    "\n",
    "max_pvalue = 1\n",
    "New_Names = X_inter_train.columns\n",
    "X_b = X_inter_train_[chosen_few]\n",
    "while (max_pvalue > .05):\n",
    "        \n",
    "    trf = zca.ZCA().fit(X_b)\n",
    "        \n",
    "    X_b_z = pd.DataFrame(trf.transform(X_b))\n",
    "    X_b_z.columns=pd.DataFrame(X_b).columns\n",
    "    X_b_z.index = train_.index\n",
    "\n",
    "    model_ = sm.OLS(train_.iloc[:,0],sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    #model_ = sm.OLS(pd.DataFrame(pd.concat([train_.iloc[:,0],X_b_z],axis=1),sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip'))        \n",
    "    results_ = model_.fit()\n",
    "\n",
    "    set_ = X_b.columns.tolist()\n",
    "    \n",
    "    max_pvalue = max(results_.pvalues[1:])\n",
    "    if (max_pvalue > .05):\n",
    "        print(max_pvalue)\n",
    "        max_pname = (results_.pvalues[1:]).idxmax(axis=1)\n",
    "        set_.remove(max_pname)\n",
    "        New_Names = set_\n",
    "    \n",
    "        X_b = X_inter_train_[New_Names]\n",
    "        X_b.index = X_inter_train_.index\n",
    "\n",
    "#from statsmodels.formula.api import ols\n",
    "#lm = ols(pd.DataFrame(train_.iloc[:,0]) ~ sm.tools.tools.add_constant(X_b_z, prepend=True, has_constant='skip')).fit()\n",
    "#table = sm.stats.anova_lm(model_, type=3)\n",
    "#print(table)\n",
    "print(results_.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e428c-cce9-46e3-b145-754fc9e4c576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def findNaNCols(df_):\n",
    "    for col in df_:        \n",
    "        num_NaNs = df_[col].isnull().sum()\n",
    "        if num_NaNs > 0:\n",
    "            print(f\"Column: {col}\")\n",
    "            print(f\"Number of NaNs: {num_NaNs}\")\n",
    "\n",
    "findNaNCols(X_b_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d4e220-4ac2-4945-9e3a-4b8ff1355a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d940860-86d6-4768-81a0-63f9127143a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "linear_plot = Plot.LinearRegressionResidualPlot(X_b_z, pd.DataFrame(train_.iloc[:,0]))\n",
    "lm = linear_plot.fit()\n",
    "summary, diag_res = linear_plot.diagnostic_plots(lm)\n",
    "print(\"Summary of Regression\\n:{}\".format(summary))\n",
    "print(\"Diagnostic Tests of Regression\\n:{}\".format(diag_res))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#sns.pairplot(pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1), hue=Y, height=2);\n",
    "\n",
    "pd.concat([pd.DataFrame(train[Y]),X_b_z],axis=1).hist()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "model_s = sklearn.linear_model.LinearRegression()\n",
    "model_s.fit(X_b_z, pd.DataFrame(train_[Y]))\n",
    "\n",
    "shap.initjs()\n",
    "e = shap.explainers.Linear(model_s, X_b_z)\n",
    "\n",
    "shap_values = e.shap_values(X_b_z)\n",
    "shap.summary_plot(shap_values, X_b_z)\n",
    "shap.plots.heatmap(e(X_b_z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53b64b8-ca00-46d1-ba16-91f3adb00065",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4871c2-4418-491a-8e5c-5776d42e4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f\n",
    "\n",
    "from statsmodels.graphics.gofplots import ProbPlot\n",
    "residuals_normalized = lm.get_influence().resid_studentized_internal\n",
    "cooks = lm.get_influence().cooks_distance[0]\n",
    "cooks = np.round(f.pdf(cooks,len(lm.tvalues)+1, len(lm.fittedvalues)-len(lm.tvalues)-1),2)\n",
    "\n",
    "res_std = lm.get_influence().resid_std\n",
    "\n",
    "leverage = lm.get_influence().hat_matrix_diag\n",
    "\n",
    "plt.hist(pd.DataFrame(leverage))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(cooks))\n",
    "plt.show()\n",
    "\n",
    "plt.hist(pd.DataFrame(residuals_normalized))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "testNormal(residuals_normalized)\n",
    "\n",
    "w = res_std\n",
    "x = cooks\n",
    "y = leverage\n",
    "z = residuals_normalized\n",
    "\n",
    "fitted_y = lm.fittedvalues\n",
    "\n",
    "labels_ = fitted_y.index\n",
    "\n",
    "outlier_check = pd.concat([pd.DataFrame(x),pd.DataFrame(y),pd.DataFrame(z),pd.DataFrame(w)],axis=1).set_index(labels_)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres']\n",
    "\n",
    "qq = ProbPlot(residuals_normalized)\n",
    "\n",
    "c_thresh = .1\n",
    "l_thresh = (2*(len(lm.tvalues)-1)/len(lm.fittedvalues))\n",
    "s_thresh = max(qq.theoretical_quantiles)\n",
    "\n",
    "print(\"Outlier threshold's\")\n",
    "print(\"Cooks distance: > .1+\")\n",
    "print(\"Leverage: > \" + str(l_thresh) + \" to \" + str(3 * (len(lm.tvalues)-1)/len(fitted_y)))\n",
    "print(\"Studentized residuals: > \" + str(s_thresh))\n",
    "print()\n",
    "\n",
    "flag = []\n",
    "\n",
    "for i in range(0,len(outlier_check)):\n",
    "    if( (outlier_check.iloc[i][0] >= c_thresh) or (outlier_check.iloc[i][1] >= l_thresh) or (abs(outlier_check.iloc[i][2]) >= s_thresh) ):\n",
    "        print(outlier_check.iloc[i])\n",
    "        print()\n",
    "        flag.append(True)\n",
    "    else:\n",
    "        flag.append(False)\n",
    "\n",
    "outlier_check = pd.concat([outlier_check,pd.DataFrame(flag).set_index(labels_)],axis=1)\n",
    "\n",
    "outlier_check.columns =  ['cooks', 'leverage', 'tsres', 'sres', 'flagged']\n",
    "\n",
    "print(np.flip(np.argsort(cooks), 0))\n",
    "#print(outlier_check)\n",
    "\n",
    "search = outlier_check[outlier_check['flagged']==1].index.to_list()\n",
    "\n",
    "rows = []\n",
    "\n",
    "for i in search:\n",
    "    v = outlier_check.index.to_list().index(i)\n",
    "    rows.append(v)\n",
    "\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2b196-7761-4420-a145-a5411053a625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_style(row):\n",
    "\n",
    "    color = 'white'\n",
    "    if bool(row.flagged) == True:\n",
    "        color = 'orange'\n",
    "\n",
    "    return ['background-color: %s' % color]*len(row.values)\n",
    "\n",
    "all_data[set(all_data.columns) & set(New_Names)]\n",
    "\n",
    "outlier_check.style.apply(custom_style, axis=1).apply(custom_style, axis=1).background_gradient(cmap ='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da39b86-0ac2-4470-9457-8e999918d47c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6b8430-ac88-4358-88c1-e6c5400fe7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = y_train.columns.to_list()\n",
    "df2 = list(set(set(all_data.columns) & set(New_Names)))\n",
    "\n",
    "flattened = [] \n",
    "for sublist in df1,df2: \n",
    "    for val in sublist: \n",
    "        flattened.append(val) \n",
    "\n",
    "all_data.iloc[rows][flattened].style.background_gradient(cmap ='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
