{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc8d721-0e1d-4ffc-857c-3c2d3fed2a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step stacked lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Layer\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f380d66-f51c-4955-b92a-a2f61a44007f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1068c889-6cd2-4a0c-af36-aca4a84f65dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'in_seq1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 56>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     53\u001b[0m in_seq00 \u001b[38;5;241m=\u001b[39m in_seq[:,\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     54\u001b[0m in_seq01 \u001b[38;5;241m=\u001b[39m in_seq[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 56\u001b[0m out_seq \u001b[38;5;241m=\u001b[39m array([in_seq00[i]\u001b[38;5;241m+\u001b[39min_seq01[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43min_seq1\u001b[49m))])\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# convert to [rows, columns] structure\u001b[39;00m\n\u001b[0;32m     58\u001b[0m in_seq00 \u001b[38;5;241m=\u001b[39m in_seq00\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;28mlen\u001b[39m(in_seq00), \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'in_seq1' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# multivariate multi-step encoder-decoder lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    " \n",
    "    \n",
    "\n",
    "def stable_sigmoid(x):\n",
    "    sig = np.where(x < 0, np.exp(x)/(1 + np.exp(x)), 1/(1 + np.exp(-x)))\n",
    "    return sig\n",
    "\n",
    "def inverse_sigmoid(x):\n",
    "    inv_sig = -np.log((1 / (x + 1e-8)) - 1)\n",
    "    return inv_sig\n",
    "\n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    " \n",
    "# define input sequence\n",
    "in_seq00 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq01 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "\n",
    "in_seq = np.transpose((\n",
    "    in_seq00, \n",
    "    in_seq01\n",
    "))\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#ensuring scaling isn't applied to test data\n",
    "x_train = in_seq[:-1]\n",
    "scaler.fit(x_train)\n",
    "in_seq = scaler.transform(in_seq)\n",
    "in_seq = stable_sigmoid(in_seq)\n",
    "\n",
    "in_seq00 = in_seq[:,0]\n",
    "in_seq01 = in_seq[:,1]\n",
    "\n",
    "out_seq = array([in_seq00[i]+in_seq01[i] for i in range(len(in_seq00))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq00 = in_seq00.reshape((len(in_seq00), 1))\n",
    "in_seq01 = in_seq01.reshape((len(in_seq01), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq00, in_seq01, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, activation='tanh', return_sequences=True), input_shape=(n_steps_in, n_features)))\n",
    "model.add((LSTM(128, activation='tanh', return_sequences=True)))\n",
    "model.add(attention(return_sequences=True)) #\n",
    "model.add(RepeatVector(n_steps_out))\n",
    "#model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=300, verbose=0)\n",
    "# demonstrate prediction\n",
    "x_input = array([[60, 65, 125], [70, 75, 145], [80, 85, 165]])\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=0)\n",
    "#print(yhat)\n",
    "\n",
    "y_ = inverse_sigmoid(y[-1])*scaler.scale_[0]+scaler.mean_[0]\n",
    "yhat = inverse_sigmoid(yhat)*scaler.scale_[0]+scaler.mean_[0]\n",
    "print(y_)\n",
    "print(*yhat)\n",
    "plt.plot(y_)\n",
    "plt.plot(*yhat)\n",
    "plt.show()\n",
    "\n",
    "rmse = mean_squared_error(y_, *yhat, squared=True)\n",
    "print(\"RMSE:\",rmse,\"MAPE:\",MAPE(y_, yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
