{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41716ad9-56fc-491f-87e0-6a782447dc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kerasrl2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 14>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#from keras.optimizers import Adam\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mK\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkerasrl2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdqn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DQNAgent\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkerasrl2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkerasrl2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SequentialMemory\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kerasrl2'"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute\n",
    "#\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "\n",
    "from rl2.agents.dqn import DQNAgent\n",
    "from rl2.policy import LinearAnnealedPolicy, BoltzmannQPolicy, EpsGreedyQPolicy\n",
    "from rl2.memory import SequentialMemory\n",
    "from rl2.core import Processor\n",
    "from rl2.callbacks import FileLogger, ModelIntervalCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759adea-dee6-4f16-92d3-048b2cd8ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "  \n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af08f5-3668-4ca9-b124-558b81a93359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the environment\n",
    "environment_name = 'MountainCar-v0'\n",
    "env = gym.make(environment_name)\n",
    "np.random.seed(0)\n",
    "env.seed(0)\n",
    "  \n",
    "# Extracting the number of possible actions\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69695e8-b914-424a-a020-80bb87333ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Sequential()\n",
    "agent.add(Flatten(input_shape =(1, ) + env.observation_space.shape))\n",
    "agent.add(Dense(16))\n",
    "agent.add(Activation('relu'))\n",
    "agent.add(Dense(num_actions))\n",
    "agent.add(Activation('linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873fc3d0-69e5-40be-ab08-85d197f6a21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model to find the optimal strategy\n",
    "strategy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit = 10000, window_length = 1)\n",
    "dqn = DQNAgent(model = agent, nb_actions = num_actions,\n",
    "               memory = memory, nb_steps_warmup = 10,\n",
    "target_model_update = 1e-2, policy = strategy)\n",
    "dqn.compile(Adam(lr = 1e-3), metrics =['mae'])\n",
    "  \n",
    "# Visualizing the training \n",
    "dqn.fit(env, nb_steps = 5000, visualize = True, verbose = 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
