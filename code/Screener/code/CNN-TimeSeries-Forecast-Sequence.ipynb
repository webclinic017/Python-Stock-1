{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d0601ea-9997-4a44-852c-e62f88bb20cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariate multi-step stacked lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Layer\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    " \n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6d5a0faa-01a9-4912-9565-19422ee2e1e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9eacf910-197b-484e-a83d-12c312fc4b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3, 2) (5, 2)\n",
      "[[10 15]\n",
      " [20 25]\n",
      " [30 35]] [45 55]\n",
      "[[20 25]\n",
      " [30 35]\n",
      " [40 45]] [55 65]\n",
      "[[30 35]\n",
      " [40 45]\n",
      " [50 55]] [65 75]\n",
      "[[40 45]\n",
      " [50 55]\n",
      " [60 65]] [75 85]\n",
      "[[50 55]\n",
      " [60 65]\n",
      " [70 75]] [85 95]\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "1/1 [==============================] - 0s 481ms/step\n",
      "[[107.06367 117.60981]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "power = PowerTransformer(method='yeo-johnson')\n",
    " \n",
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "        super(attention,self).__init__()\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "    \n",
    "\n",
    "def stable_sigmoid(x):\n",
    "\n",
    "    sig = np.where(x < 0, np.exp(x)/(1 + np.exp(x)), 1/(1 + np.exp(-x)))\n",
    "    return sig\n",
    "\n",
    "def inverse_sigmoid(x):\n",
    "    inv_sig = -np.log((1 / (x + 1e-8)) - 1)\n",
    "    return inv_sig\n",
    "    \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "\tX, y = list(), list()\n",
    "\tfor i in range(len(sequences)):\n",
    "\t\t# find the end of this pattern\n",
    "\t\tend_ix = i + n_steps_in\n",
    "\t\tout_end_ix = end_ix + n_steps_out\n",
    "\t\t# check if we are beyond the dataset\n",
    "\t\tif out_end_ix > len(sequences):\n",
    "\t\t\tbreak\n",
    "\t\t# gather input and output parts of the pattern\n",
    "\t\tseq_x, seq_y = sequences[i:end_ix, 0:2], sequences[end_ix-1:out_end_ix, 1][1:3]\n",
    "\t\tX.append(seq_x)\n",
    "\t\ty.append(seq_y)\n",
    "\treturn array(X), array(y)\n",
    "     \n",
    "# define input sequence\n",
    "in_seq1 = array([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
    "in_seq2 = array([15, 25, 35, 45, 55, 65, 75, 85, 95])\n",
    "\n",
    "in_seq = np.transpose((in_seq1, in_seq2))\n",
    "#np.array(pd.concat([pd.DataFrame(in_seq1),pd.DataFrame(in_seq2)],axis=1))\n",
    "\n",
    "in_seq1 = in_seq[:,0]\n",
    "in_seq2 = in_seq[:,1]\n",
    "\n",
    "out_seq = array([in_seq1[i]+in_seq2[i] for i in range(len(in_seq1))])\n",
    "# convert to [rows, columns] structure\n",
    "in_seq1 = in_seq1.reshape((len(in_seq1), 1))\n",
    "in_seq2 = in_seq2.reshape((len(in_seq2), 1))\n",
    "out_seq = out_seq.reshape((len(out_seq), 1))\n",
    "# horizontally stack columns\n",
    "dataset = hstack((in_seq1, in_seq2, out_seq))\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 3, 2\n",
    "# covert into input/output\n",
    "X, y = split_sequences(dataset, n_steps_in, n_steps_out)\n",
    "\n",
    "print(X.shape, y.shape)\n",
    "# summarize the data\n",
    "for i in range(len(X)):\n",
    "\tprint(X[i], y[i])\n",
    "\n",
    "\n",
    "n_features = X.shape[2]\n",
    "# define model\n",
    "model = Sequential()\n",
    "\n",
    "\"\"\"\n",
    "model.add(LSTM(100, activation='tanh', return_sequences=True, input_shape=(n_steps_in, n_features)))\n",
    "model.add(LSTM(100, activation='tanh'))\n",
    "model.add(Dense(n_steps_out))\n",
    "\"\"\"\n",
    "model.add(Bidirectional(LSTM(128, activation='relu', return_sequences=True), input_shape=(n_steps_in, n_features)))\n",
    "model.add((LSTM(128, activation='relu', return_sequences=True)))\n",
    "#model.add(BatchNormalization())\n",
    "model.add(attention(return_sequences=True)) #\n",
    "model.add(Dropout(0.05))\n",
    "model.add(Dense(n_steps_out))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "# fit model\n",
    "model.fit(X, y, epochs=200, verbose=0)\n",
    "# demonstrate prediction\n",
    "#x_input = array([[70, 75], [80, 85], [90, 95]])\n",
    "x_input = in_seq[-n_steps_in:]\n",
    "x_input = x_input.reshape((1, n_steps_in, n_features))\n",
    "yhat = model.predict(x_input, verbose=1)\n",
    "print(yhat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6dc7f584-4229-4f13-85ae-23186ed902b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[70, 75],\n",
       "        [80, 85],\n",
       "        [90, 95]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b9124515-bd47-451e-b3a2-b5d7c7ac5171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 75],\n",
       "       [80, 85],\n",
       "       [90, 95]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "80ffbc61-7260-4f53-9da9-460340818391",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
