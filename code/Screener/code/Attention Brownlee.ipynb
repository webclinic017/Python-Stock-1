{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5c78884-ef28-4158-a955-5d6f773546dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "826/826 - 61s - loss: 0.0043 - 61s/epoch - 73ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 63s - loss: 0.0041 - 63s/epoch - 76ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 65s - loss: 0.0037 - 65s/epoch - 78ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 65s - loss: 0.0033 - 65s/epoch - 78ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 66s - loss: 0.0029 - 66s/epoch - 79ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 66s - loss: 0.0026 - 66s/epoch - 80ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 62s - loss: 0.0023 - 62s/epoch - 75ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 61s - loss: 0.0021 - 61s/epoch - 74ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 62s - loss: 0.0020 - 62s/epoch - 75ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 67s - loss: 0.0018 - 67s/epoch - 81ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 67s - loss: 0.0017 - 67s/epoch - 82ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 69s - loss: 0.0016 - 69s/epoch - 83ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 74s - loss: 0.0014 - 74s/epoch - 89ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 70s - loss: 0.0013 - 70s/epoch - 85ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 70s - loss: 0.0011 - 70s/epoch - 85ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 64s - loss: 9.5068e-04 - 64s/epoch - 78ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 70s - loss: 7.8007e-04 - 70s/epoch - 85ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 66s - loss: 6.0897e-04 - 66s/epoch - 80ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 60s - loss: 5.3734e-04 - 60s/epoch - 73ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 69s - loss: 3.8687e-04 - 69s/epoch - 83ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 77s - loss: 3.0281e-04 - 77s/epoch - 93ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 71s - loss: 2.4070e-04 - 71s/epoch - 86ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 63s - loss: 1.8204e-04 - 63s/epoch - 77ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 63s - loss: 1.4153e-04 - 63s/epoch - 77ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 64s - loss: 1.1095e-04 - 64s/epoch - 77ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 78s - loss: 8.4969e-05 - 78s/epoch - 95ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 65s - loss: 7.5881e-05 - 65s/epoch - 79ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 64s - loss: 6.3619e-05 - 64s/epoch - 77ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 71s - loss: 6.1446e-05 - 71s/epoch - 86ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 83s - loss: 4.7600e-05 - 83s/epoch - 101ms/step\n",
      "26/26 [==============================] - 1s 28ms/step - loss: 5.9746e-05\n",
      "12/12 [==============================] - 1s 46ms/step - loss: 4.1467e-05\n",
      "Train set MSE =  5.9745998441940174e-05\n",
      "Test set MSE =  4.14672504120972e-05\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, 20, 2)             8         \n",
      "                                                                 \n",
      " attention (attention)       (None, 2)                 22        \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 3         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 33\n",
      "Trainable params: 33\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/30\n",
      "826/826 - 72s - loss: 0.0018 - 72s/epoch - 88ms/step\n",
      "Epoch 2/30\n",
      "826/826 - 71s - loss: 0.0017 - 71s/epoch - 85ms/step\n",
      "Epoch 3/30\n",
      "826/826 - 78s - loss: 0.0016 - 78s/epoch - 94ms/step\n",
      "Epoch 4/30\n",
      "826/826 - 64s - loss: 0.0016 - 64s/epoch - 77ms/step\n",
      "Epoch 5/30\n",
      "826/826 - 71s - loss: 0.0016 - 71s/epoch - 86ms/step\n",
      "Epoch 6/30\n",
      "826/826 - 74s - loss: 0.0015 - 74s/epoch - 90ms/step\n",
      "Epoch 7/30\n",
      "826/826 - 62s - loss: 0.0015 - 62s/epoch - 76ms/step\n",
      "Epoch 8/30\n",
      "826/826 - 65s - loss: 0.0015 - 65s/epoch - 79ms/step\n",
      "Epoch 9/30\n",
      "826/826 - 68s - loss: 0.0015 - 68s/epoch - 82ms/step\n",
      "Epoch 10/30\n",
      "826/826 - 69s - loss: 0.0015 - 69s/epoch - 84ms/step\n",
      "Epoch 11/30\n",
      "826/826 - 73s - loss: 0.0015 - 73s/epoch - 88ms/step\n",
      "Epoch 12/30\n",
      "826/826 - 69s - loss: 0.0014 - 69s/epoch - 83ms/step\n",
      "Epoch 13/30\n",
      "826/826 - 72s - loss: 0.0014 - 72s/epoch - 87ms/step\n",
      "Epoch 14/30\n",
      "826/826 - 64s - loss: 0.0014 - 64s/epoch - 77ms/step\n",
      "Epoch 15/30\n",
      "826/826 - 59s - loss: 0.0014 - 59s/epoch - 71ms/step\n",
      "Epoch 16/30\n",
      "826/826 - 59s - loss: 0.0014 - 59s/epoch - 71ms/step\n",
      "Epoch 17/30\n",
      "826/826 - 60s - loss: 0.0014 - 60s/epoch - 73ms/step\n",
      "Epoch 18/30\n",
      "826/826 - 67s - loss: 0.0014 - 67s/epoch - 82ms/step\n",
      "Epoch 19/30\n",
      "826/826 - 67s - loss: 0.0013 - 67s/epoch - 81ms/step\n",
      "Epoch 20/30\n",
      "826/826 - 63s - loss: 0.0013 - 63s/epoch - 76ms/step\n",
      "Epoch 21/30\n",
      "826/826 - 64s - loss: 0.0013 - 64s/epoch - 78ms/step\n",
      "Epoch 22/30\n",
      "826/826 - 63s - loss: 0.0012 - 63s/epoch - 76ms/step\n",
      "Epoch 23/30\n",
      "826/826 - 62s - loss: 0.0012 - 62s/epoch - 75ms/step\n",
      "Epoch 24/30\n",
      "826/826 - 65s - loss: 0.0012 - 65s/epoch - 79ms/step\n",
      "Epoch 25/30\n",
      "826/826 - 65s - loss: 0.0012 - 65s/epoch - 79ms/step\n",
      "Epoch 26/30\n",
      "826/826 - 71s - loss: 0.0011 - 71s/epoch - 85ms/step\n",
      "Epoch 27/30\n",
      "826/826 - 85s - loss: 0.0011 - 85s/epoch - 102ms/step\n",
      "Epoch 28/30\n",
      "826/826 - 75s - loss: 0.0010 - 75s/epoch - 91ms/step\n",
      "Epoch 29/30\n",
      "826/826 - 68s - loss: 9.7728e-04 - 68s/epoch - 82ms/step\n",
      "Epoch 30/30\n",
      "826/826 - 64s - loss: 9.1968e-04 - 64s/epoch - 77ms/step\n",
      "26/26 [==============================] - 1s 18ms/step - loss: 8.1088e-04\n",
      "12/12 [==============================] - 0s 18ms/step - loss: 7.0461e-04\n",
      "Train set MSE with attention =  0.0008108848705887794\n",
      "Test set MSE with attention =  0.0007046139799058437\n"
     ]
    }
   ],
   "source": [
    "from pandas import read_csv\n",
    "import numpy as np\n",
    "from keras import Model\n",
    "from keras.layers import Layer\n",
    "import keras.backend as K\n",
    "from keras.layers import Input, Dense, SimpleRNN\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import mean_squared_error\n",
    " \n",
    "# Prepare data\n",
    "def get_fib_seq(n, scale_data=True):\n",
    "    # Get the Fibonacci sequence\n",
    "    seq = np.zeros(n)\n",
    "    fib_n1 = 0.0\n",
    "    fib_n = 1.0 \n",
    "    for i in range(n):\n",
    "            seq[i] = fib_n1 + fib_n\n",
    "            fib_n1 = fib_n\n",
    "            fib_n = seq[i] \n",
    "    scaler = []\n",
    "    if scale_data:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        seq = np.reshape(seq, (n, 1))\n",
    "        seq = scaler.fit_transform(seq).flatten()        \n",
    "    return seq, scaler\n",
    " \n",
    "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
    "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)    \n",
    "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
    "    Y = dat[Y_ind]\n",
    "    rows_x = len(Y)\n",
    "    X = dat[0:rows_x]\n",
    "    for i in range(time_steps-1):\n",
    "        temp = dat[i+1:rows_x+i+1]\n",
    "        X = np.column_stack((X, temp))\n",
    "    # random permutation with fixed seed   \n",
    "    rand = np.random.RandomState(seed=13)\n",
    "    idx = rand.permutation(rows_x)\n",
    "    split = int(train_percent*rows_x)\n",
    "    train_ind = idx[0:split]\n",
    "    test_ind = idx[split:]\n",
    "    trainX = X[train_ind]\n",
    "    trainY = Y[train_ind]\n",
    "    testX = X[test_ind]\n",
    "    testY = Y[test_ind]\n",
    "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))    \n",
    "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
    "    return trainX, trainY, testX, testY, scaler\n",
    " \n",
    "# Set up parameters\n",
    "time_steps = 20\n",
    "hidden_units = 2\n",
    "epochs = 30\n",
    " \n",
    "# Create a traditional RNN network\n",
    "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
    "    model = Sequential()\n",
    "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
    "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
    "    model.compile(loss='mse', optimizer='adam')\n",
    "    return model\n",
    " \n",
    "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps,1), \n",
    "                   activation=['tanh', 'tanh'])\n",
    " \n",
    "# Generate the dataset for the network\n",
    "trainX, trainY, testX, testY, scaler  = get_fib_XY(1200, time_steps, 0.7)\n",
    "# Train the network\n",
    "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    " \n",
    " \n",
    "# Evalute model\n",
    "train_mse = model_RNN.evaluate(trainX, trainY)\n",
    "test_mse = model_RNN.evaluate(testX, testY)\n",
    " \n",
    "# Print error\n",
    "print(\"Train set MSE = \", train_mse)\n",
    "print(\"Test set MSE = \", test_mse)\n",
    " \n",
    " \n",
    "# Add attention layer to the deep learning network\n",
    "class attention(Layer):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(attention,self).__init__(**kwargs)\n",
    " \n",
    "    def build(self,input_shape):\n",
    "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1), \n",
    "                               initializer='random_normal', trainable=True)\n",
    "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1), \n",
    "                               initializer='zeros', trainable=True)        \n",
    "        super(attention, self).build(input_shape)\n",
    " \n",
    "    def call(self,x):\n",
    "        # Alignment scores. Pass them through tanh function\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        # Remove dimension of size 1\n",
    "        e = K.squeeze(e, axis=-1)   \n",
    "        # Compute the weights\n",
    "        alpha = K.softmax(e)\n",
    "        # Reshape to tensorFlow format\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        # Compute the context vector\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "    \n",
    "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
    "    x=Input(shape=input_shape)\n",
    "    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
    "    attention_layer = attention()(RNN_layer)\n",
    "    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
    "    model=Model(x,outputs)\n",
    "    model.compile(loss='mse', optimizer='adam')    \n",
    "    return model    \n",
    " \n",
    "# Create the model with attention, train and evaluate\n",
    "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1, \n",
    "                                  input_shape=(time_steps,1), activation='tanh')\n",
    "model_attention.summary()    \n",
    " \n",
    " \n",
    "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
    " \n",
    "# Evalute model\n",
    "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
    "test_mse_attn = model_attention.evaluate(testX, testY)\n",
    " \n",
    "# Print error\n",
    "print(\"Train set MSE with attention = \", train_mse_attn)\n",
    "print(\"Test set MSE with attention = \", test_mse_attn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
